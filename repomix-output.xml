This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.gitignore
backend/.env.example
backend/app/__init__.py
backend/app/api/__init__.py
backend/app/api/routes.py
backend/app/api/websocket.py
backend/app/config.py
backend/app/db/__init__.py
backend/app/db/supabase_client.py
backend/app/main.py
backend/app/models/__init__.py
backend/app/models/research_job.py
backend/app/orchestrator/__init__.py
backend/app/orchestrator/research_engine.py
backend/app/orchestrator/research_engine.py.backup
backend/app/orchestrator/tongyi_client.py
backend/app/tools/__init__.py
backend/app/tools/tool_registry.py
backend/app/tools/web_fetch.py
backend/app/tools/web_search.py
backend/README.md
backend/requirements.txt
backend/start.ps1
backend/test_websocket.py
disable_rls.sql
frontend/.env.local.example
frontend/app/api/research/route.ts
frontend/app/globals.css
frontend/app/layout.tsx
frontend/app/page.tsx
frontend/app/research/[jobId]/page.tsx
frontend/components/IterationCard.tsx
frontend/components/OutputViewer.tsx
frontend/components/ProgressBar.tsx
frontend/components/SourcePanel.tsx
frontend/next-env.d.ts
frontend/next.config.js
frontend/package.json
frontend/postcss.config.js
frontend/README.md
frontend/tailwind.config.ts
frontend/tsconfig.json
frontend/types/research.ts
MIGRATION_INSTRUCTIONS.md
QUICKSTART.md
README.md
run_migration_dashboard.py
run-backend.snip
supabase/migrations/001_initial_schema.sql
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".gitignore">
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST
venv/
env/
ENV/
.venv

# Node
node_modules/
npm-debug.log*
yarn-debug.log*
yarn-error.log*
.pnpm-debug.log*
.next/
out/
build/
dist/

# Environment variables
.env
.env.local
.env.development.local
.env.test.local
.env.production.local
.env*.local

# IDE
.vscode/
.idea/
*.swp
*.swo
*~
.DS_Store

# Testing
.coverage
.pytest_cache/
.tox/
htmlcov/

# Logs
*.log
logs/

# Supabase
.supabase/

# Misc
*.tmp
*.bak
.cache/
</file>

<file path="backend/app/__init__.py">
"""
Agent Bletchley Backend Application
"""
</file>

<file path="backend/app/api/__init__.py">
"""
API routes module.
"""
</file>

<file path="backend/app/main.py">
"""
FastAPI application entry point.
"""
import logging
from fastapi import FastAPI, WebSocket
from fastapi.middleware.cors import CORSMiddleware
from app.config import settings
from app.api import routes
from app.api.websocket import websocket_endpoint

# Configure logging
logging.basicConfig(
    level=getattr(logging, settings.LOG_LEVEL),
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# Create FastAPI app
app = FastAPI(
    title="Agent Bletchley API",
    description="Agentic web research system for investment due diligence",
    version="1.0.0",
)

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=[settings.FRONTEND_URL],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include routers
app.include_router(routes.router)


@app.get("/")
async def root() -> dict[str, str]:
    """Root endpoint returning greeting."""
    return {"message": "Hello Agent Bletchley"}


@app.get("/health")
async def health_check() -> dict[str, str]:
    """Health check endpoint."""
    return {"status": "healthy"}


@app.websocket("/ws/research/{job_id}")
async def websocket_route(websocket: WebSocket, job_id: str) -> None:
    """WebSocket endpoint for real-time research updates."""
    await websocket_endpoint(websocket, job_id)


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "app.main:app",
        host=settings.BACKEND_HOST,
        port=settings.BACKEND_PORT,
        reload=True,
    )
</file>

<file path="backend/app/models/__init__.py">
"""
Data models module.
"""
</file>

<file path="backend/app/models/research_job.py">
"""
Pydantic models for research jobs.
"""
from datetime import datetime
from typing import Optional, List, Dict, Any
from enum import Enum
from pydantic import BaseModel, Field


class ResearchJobStatus(str, Enum):
    """Research job status enumeration."""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"


class ResearchJobCreate(BaseModel):
    """Model for creating a new research job."""
    query: str = Field(..., description="The research query or question")
    context: Optional[Dict[str, Any]] = Field(None, description="Optional context for the research")


class ResearchJob(BaseModel):
    """Model representing a research job."""
    id: str = Field(..., description="Unique job identifier")
    query: str = Field(..., description="The research query")
    status: ResearchJobStatus = Field(..., description="Current job status")
    progress: float = Field(0.0, ge=0.0, le=100.0, description="Progress percentage")
    created_at: datetime = Field(default_factory=datetime.utcnow, description="Job creation timestamp")
    updated_at: Optional[datetime] = Field(None, description="Last update timestamp")
    completed_at: Optional[datetime] = Field(None, description="Completion timestamp")
    sources: List[Dict[str, Any]] = Field(default_factory=list, description="Gathered sources")
    iterations: List[Dict[str, Any]] = Field(default_factory=list, description="Research iterations")
    report: Optional[str] = Field(None, description="Final research report")
    error: Optional[str] = Field(None, description="Error message if job failed")
    
    class Config:
        """Pydantic config."""
        use_enum_values = True
</file>

<file path="backend/app/orchestrator/__init__.py">
"""
Research orchestrator module.
"""
</file>

<file path="backend/app/orchestrator/research_engine.py.backup">
"""
Main research engine that orchestrates the research process.
"""
import logging
import asyncio
import json
from datetime import datetime
from typing import Dict, List, Any, Optional, TYPE_CHECKING
from app.models.research_job import ResearchJob, ResearchJobStatus
from app.orchestrator.tongyi_client import TongyiClient
from app.tools.tool_registry import ToolRegistry
from app.db.supabase_client import (
    update_job_status as db_update_job_status,
    update_job_report,
    add_iteration,
    add_source,
    get_sources_by_job,
    get_job
)

if TYPE_CHECKING:
    from app.api.websocket import ConnectionManager

logger = logging.getLogger(__name__)


class ResearchEngine:
    """
    Main research engine that coordinates AI agent research activities.
    
    TODO: Implement the main research loop that:
    1. Receives research query
    2. Uses Tongyi DeepResearch agent to plan research steps
    3. Executes tools (web search, web fetch) based on agent decisions
    4. Aggregates and synthesizes results
    5. Returns comprehensive research report
    """
    
    def __init__(self):
        """Initialize the research engine."""
        self.tongyi_client = TongyiClient()
        self.tool_registry = ToolRegistry()
    
    async def start_research(
        self, 
        query: str, 
        job_id: str, 
        connection_manager: Optional["ConnectionManager"] = None
    ) -> ResearchJob:
        """
        Start a new research job.
        
        Args:
            query: The research query/question
            job_id: Unique identifier for this research job
            connection_manager: Optional WebSocket connection manager for real-time updates
            
        Returns:
            ResearchJob instance with initial status
        """
        # TODO: Create initial research job in database
        # TODO: Initialize research context and state
        
        logger.info(f"Starting research job {job_id} with query: {query}")
        
        # Update job status to running in database
        await db_update_job_status(job_id, "running", 0.0)
        
        job = ResearchJob(
            id=job_id,
            query=query,
            status=ResearchJobStatus.RUNNING,
        )
        
        # Broadcast initial RUNNING status
        if connection_manager:
            asyncio.create_task(
                connection_manager.broadcast_status(job_id, ResearchJobStatus.RUNNING.value, 0.0)
            )
        
        # Begin research loop
        asyncio.create_task(self._run_research_loop(job_id, query, connection_manager))
        
        return job
    
    async def _run_research_loop(
        self,
        job_id: str,
        query: str,
        connection_manager: Optional["ConnectionManager"] = None
    ) -> None:
        """
        Run the main research loop with Tongyi DeepResearch agent.
        
        Args:
            job_id: Research job ID
            query: Research query
            connection_manager: Optional WebSocket connection manager
        """
        max_iterations = 20
        iteration_count = 0
        
        try:
            # Update status to RUNNING in database (already done in start_research, but ensure it's set)
            await db_update_job_status(job_id, "running", 0.0)
            
            # Broadcast status to WebSocket
            if connection_manager:
                asyncio.create_task(
                    connection_manager.broadcast_status(job_id, ResearchJobStatus.RUNNING.value, 0.0)
                )
            
            # Initialize conversation with system prompt and get tool definitions
            tools = self.tool_registry.get_tool_definitions()
            
            system_prompt = (
                "You are an expert research assistant specialized in investment due diligence. "
                "Your goal is to conduct thorough, comprehensive research on companies, markets, and investment opportunities.\n\n"
                
                "MANDATORY TOOL USAGE RULES:\n"
                "- You MUST use tool calls (not text descriptions) to gather information.\n"
                "- When you need information, invoke tools using the tool_calls format (JSON-structured function calls).\n"
                "- NEVER describe what tools to use in your response content - always make actual tool calls.\n"
                "- DO NOT output text like 'I should use web_search' or 'I need to call web_fetch'.\n"
                "- DO NOT write tool names, function names, or arguments as plain text.\n"
                "- ALWAYS invoke tools directly - the system will execute them automatically.\n\n"
                
                "Available tools:\n"
                "- web_search: Search the web for information using Brave Search. Use this FIRST to find relevant sources.\n"
                "- web_fetch: Fetch and parse content from web URLs. Use this AFTER search to read articles.\n\n"
                
                "Workflow:\n"
                "1. Start by using web_search to find relevant sources about the topic\n"
                "2. Use web_fetch to read important articles and pages you found\n"
                "3. Continue gathering information until you have comprehensive coverage\n"
                "4. Only provide your final answer when you have sufficient information from tools\n\n"
                
                "Explore multiple perspectives, verify facts, and synthesize your findings into a well-structured analysis. "
                "Remember: ALWAYS use tool calls, NEVER describe tools in text."
            )
            
            messages = [
                {
                    "role": "system",
                    "content": system_prompt
                },
                {
                    "role": "user",
                    "content": query
                }
            ]
            
            logger.info(f"Starting research loop for job {job_id} with query: {query}")
            
            # Main research loop
            while iteration_count < max_iterations:
                iteration_count += 1
                logger.info(f"Research iteration {iteration_count}/{max_iterations} for job {job_id}")
                
                try:
                    # Call Tongyi API with current conversation and tools
                    response = await self.tongyi_client.chat_completion(
                        messages=messages,
                        tools=tools
                    )
                    
                    # Extract tool calls and content
                    tool_calls = response.get("tool_calls", [])
                    content = response.get("content", "")
                    assistant_message = response.get("message", {})
                    
                    # Add assistant message to conversation
                    messages.append(assistant_message)
                    
                    # Process tool calls if any
                    if tool_calls:
                        logger.info(f"Processing {len(tool_calls)} tool call(s) in iteration {iteration_count}")
                        
                        # Execute tools sequentially
                        tool_results = []
                        for tool_call in tool_calls:
                            tool_id = tool_call.get("id", "")
                            function = tool_call.get("function", {})
                            tool_name = function.get("name", "")
                            tool_args = function.get("arguments", {})
                            
                            if not tool_name:
                                logger.warning(f"Skipping tool call with missing name: {tool_call}")
                                continue
                            
                            try:
                                # Execute tool
                                logger.info(f"Executing tool: {tool_name} with args: {tool_args}")
                                tool_result = await self.tool_registry.execute_tool(tool_name, tool_args)
                                
                                # Handle tool results
                                if tool_name == "web_search":
                                    # Extract search results and save sources
                                    search_results = tool_result.get("results", [])
                                    for result in search_results:
                                        url = result.get("url", "")
                                        title = result.get("title", "")
                                        snippet = result.get("snippet", "")
                                        
                                        if url:
                                            try:
                                                await add_source(job_id, url, title, snippet)
                                                
                                                # Broadcast source discovery
                                                if connection_manager:
                                                    source_data = {
                                                        "url": url,
                                                        "title": title,
                                                        "snippet": snippet,
                                                        "fetched_at": None
                                                    }
                                                    asyncio.create_task(
                                                        connection_manager.broadcast_source(job_id, source_data)
                                                    )
                                            except Exception as e:
                                                logger.error(f"Error saving source {url}: {e}", exc_info=True)
                                
                                elif tool_name == "web_fetch":
                                    # Extract fetched content and save source
                                    fetch_content = tool_result.get("content", {})
                                    if isinstance(fetch_content, dict):
                                        url = fetch_content.get("url", "")
                                        title = fetch_content.get("title", "")
                                        content = fetch_content.get("content", "")
                                        
                                        if url:
                                            try:
                                                await add_source(job_id, url, title, None, content)
                                                
                                                # Broadcast source fetch
                                                if connection_manager:
                                                    source_data = {
                                                        "url": url,
                                                        "title": title,
                                                        "snippet": content[:200] + "..." if len(content) > 200 else content,
                                                        "fetched_at": datetime.utcnow().isoformat()
                                                    }
                                                    asyncio.create_task(
                                                        connection_manager.broadcast_source(job_id, source_data)
                                                    )
                                            except Exception as e:
                                                logger.error(f"Error saving fetched source {url}: {e}", exc_info=True)
                                
                                # Format tool result for message
                                # Note: OpenAI/OpenRouter format requires only role, tool_call_id, and content
                                # The "name" field is NOT part of tool result messages (it's only in function definitions)
                                tool_results.append({
                                    "tool_call_id": tool_id,
                                    "role": "tool",
                                    "content": json.dumps(tool_result)
                                })
                                
                            except Exception as e:
                                logger.error(f"Error executing tool {tool_name}: {e}", exc_info=True)
                                # Add error result to continue loop
                                # Note: OpenAI/OpenRouter format requires only role, tool_call_id, and content
                                # The "name" field is NOT part of tool result messages
                                tool_results.append({
                                    "tool_call_id": tool_id,
                                    "role": "tool",
                                    "content": json.dumps({"error": str(e)})
                                })
                        
                        # Add tool results to conversation
                        messages.extend(tool_results)
                        
                        # Prepare iteration data
                        action = f"tool_execution: {', '.join([tc.get('function', {}).get('name', 'unknown') for tc in tool_calls])}"
                        step_result = {
                            "status": "completed",
                            "action": action,
                            "timestamp": datetime.utcnow().isoformat(),
                            "step": iteration_count,
                            "tool_calls": tool_calls,
                            "tool_results": [r.get("content", "") for r in tool_results]
                        }
                        
                    else:
                        # No tool calls - final answer received
                        logger.info(f"Final answer received in iteration {iteration_count}")
                        
                        # Prepare iteration data
                        action = "final_answer"
                        step_result = {
                            "status": "completed",
                            "action": action,
                            "timestamp": datetime.utcnow().isoformat(),
                            "step": iteration_count,
                            "content": content
                        }
                        
                        # Persist final iteration
                        await add_iteration(job_id, iteration_count, action, step_result)
                        
                        # Broadcast final iteration
                        if connection_manager:
                            iteration_data = {
                                "id": f"{job_id}-iter-{iteration_count}",
                                "step": iteration_count,
                                "action": action,
                                "timestamp": step_result.get("timestamp"),
                                "results": step_result
                            }
                            asyncio.create_task(
                                connection_manager.broadcast_iteration(job_id, iteration_data)
                            )
                        
                        # Break from loop - research complete
                        break
                    
                    # Persist iteration to database
                    await add_iteration(job_id, iteration_count, action, step_result)
                    
                    # Broadcast iteration update
                    if connection_manager:
                        iteration_data = {
                            "id": f"{job_id}-iter-{iteration_count}",
                            "step": iteration_count,
                            "action": action,
                            "timestamp": step_result.get("timestamp"),
                            "results": step_result
                        }
                        asyncio.create_task(
                            connection_manager.broadcast_iteration(job_id, iteration_data)
                        )
                    
                    # Update progress: research phase is 0-90%
                    progress = min(90, (iteration_count / max_iterations) * 90)
                    await db_update_job_status(job_id, "running", progress)
                    
                    # Broadcast progress update
                    if connection_manager:
                        asyncio.create_task(
                            connection_manager.broadcast_status(job_id, ResearchJobStatus.RUNNING.value, progress)
                        )
                    
                except Exception as e:
                    logger.error(f"Error in iteration {iteration_count} for job {job_id}: {e}", exc_info=True)
                    # Continue to next iteration - don't break entire loop
                    # Save error iteration
                    error_result = {
                        "status": "error",
                        "action": "error",
                        "timestamp": datetime.utcnow().isoformat(),
                        "step": iteration_count,
                        "error": str(e)
                    }
                    try:
                        await add_iteration(job_id, iteration_count, "error", error_result)
                    except Exception:
                        pass  # Don't fail on iteration save error
                    continue
            
            # Synthesize results
            logger.info(f"Starting synthesis for job {job_id}")
            
            # Update progress to 90% (entering synthesis phase)
            await db_update_job_status(job_id, "running", 90.0)
            if connection_manager:
                asyncio.create_task(
                    connection_manager.broadcast_status(job_id, ResearchJobStatus.RUNNING.value, 90.0)
                )
            
            # Fetch all sources from database
            sources = await get_sources_by_job(job_id)
            logger.info(f"Found {len(sources)} sources for synthesis")
            
            # Generate final report
            report = await self.synthesize_results(job_id, sources)
            
            # Update progress to 100%
            await db_update_job_status(job_id, "running", 100.0)
            if connection_manager:
                asyncio.create_task(
                    connection_manager.broadcast_status(job_id, ResearchJobStatus.RUNNING.value, 100.0)
                )
            
            # Persist report to database
            await update_job_report(job_id, report)
            
            # Broadcast final report
            if connection_manager:
                asyncio.create_task(
                    connection_manager.broadcast_report(job_id, report)
                )
            
            # Update status to COMPLETED in database
            await db_update_job_status(job_id, "completed", 100.0)
            
            # Broadcast completion status
            if connection_manager:
                asyncio.create_task(
                    connection_manager.broadcast_status(job_id, ResearchJobStatus.COMPLETED.value, 100.0)
                )
            
            logger.info(f"Research loop completed successfully for job {job_id}")
                
        except Exception as e:
            logger.error(f"Error in research loop for job {job_id}: {e}", exc_info=True)
            
            # Update status to FAILED in database
            await db_update_job_status(job_id, "failed", None)
            
            # Broadcast error
            if connection_manager:
                asyncio.create_task(
                    connection_manager.broadcast_error(job_id, str(e))
                )
                asyncio.create_task(
                    connection_manager.broadcast_status(job_id, ResearchJobStatus.FAILED.value, None)
                )
    
    async def execute_research_step(
        self,
        job_id: str,
        step: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Execute a single research step.
        
        Args:
            job_id: The research job ID
            step: Step definition from AI agent
            
        Returns:
            Step execution results
        """
        # TODO: Parse step instruction from agent
        # TODO: Select appropriate tool(s) from registry
        # TODO: Execute tool with parameters
        # TODO: Return results to agent for next step
        
        from datetime import datetime
        
        logger.info(f"Executing research step {step.get('step', 'unknown')} for job {job_id}")
        return {
            "status": "completed",
            "action": step.get("action", "research"),
            "timestamp": datetime.utcnow().isoformat(),
            "step": step.get("step")
        }
    
    async def synthesize_results(
        self,
        job_id: str,
        sources: List[Dict[str, Any]]
    ) -> str:
        """
        Synthesize research results into final report.
        
        Args:
            job_id: The research job ID
            sources: List of gathered sources and findings (fallback if fetch fails)
            
        Returns:
            Final research report as formatted markdown text
        """
        logger.info(f"Synthesizing results for job {job_id}")
        
        try:
            # Fetch job to get the original query
            job_data = await get_job(job_id)
            if not job_data:
                logger.error(f"Job {job_id} not found for synthesis")
                return "# Research Report\n\nError: Could not retrieve job data."
            
            query = job_data.get("query", "Unknown query")
            
            # Fetch all sources from database (as per plan)
            try:
                sources = await get_sources_by_job(job_id)
                logger.info(f"Fetched {len(sources)} sources for synthesis")
            except Exception as e:
                logger.warning(f"Error fetching sources, using provided sources: {e}")
                # Use provided sources as fallback
                if not sources:
                    sources = []
            
            # Build sources list for prompt
            sources_list = []
            for idx, source in enumerate(sources, 1):
                source_info = f"[{idx}] {source.get('title', 'Untitled')}\n   URL: {source.get('url', 'N/A')}"
                if source.get('snippet'):
                    source_info += f"\n   Preview: {source.get('snippet', '')[:200]}"
                sources_list.append(source_info)
            
            sources_text = "\n".join(sources_list) if sources_list else "No sources were gathered during research."
            
            # Build synthesis prompt
            synthesis_prompt = f"""You are synthesizing a comprehensive investment research report based on gathered information.

Research Query: {query}

Sources Gathered ({len(sources)} total):
{sources_text}

Please generate a comprehensive, well-structured research report in markdown format that includes:

1. **Executive Summary** - A concise overview of key findings and conclusions
2. **Key Findings** - Main discoveries with source citations using [1], [2], etc. format
3. **Detailed Analysis** - In-depth examination of important aspects
4. **Risk Factors** - Potential risks and concerns identified
5. **Sources** - Numbered list of all sources with full URLs

Requirements:
- Use markdown formatting for structure and readability
- Cite sources using [1], [2], etc. format corresponding to the numbered sources above
- Be thorough and objective
- Include specific details and data points where available
- Maintain professional tone suitable for investment due diligence

Generate the report now:"""
            
            # Call Tongyi API for synthesis
            messages = [
                {
                    "role": "system",
                    "content": "You are an expert financial analyst specializing in investment research and due diligence reports."
                },
                {
                    "role": "user",
                    "content": synthesis_prompt
                }
            ]
            
            logger.info(f"Calling Tongyi API for report synthesis")
            response = await self.tongyi_client.chat_completion(
                messages=messages,
                tools=None  # No tools needed for synthesis
            )
            
            # Extract report content
            report_content = response.get("content", "")
            
            if not report_content or len(report_content.strip()) < 50:
                logger.warning("Synthesis returned empty or very short report, generating fallback")
                # Generate fallback report
                report_content = self._generate_fallback_report(query, sources)
            
            # Ensure sources section is included
            if "## Sources" not in report_content and "**Sources**" not in report_content:
                report_content += self._format_sources_section(sources)
            
            logger.info(f"Successfully synthesized report for job {job_id} ({len(report_content)} characters)")
            return report_content
            
        except Exception as e:
            logger.error(f"Error synthesizing results for job {job_id}: {e}", exc_info=True)
            # Return fallback report
            try:
                job_data = await get_job(job_id)
                query = job_data.get("query", "Unknown query") if job_data else "Unknown query"
            except Exception:
                query = "Unknown query"
            
            return self._generate_fallback_report(query, sources)
    
    def _generate_fallback_report(self, query: str, sources: List[Dict[str, Any]]) -> str:
        """Generate a fallback report when synthesis fails."""
        report = f"""# Research Report

## Executive Summary

Research was conducted on: {query}

## Key Findings

{len(sources)} source(s) were gathered during research, but automatic synthesis encountered an error. Please review the sources below for detailed information.

## Sources

{self._format_sources_section(sources)}
"""
        return report
    
    def _format_sources_section(self, sources: List[Dict[str, Any]]) -> str:
        """Format sources into a numbered list section."""
        if not sources:
            return "\n## Sources\n\nNo sources were gathered during research.\n"
        
        sources_text = "\n## Sources\n\n"
        for idx, source in enumerate(sources, 1):
            title = source.get("title", "Untitled")
            url = source.get("url", "")
            sources_text += f"{idx}. **{title}**\n   - {url}\n\n"
        
        return sources_text
    
    async def update_job_status(
        self,
        job_id: str,
        status: ResearchJobStatus,
        progress: Optional[float] = None,
        connection_manager: Optional["ConnectionManager"] = None
    ) -> None:
        """
        Update research job status.
        
        Args:
            job_id: The research job ID
            status: New status
            progress: Optional progress percentage (0-100)
            connection_manager: Optional WebSocket connection manager for real-time updates
        """
        # Update job status in database
        await db_update_job_status(job_id, status.value, progress)
        
        logger.info(f"Updating job {job_id} status to {status}")
        
        # Broadcast status update via WebSocket
        if connection_manager:
            asyncio.create_task(
                connection_manager.broadcast_status(job_id, status.value, progress)
            )
</file>

<file path="backend/app/tools/__init__.py">
"""
Research tools module.
"""
</file>

<file path="backend/README.md">
# Agent Bletchley Backend

FastAPI backend for the Agent Bletchley research system.

## Setup

### Prerequisites

- Python 3.11 or higher
- Virtual environment (recommended)

### Installation

**IMPORTANT: Run all commands from the `backend` directory!**

1. Navigate to the backend directory:
```powershell
cd backend
```

2. Create a virtual environment (if it doesn't exist):
```bash
python -m venv venv
```

3. Activate the virtual environment:
```powershell
# On Windows PowerShell (from backend directory):
.\venv\Scripts\Activate.ps1

# If you get an execution policy error, run this first:
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser

# On Windows Command Prompt:
venv\Scripts\activate

# On macOS/Linux:
source venv/bin/activate
```

4. Install dependencies:
```bash
pip install -r requirements.txt
```

5. Configure environment variables:
```bash
# On Windows PowerShell:
Copy-Item .env.example .env

# On macOS/Linux:
cp .env.example .env

# Edit .env with your API keys and configuration
```

### Required Environment Variables

- `OPENROUTER_API_KEY` - Your OpenRouter API key
- `BRAVE_SEARCH_API_KEY` - Your Brave Search API key
- `JINA_READER_API_KEY` - Your Jina Reader API key
- `SUPABASE_URL` - Your Supabase project URL
- `SUPABASE_KEY` - Your Supabase anon key

### Running the Server

Development server with auto-reload:
```bash
uvicorn app.main:app --reload
```

Production server:
```bash
uvicorn app.main:app --host 0.0.0.0 --port 8000
```

The API will be available at `http://localhost:8000`

### API Documentation

Once the server is running, visit:
- Swagger UI: `http://localhost:8000/docs`
- ReDoc: `http://localhost:8000/redoc`

### Project Structure

```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # FastAPI application entry point
â”‚   â”œâ”€â”€ config.py            # Configuration management
â”‚   â”œâ”€â”€ orchestrator/         # Research orchestration logic
â”‚   â”œâ”€â”€ tools/                # External API integrations
â”‚   â”œâ”€â”€ api/                  # REST API routes
â”‚   â””â”€â”€ models/               # Pydantic data models
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ README.md                 # This file
```

### Development

The codebase uses:
- Async/await patterns throughout
- Type hints on all functions
- TODO comments marking areas for implementation
- Proper logging configuration
</file>

<file path="backend/requirements.txt">
fastapi==0.115.0
uvicorn[standard]==0.32.0
httpx==0.27.2
python-dotenv==1.0.1
pydantic==2.9.2
websockets==13.1
supabase==2.8.0
</file>

<file path="backend/start.ps1">
# PowerShell script to start the backend development server
# Run this script from the backend directory: .\start.ps1

Write-Host "Agent Bletchley Backend - Starting..." -ForegroundColor Cyan

# Change to backend directory if not already there
$scriptPath = Split-Path -Parent $MyInvocation.MyCommand.Path
Set-Location $scriptPath

# Check if venv exists
if (-not (Test-Path "venv")) {
    Write-Host "Creating virtual environment..." -ForegroundColor Yellow
    python -m venv venv
}

# Activate virtual environment
Write-Host "Activating virtual environment..." -ForegroundColor Yellow
& "$scriptPath\venv\Scripts\Activate.ps1"

# Check if .env exists
if (-not (Test-Path ".env")) {
    Write-Host "Creating .env file from .env.example..." -ForegroundColor Yellow
    Copy-Item .env.example .env
    Write-Host "Please edit .env file with your API keys!" -ForegroundColor Red
}

# Install/upgrade dependencies
Write-Host "Installing dependencies..." -ForegroundColor Yellow
pip install -r requirements.txt

# Run the server
Write-Host "Starting FastAPI server..." -ForegroundColor Green
uvicorn app.main:app --reload
</file>

<file path="backend/test_websocket.py">
"""
Test script for WebSocket connection to the research job endpoint.
"""
import asyncio
import websockets
import json


async def test_websocket():
    """Test WebSocket connection to research job endpoint."""
    uri = "ws://localhost:8000/ws/research/test-job-123"
    
    # First, check if server is running by testing HTTP endpoint
    import httpx
    try:
        with httpx.Client(timeout=2.0) as client:
            resp = client.get("http://localhost:8000/health")
            if resp.status_code == 200:
                print("âœ“ Server is running")
            else:
                print(f"âš  Server responded with status {resp.status_code}")
    except httpx.ConnectError:
        print(f"âœ— Cannot reach server at http://localhost:8000")
        print(f"\nðŸ’¡ Make sure the server is running:")
        print(f"   cd backend")
        print(f"   .\\venv\\Scripts\\Activate.ps1")
        print(f"   uvicorn app.main:app --reload")
        return
    except Exception as e:
        print(f"âœ— Error checking server: {e}")
        print(f"\nðŸ’¡ Make sure the server is running:")
        print(f"   cd backend")
        print(f"   .\\venv\\Scripts\\Activate.ps1")
        print(f"   uvicorn app.main:app --reload")
        return
    
    try:
        print(f"\nConnecting to {uri}...")
        async with websockets.connect(uri, ping_interval=20, ping_timeout=10) as websocket:
            print("âœ“ Connected! Waiting for initial message from server...")
            
            # Create a task to receive messages
            async def receive_messages():
                try:
                    while True:
                        # Use a timeout to detect if connection is still alive
                        try:
                            message = await asyncio.wait_for(websocket.recv(), timeout=5.0)
                            data = json.loads(message)
                            print(f"\n[Received] Type: {data.get('type', 'unknown')}")
                            print(f"         Data: {json.dumps(data.get('data', {}), indent=2)}")
                        except asyncio.TimeoutError:
                            print("(Still waiting for messages...)")
                            continue
                except websockets.exceptions.ConnectionClosed as e:
                    print(f"\nâœ— Connection closed by server (code: {e.code}, reason: {e.reason})")
                    return
                except Exception as e:
                    print(f"\nâœ— Error receiving messages: {e}")
                    import traceback
                    traceback.print_exc()
                    return
            
            # Create a task to send ping messages periodically
            async def send_ping():
                await asyncio.sleep(1)  # Wait a bit before first ping
                ping_count = 0
                while True:
                    try:
                        ping_count += 1
                        await websocket.send(json.dumps({
                            "type": "ping", 
                            "message": f"Ping #{ping_count} from test client"
                        }))
                        print(f"\nâœ“ Sent ping #{ping_count}")
                        await asyncio.sleep(3)  # Send ping every 3 seconds
                    except websockets.exceptions.ConnectionClosed:
                        return
                    except Exception as e:
                        print(f"\nâœ— Error sending ping: {e}")
                        return
            
            # Run both tasks concurrently
            try:
                await asyncio.gather(
                    receive_messages(),
                    send_ping()
                )
            except KeyboardInterrupt:
                print("\n\nâœ“ Test interrupted by user (Ctrl+C)")
                print("âœ“ WebSocket connection test completed successfully!")
                
    except websockets.exceptions.InvalidURI:
        print(f"âœ— Invalid URI: {uri}")
    except websockets.exceptions.InvalidStatus as e:
        print(f"âœ— Connection failed with status {e.status_code}: {e.status_line}")
    except ConnectionRefusedError:
        print("âœ— Connection refused. Is the server running on port 8000?")
    except websockets.exceptions.ConnectionClosed as e:
        print(f"âœ— Connection closed (code: {e.code}, reason: {e.reason})")
    except Exception as e:
        print(f"\nâœ— Unexpected error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(test_websocket())
</file>

<file path="disable_rls.sql">
-- Disable Row Level Security on all research tables
-- This allows all operations without requiring RLS policies
-- Suitable for development/testing when authentication is not yet implemented

ALTER TABLE research_jobs DISABLE ROW LEVEL SECURITY;

ALTER TABLE research_iterations DISABLE ROW LEVEL SECURITY;

ALTER TABLE research_sources DISABLE ROW LEVEL SECURITY;
</file>

<file path="frontend/.env.local.example">
# Backend API URL
NEXT_PUBLIC_BACKEND_URL=http://localhost:8000

# WebSocket URL
NEXT_PUBLIC_WS_URL=ws://localhost:8000

# Supabase Configuration
NEXT_PUBLIC_SUPABASE_URL=your_supabase_project_url
NEXT_PUBLIC_SUPABASE_ANON_KEY=your_supabase_anon_key
</file>

<file path="frontend/app/api/research/route.ts">
/**
 * API route for proxying requests to the backend.
 */
import { NextRequest, NextResponse } from "next/server";

const BACKEND_URL = process.env.NEXT_PUBLIC_BACKEND_URL || "http://localhost:8000";

export async function GET(request: NextRequest) {
  try {
    // TODO: Implement GET request to backend
    const response = await fetch(`${BACKEND_URL}/api/research/jobs`);
    
    if (!response.ok) {
      throw new Error(`Backend API error: ${response.statusText}`);
    }
    
    const data = await response.json();
    return NextResponse.json(data);
  } catch (error) {
    console.error("API route error:", error);
    return NextResponse.json(
      { error: "Failed to fetch research jobs" },
      { status: 500 }
    );
  }
}

export async function POST(request: NextRequest) {
  try {
    const body = await request.json();
    
    // TODO: Implement POST request to backend
    const response = await fetch(`${BACKEND_URL}/api/research/jobs`, {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
      },
      body: JSON.stringify(body),
    });
    
    if (!response.ok) {
      throw new Error(`Backend API error: ${response.statusText}`);
    }
    
    const data = await response.json();
    return NextResponse.json(data);
  } catch (error) {
    console.error("API route error:", error);
    return NextResponse.json(
      { error: "Failed to create research job" },
      { status: 500 }
    );
  }
}
</file>

<file path="frontend/app/globals.css">
@tailwind base;
@tailwind components;
@tailwind utilities;

:root {
  --foreground-rgb: 0, 0, 0;
  --background-start-rgb: 214, 219, 220;
  --background-end-rgb: 255, 255, 255;
}

@media (prefers-color-scheme: dark) {
  :root {
    --foreground-rgb: 255, 255, 255;
    --background-start-rgb: 0, 0, 0;
    --background-end-rgb: 0, 0, 0;
  }
}

body {
  color: rgb(var(--foreground-rgb));
  background: linear-gradient(
      to bottom,
      transparent,
      rgb(var(--background-end-rgb))
    )
    rgb(var(--background-start-rgb));
}

@layer utilities {
  .text-balance {
    text-wrap: balance;
  }
}
</file>

<file path="frontend/app/layout.tsx">
/**
 * Root layout component with dark mode support.
 */
import type { Metadata } from "next";
import { Inter } from "next/font/google";
import "./globals.css";

const inter = Inter({ subsets: ["latin"] });

export const metadata: Metadata = {
  title: "Agent Bletchley",
  description: "Agentic web research system for investment due diligence",
};

export default function RootLayout({
  children,
}: Readonly<{
  children: React.ReactNode;
}>) {
  return (
    <html lang="en" suppressHydrationWarning>
      <body className={`${inter.className} antialiased`}>{children}</body>
    </html>
  );
}
</file>

<file path="frontend/components/IterationCard.tsx">
/**
 * Component for displaying a research iteration.
 */
import React from "react";
import type { ResearchIteration } from "@/types/research";

interface IterationCardProps {
  iteration: ResearchIteration;
}

export default function IterationCard({ iteration }: IterationCardProps) {
  return (
    <div className="rounded-lg border border-gray-200 dark:border-gray-700 bg-white dark:bg-gray-800 p-4 shadow-sm">
      <div className="flex items-center justify-between mb-2">
        <span className="text-sm font-medium text-gray-500 dark:text-gray-400">
          Step {iteration.step}
        </span>
        <span className="text-xs text-gray-400 dark:text-gray-500">
          {new Date(iteration.timestamp).toLocaleTimeString()}
        </span>
      </div>
      <p className="text-sm text-gray-900 dark:text-gray-100">{iteration.action}</p>
      {/* TODO: Display iteration results if available */}
    </div>
  );
}
</file>

<file path="frontend/components/OutputViewer.tsx">
/**
 * Component for displaying the final research output.
 */
import React from "react";

interface OutputViewerProps {
  report: string;
}

export default function OutputViewer({ report }: OutputViewerProps) {
  return (
    <div className="rounded-lg border border-gray-200 dark:border-gray-700 bg-white dark:bg-gray-800 p-6">
      <h3 className="text-lg font-semibold text-gray-900 dark:text-gray-100 mb-4">
        Research Report
      </h3>
      <div className="prose dark:prose-invert max-w-none">
        <div className="whitespace-pre-wrap text-gray-700 dark:text-gray-300">
          {report}
        </div>
      </div>
    </div>
  );
}
</file>

<file path="frontend/components/ProgressBar.tsx">
/**
 * Animated progress bar component.
 */
"use client";

import React from "react";
import { motion } from "framer-motion";

interface ProgressBarProps {
  progress: number;
  status?: string;
}

export default function ProgressBar({ progress, status }: ProgressBarProps) {
  return (
    <div className="w-full">
      <div className="flex items-center justify-between mb-2">
        <span className="text-sm font-medium text-gray-700 dark:text-gray-300">
          {status || "Progress"}
        </span>
        <span className="text-sm text-gray-500 dark:text-gray-400">{Math.round(progress)}%</span>
      </div>
      <div className="w-full bg-gray-200 dark:bg-gray-700 rounded-full h-2 overflow-hidden">
        <motion.div
          className="h-full bg-blue-600 dark:bg-blue-500 rounded-full"
          initial={{ width: 0 }}
          animate={{ width: `${progress}%` }}
          transition={{ duration: 0.3, ease: "easeOut" }}
        />
      </div>
    </div>
  );
}
</file>

<file path="frontend/components/SourcePanel.tsx">
/**
 * Component for displaying research sources.
 */
import React from "react";
import type { Source } from "@/types/research";

interface SourcePanelProps {
  sources: Source[];
}

export default function SourcePanel({ sources }: SourcePanelProps) {
  if (sources.length === 0) {
    return (
      <div className="text-center py-8 text-gray-500 dark:text-gray-400">
        No sources found yet.
      </div>
    );
  }

  return (
    <div className="space-y-3">
      {sources.map((source, index) => (
        <div
          key={index}
          className="rounded-lg border border-gray-200 dark:border-gray-700 bg-white dark:bg-gray-800 p-4 hover:shadow-md transition-shadow"
        >
          <a
            href={source.url}
            target="_blank"
            rel="noopener noreferrer"
            className="text-sm font-medium text-blue-600 dark:text-blue-400 hover:underline block mb-1"
          >
            {source.title}
          </a>
          {source.snippet && (
            <p className="text-xs text-gray-600 dark:text-gray-300 line-clamp-2">
              {source.snippet}
            </p>
          )}
          {source.fetched_at && (
            <p className="text-xs text-gray-400 dark:text-gray-500 mt-2">
              Fetched: {new Date(source.fetched_at).toLocaleString()}
            </p>
          )}
        </div>
      ))}
    </div>
  );
}
</file>

<file path="frontend/next-env.d.ts">
/// <reference types="next" />
/// <reference types="next/image-types/global" />
import "./.next/dev/types/routes.d.ts";

// NOTE: This file should not be edited
// see https://nextjs.org/docs/app/api-reference/config/typescript for more information.
</file>

<file path="frontend/next.config.js">
/** @type {import('next').NextConfig} */
const nextConfig = {
  reactStrictMode: true,
  typescript: {
    // TODO: Set to false once all TypeScript errors are resolved
    ignoreBuildErrors: false,
  },
  eslint: {
    // TODO: Set to false once all ESLint errors are resolved
    ignoreDuringBuilds: false,
  },
};

module.exports = nextConfig;
</file>

<file path="frontend/package.json">
{
  "name": "agent-bletchley-frontend",
  "version": "1.0.0",
  "private": true,
  "scripts": {
    "dev": "next dev",
    "build": "next build",
    "start": "next start",
    "lint": "next lint"
  },
  "dependencies": {
    "next": "^16.0.0",
    "react": "^18.3.1",
    "react-dom": "^18.3.1",
    "typescript": "^5.6.0",
    "tailwindcss": "^3.4.0",
    "framer-motion": "^11.0.0",
    "@supabase/supabase-js": "^2.39.0"
  },
  "devDependencies": {
    "@types/node": "^20.11.0",
    "@types/react": "^18.3.0",
    "@types/react-dom": "^18.3.0",
    "autoprefixer": "^10.4.17",
    "postcss": "^8.4.33",
    "eslint": "^9.0.0",
    "eslint-config-next": "^16.0.0"
  }
}
</file>

<file path="frontend/postcss.config.js">
module.exports = {
  plugins: {
    tailwindcss: {},
    autoprefixer: {},
  },
};
</file>

<file path="frontend/README.md">
# Agent Bletchley Frontend

Next.js 16 frontend application for the Agent Bletchley research system.

## Setup

### Prerequisites

- Node.js 18 or higher
- npm or yarn

### Installation

1. Install dependencies:
```bash
npm install
```

2. Configure environment variables:
```bash
cp .env.local.example .env.local
# Edit .env.local with your configuration
```

### Required Environment Variables

- `NEXT_PUBLIC_BACKEND_URL` - Backend API URL (default: http://localhost:8000)
- `NEXT_PUBLIC_WS_URL` - WebSocket URL (default: ws://localhost:8000)
- `NEXT_PUBLIC_SUPABASE_URL` - Your Supabase project URL
- `NEXT_PUBLIC_SUPABASE_ANON_KEY` - Your Supabase anon key

### Running the Development Server

```bash
npm run dev
```

The frontend will be available at `http://localhost:3000`

### Building for Production

```bash
npm run build
npm start
```

### Project Structure

```
frontend/
â”œâ”€â”€ app/                    # Next.js App Router pages
â”‚   â”œâ”€â”€ layout.tsx         # Root layout
â”‚   â”œâ”€â”€ page.tsx           # Home page
â”‚   â”œâ”€â”€ research/          # Research dashboard pages
â”‚   â””â”€â”€ api/               # API routes
â”œâ”€â”€ components/            # React components
â”œâ”€â”€ lib/                   # Utility libraries
â”œâ”€â”€ types/                 # TypeScript type definitions
â”œâ”€â”€ package.json           # Dependencies
â””â”€â”€ README.md              # This file
```

### Features

- **Next.js 16 App Router** - Modern routing and server components
- **TypeScript** - Type-safe development
- **Tailwind CSS** - Utility-first CSS with dark mode support
- **Framer Motion** - Animations and transitions
- **Supabase** - Database integration
- **WebSocket** - Real-time updates for research jobs

### Development

The codebase uses:
- TypeScript strict mode
- App Router (not Pages Router)
- Tailwind CSS with dark mode
- Proper error boundaries
- TypeScript interfaces for all props
</file>

<file path="frontend/tailwind.config.ts">
import type { Config } from "tailwindcss";

const config: Config = {
  content: [
    "./pages/**/*.{js,ts,jsx,tsx,mdx}",
    "./components/**/*.{js,ts,jsx,tsx,mdx}",
    "./app/**/*.{js,ts,jsx,tsx,mdx}",
  ],
  darkMode: "class",
  theme: {
    extend: {
      colors: {
        background: "var(--background)",
        foreground: "var(--foreground)",
      },
    },
  },
  plugins: [],
};
export default config;
</file>

<file path="frontend/tsconfig.json">
{
  "compilerOptions": {
    "target": "ES2020",
    "lib": [
      "dom",
      "dom.iterable",
      "esnext"
    ],
    "allowJs": true,
    "skipLibCheck": true,
    "strict": true,
    "noEmit": true,
    "esModuleInterop": true,
    "module": "esnext",
    "moduleResolution": "bundler",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "jsx": "react-jsx",
    "incremental": true,
    "plugins": [
      {
        "name": "next"
      }
    ],
    "paths": {
      "@/*": [
        "./*"
      ]
    }
  },
  "include": [
    "next-env.d.ts",
    "**/*.ts",
    "**/*.tsx",
    ".next/types/**/*.ts",
    ".next/dev/types/**/*.ts"
  ],
  "exclude": [
    "node_modules"
  ]
}
</file>

<file path="frontend/types/research.ts">
/**
 * TypeScript types for research jobs and related data structures.
 */

export enum ResearchJobStatus {
  PENDING = "pending",
  RUNNING = "running",
  COMPLETED = "completed",
  FAILED = "failed",
  CANCELLED = "cancelled",
}

export interface ResearchJobCreate {
  query: string;
  context?: Record<string, unknown>;
}

export interface ResearchJob {
  id: string;
  query: string;
  status: ResearchJobStatus;
  progress: number;
  created_at: string;
  updated_at?: string;
  completed_at?: string;
  sources: Source[];
  iterations: ResearchIteration[];
  report?: string;
  error?: string;
}

export interface Source {
  url: string;
  title: string;
  snippet?: string;
  fetched_at?: string;
  content?: string;
}

export interface ResearchIteration {
  id: string;
  step: number;
  action: string;
  timestamp: string;
  results?: unknown;
}

export interface WebSocketMessage {
  type: "status" | "progress" | "iteration" | "source" | "report" | "error";
  job_id: string;
  data: unknown;
}
</file>

<file path="MIGRATION_INSTRUCTIONS.md">
# Running Supabase Migration via Dashboard

## Step-by-Step Instructions

### Step 1: Access Supabase Dashboard
1. Go to: https://supabase.com/dashboard
2. Log in to your account (if not already logged in)

### Step 2: Select Your Project
1. In the project list, find and click on: **pdkyyrvikbnizgtmzzhz**
   - Or look for your project name if it's different

### Step 3: Open SQL Editor
1. In the **left sidebar**, find and click: **"SQL Editor"** (it has a database icon)
2. You'll see a query editor interface

### Step 4: Create New Query
1. Click the **"New Query"** button at the top (or click the "+" icon if available)
2. This opens a blank SQL editor pane

### Step 5: Copy and Paste the Migration SQL
Copy the entire SQL below and paste it into the editor:

```sql
-- Initial database schema for Agent Bletchley
-- Run this migration in your Supabase SQL editor

-- Research jobs table
CREATE TABLE IF NOT EXISTS research_jobs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    query TEXT NOT NULL,
    status TEXT NOT NULL DEFAULT 'pending' CHECK (status IN ('pending', 'running', 'completed', 'failed', 'cancelled')),
    progress DECIMAL(5, 2) DEFAULT 0.0 CHECK (progress >= 0.0 AND progress <= 100.0),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE,
    completed_at TIMESTAMP WITH TIME ZONE,
    report TEXT,
    error TEXT,
    context JSONB DEFAULT '{}'::jsonb
);

-- Research iterations table
CREATE TABLE IF NOT EXISTS research_iterations (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    job_id UUID NOT NULL REFERENCES research_jobs(id) ON DELETE CASCADE,
    step INTEGER NOT NULL,
    action TEXT NOT NULL,
    results JSONB DEFAULT '{}'::jsonb,
    timestamp TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    CONSTRAINT unique_job_step UNIQUE (job_id, step)
);

-- Research sources table
CREATE TABLE IF NOT EXISTS research_sources (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    job_id UUID NOT NULL REFERENCES research_jobs(id) ON DELETE CASCADE,
    url TEXT NOT NULL,
    title TEXT,
    snippet TEXT,
    content TEXT,
    fetched_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    UNIQUE(job_id, url)
);

-- Indexes for better query performance
CREATE INDEX IF NOT EXISTS idx_research_jobs_status ON research_jobs(status);
CREATE INDEX IF NOT EXISTS idx_research_jobs_created_at ON research_jobs(created_at DESC);
CREATE INDEX IF NOT EXISTS idx_research_iterations_job_id ON research_iterations(job_id);
CREATE INDEX IF NOT EXISTS idx_research_sources_job_id ON research_sources(job_id);

-- Function to automatically update updated_at timestamp
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = NOW();
    RETURN NEW;
END;
$$ language 'plpgsql';

-- Trigger to update updated_at on research_jobs
CREATE TRIGGER update_research_jobs_updated_at
    BEFORE UPDATE ON research_jobs
    FOR EACH ROW
    EXECUTE FUNCTION update_updated_at_column();

-- Enable Row Level Security (RLS)
ALTER TABLE research_jobs ENABLE ROW LEVEL SECURITY;
ALTER TABLE research_iterations ENABLE ROW LEVEL SECURITY;
ALTER TABLE research_sources ENABLE ROW LEVEL SECURITY;

-- TODO: Create RLS policies based on your authentication requirements
-- Example policy (adjust as needed):
-- CREATE POLICY "Users can view their own research jobs"
--     ON research_jobs FOR SELECT
--     USING (auth.uid() = user_id);
--
-- CREATE POLICY "Users can create research jobs"
--     ON research_jobs FOR INSERT
--     WITH CHECK (true);
--
-- CREATE POLICY "Users can update their own research jobs"
--     ON research_jobs FOR UPDATE
--     USING (auth.uid() = user_id);
```

### Step 6: Run the Migration
1. Review the SQL to make sure it's all there
2. Click the **"Run"** button (usually green, in the bottom right)
   - OR press **Ctrl+Enter** (Windows) or **Cmd+Enter** (Mac)
3. Wait for execution to complete (usually takes a few seconds)

### Step 7: Verify Success
You should see:
- âœ… **Success message** at the bottom (like "Success. No rows returned")
- âœ… No error messages in red
- âœ… A message indicating how many statements were executed

### Step 8: Verify Tables Were Created
1. In the left sidebar, click **"Table Editor"**
2. You should now see three new tables:
   - `research_jobs`
   - `research_iterations`
   - `research_sources`

## What This Migration Does

This migration creates:
- âœ… **research_jobs** table - Stores research job information (query, status, progress, report)
- âœ… **research_iterations** table - Tracks each step/iteration of the research process
- âœ… **research_sources** table - Stores URLs and content of research sources
- âœ… **Indexes** - For faster queries
- âœ… **Auto-update trigger** - Automatically updates `updated_at` timestamps
- âœ… **Row Level Security (RLS)** - Enabled on all tables (you'll need to add policies later)

## Troubleshooting

**If you see errors:**
- Make sure you copied the entire SQL (all 81 lines)
- Check that there are no syntax errors
- Ensure you're connected to the correct project
- Some statements might show warnings if tables already exist (that's okay - the `IF NOT EXISTS` clause handles it)

**If tables don't appear:**
- Refresh the Table Editor page
- Check the SQL Editor output for any error messages
- Make sure the migration completed successfully

## Next Steps

After running this migration, you may want to:
1. Create RLS policies (see the commented examples in the SQL)
2. Test inserting a sample research job
3. Verify the trigger works by updating a job
</file>

<file path="QUICKSTART.md">
# Quick Start Guide - Windows PowerShell

## Backend Setup (from project root)

```powershell
# 1. Navigate to backend directory
cd backend

# 2. Activate virtual environment
.\venv\Scripts\Activate.ps1

# If you get execution policy error, run this first:
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser

# 3. Install dependencies (if not already installed)
pip install -r requirements.txt

# 4. Create .env file (if not exists)
Copy-Item .env.example .env
# Then edit .env with your API keys

# 5. Start the server
uvicorn app.main:app --reload
```

## Alternative: Use the startup script

From the `backend` directory:
```powershell
.\start.ps1
```

## Verify it's working

Once the server starts, visit:
- API: http://localhost:8000
- Should return: `{"message": "Hello Agent Bletchley"}`
- Docs: http://localhost:8000/docs

## Troubleshooting

**Problem**: `Activate.ps1` not found
- **Solution**: Make sure you're in the `backend` directory, not the root

**Problem**: Execution policy error
- **Solution**: Run `Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser`

**Problem**: `uvicorn` not found
- **Solution**: Make sure venv is activated (you should see `(venv)` in your prompt)

**Problem**: `requirements.txt` not found
- **Solution**: Make sure you're in the `backend` directory
</file>

<file path="run_migration_dashboard.py">
"""
Helper script to display the migration SQL for easy copy-paste into Supabase Dashboard.
This is the recommended way to run migrations on remote Supabase instances.
"""
from pathlib import Path

def display_migration():
    """Display the migration SQL with instructions."""
    migration_file = Path(__file__).parent / "supabase" / "migrations" / "001_initial_schema.sql"
    
    if not migration_file.exists():
        print(f"Error: Migration file not found at {migration_file}")
        return
    
    print("=" * 80)
    print("SUPABASE MIGRATION: 001_initial_schema.sql")
    print("=" * 80)
    print("\nINSTRUCTIONS:")
    print("1. Go to your Supabase Dashboard: https://supabase.com/dashboard")
    print("2. Select your project: pdkyyrvikbnizgtmzzhz")
    print("3. Navigate to: SQL Editor (left sidebar)")
    print("4. Click: 'New Query'")
    print("5. Copy and paste the SQL below into the editor")
    print("6. Click: 'Run' (or press Ctrl+Enter)")
    print("\n" + "=" * 80)
    print("SQL TO COPY:")
    print("=" * 80 + "\n")
    
    with open(migration_file, 'r', encoding='utf-8') as f:
        sql_content = f.read()
    
    print(sql_content)
    
    print("\n" + "=" * 80)
    print("MIGRATION SUMMARY:")
    print("=" * 80)
    print("This migration will create:")
    print("  - research_jobs table (stores research job information)")
    print("  - research_iterations table (tracks each step of research)")
    print("  - research_sources table (stores research sources/URLs)")
    print("  - Indexes for better query performance")
    print("  - Automatic timestamp update function and trigger")
    print("  - Row Level Security (RLS) enabled on all tables")
    print("\nAfter running, remember to create RLS policies as needed!")
    print("=" * 80)

if __name__ == "__main__":
    display_migration()
</file>

<file path="run-backend.snip">
cd backend
.\venv\Scripts\Activate.ps1
uvicorn app.main:app --reload --log-level=debug
</file>

<file path="supabase/migrations/001_initial_schema.sql">
-- Initial database schema for Agent Bletchley
-- Run this migration in your Supabase SQL editor

-- Research jobs table
CREATE TABLE IF NOT EXISTS research_jobs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    query TEXT NOT NULL,
    status TEXT NOT NULL DEFAULT 'pending' CHECK (status IN ('pending', 'running', 'completed', 'failed', 'cancelled')),
    progress DECIMAL(5, 2) DEFAULT 0.0 CHECK (progress >= 0.0 AND progress <= 100.0),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE,
    completed_at TIMESTAMP WITH TIME ZONE,
    report TEXT,
    error TEXT,
    context JSONB DEFAULT '{}'::jsonb
);

-- Research iterations table
CREATE TABLE IF NOT EXISTS research_iterations (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    job_id UUID NOT NULL REFERENCES research_jobs(id) ON DELETE CASCADE,
    step INTEGER NOT NULL,
    action TEXT NOT NULL,
    results JSONB DEFAULT '{}'::jsonb,
    timestamp TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    CONSTRAINT unique_job_step UNIQUE (job_id, step)
);

-- Research sources table
CREATE TABLE IF NOT EXISTS research_sources (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    job_id UUID NOT NULL REFERENCES research_jobs(id) ON DELETE CASCADE,
    url TEXT NOT NULL,
    title TEXT,
    snippet TEXT,
    content TEXT,
    fetched_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    UNIQUE(job_id, url)
);

-- Indexes for better query performance
CREATE INDEX IF NOT EXISTS idx_research_jobs_status ON research_jobs(status);
CREATE INDEX IF NOT EXISTS idx_research_jobs_created_at ON research_jobs(created_at DESC);
CREATE INDEX IF NOT EXISTS idx_research_iterations_job_id ON research_iterations(job_id);
CREATE INDEX IF NOT EXISTS idx_research_sources_job_id ON research_sources(job_id);

-- Function to automatically update updated_at timestamp
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = NOW();
    RETURN NEW;
END;
$$ language 'plpgsql';

-- Trigger to update updated_at on research_jobs
CREATE TRIGGER update_research_jobs_updated_at
    BEFORE UPDATE ON research_jobs
    FOR EACH ROW
    EXECUTE FUNCTION update_updated_at_column();

-- Enable Row Level Security (RLS)
ALTER TABLE research_jobs ENABLE ROW LEVEL SECURITY;
ALTER TABLE research_iterations ENABLE ROW LEVEL SECURITY;
ALTER TABLE research_sources ENABLE ROW LEVEL SECURITY;

-- TODO: Create RLS policies based on your authentication requirements
-- Example policy (adjust as needed):
-- CREATE POLICY "Users can view their own research jobs"
--     ON research_jobs FOR SELECT
--     USING (auth.uid() = user_id);
--
-- CREATE POLICY "Users can create research jobs"
--     ON research_jobs FOR INSERT
--     WITH CHECK (true);
--
-- CREATE POLICY "Users can update their own research jobs"
--     ON research_jobs FOR UPDATE
--     USING (auth.uid() = user_id);
</file>

<file path="backend/.env.example">
# OpenRouter API Configuration
OPENROUTER_API_KEY=your_openrouter_api_key_here
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
TONGYI_MODEL=alibaba/tongyi-deepresearch-30b-a3b

# Brave Search API
BRAVE_SEARCH_API_KEY=your_brave_search_api_key_here

# Jina Reader API
JINA_READER_API_KEY=your_jina_reader_api_key_here

# Supabase Configuration
SUPABASE_URL=your_supabase_project_url
SUPABASE_KEY=your_supabase_anon_key

# Server Configuration
BACKEND_HOST=0.0.0.0
BACKEND_PORT=8000

# Frontend URL (for CORS)
FRONTEND_URL=http://localhost:3000

# Logging
LOG_LEVEL=INFO

Supabase Password: "#63XZm^k*IlaABX4"
</file>

<file path="backend/app/api/routes.py">
"""
REST API routes for research jobs.
"""
import logging
from datetime import datetime
from typing import List
from fastapi import APIRouter, HTTPException, BackgroundTasks
from app.models.research_job import ResearchJob, ResearchJobCreate, ResearchJobStatus
from app.orchestrator.research_engine import ResearchEngine
from app.api.websocket import manager as connection_manager
from app.db.supabase_client import create_job, get_job, list_jobs

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api/research", tags=["research"])
research_engine = ResearchEngine()


@router.post("/jobs", response_model=ResearchJob)
async def create_research_job(
    job_data: ResearchJobCreate,
    background_tasks: BackgroundTasks
) -> ResearchJob:
    """
    Create a new research job.
    
    Args:
        job_data: Research job creation data
        background_tasks: FastAPI background tasks
        
    Returns:
        Created research job
    """
    # Store job in database
    job_id = await create_job(job_data.query, job_data.context)
    logger.info(f"Created research job {job_id} with query: {job_data.query}")
    
    # Start research in background with WebSocket support
    background_tasks.add_task(
        research_engine.start_research,
        job_data.query,
        job_id,
        connection_manager
    )
    
    # Fetch and return the created job
    db_job = await get_job(job_id)
    if not db_job:
        raise HTTPException(status_code=500, detail="Failed to retrieve created job")
    
    # Convert database dict to ResearchJob model
    job = _dict_to_research_job(db_job)
    return job


@router.get("/jobs/{job_id}", response_model=ResearchJob)
async def get_research_job(job_id: str) -> ResearchJob:
    """
    Get a research job by ID.
    
    Args:
        job_id: Research job ID
        
    Returns:
        Research job
    """
    # Fetch job from database
    db_job = await get_job(job_id)
    
    if not db_job:
        raise HTTPException(status_code=404, detail="Job not found")
    
    # Convert database dict to ResearchJob model
    job = _dict_to_research_job(db_job)
    return job


@router.get("/jobs", response_model=List[ResearchJob])
async def list_research_jobs(
    skip: int = 0,
    limit: int = 100
) -> List[ResearchJob]:
    """
    List all research jobs.
    
    Args:
        skip: Number of jobs to skip
        limit: Maximum number of jobs to return
        
    Returns:
        List of research jobs
    """
    # Fetch jobs from database with pagination
    db_jobs = await list_jobs(skip, limit)
    
    # Convert list of dicts to list of ResearchJob models
    jobs = [_dict_to_research_job(db_job) for db_job in db_jobs]
    return jobs


def _dict_to_research_job(db_job: dict) -> ResearchJob:
    """
    Convert database dictionary to ResearchJob model.
    
    Args:
        db_job: Database job dictionary
        
    Returns:
        ResearchJob model instance
    """
    # Parse timestamps
    created_at = db_job.get("created_at")
    if isinstance(created_at, str):
        created_at = datetime.fromisoformat(created_at.replace("Z", "+00:00"))
    
    updated_at = db_job.get("updated_at")
    if updated_at and isinstance(updated_at, str):
        updated_at = datetime.fromisoformat(updated_at.replace("Z", "+00:00"))
    
    completed_at = db_job.get("completed_at")
    if completed_at and isinstance(completed_at, str):
        completed_at = datetime.fromisoformat(completed_at.replace("Z", "+00:00"))
    
    return ResearchJob(
        id=str(db_job["id"]),
        query=db_job["query"],
        status=ResearchJobStatus(db_job["status"]),
        progress=float(db_job.get("progress", 0.0)),
        created_at=created_at or datetime.utcnow(),
        updated_at=updated_at,
        completed_at=completed_at,
        sources=db_job.get("sources", []),
        iterations=db_job.get("iterations", []),
        report=db_job.get("report"),
        error=db_job.get("error")
    )


@router.delete("/jobs/{job_id}")
async def delete_research_job(job_id: str) -> dict:
    """
    Delete a research job.
    
    Args:
        job_id: Research job ID
        
    Returns:
        Success message
    """
    # TODO: Delete job from database
    
    return {"message": "Job deleted"}
</file>

<file path="backend/app/api/websocket.py">
"""
WebSocket handler for real-time research updates.
"""
import logging
import json
from typing import Set, Dict, Optional
from collections import defaultdict
from fastapi import WebSocket, WebSocketDisconnect

logger = logging.getLogger(__name__)


class ConnectionManager:
    """Manages WebSocket connections for real-time updates."""
    
    def __init__(self):
        """Initialize the connection manager."""
        self.active_connections: Dict[str, Set[WebSocket]] = defaultdict(set)
    
    async def connect(self, websocket: WebSocket, job_id: str) -> None:
        """
        Accept a new WebSocket connection and associate it with a job_id.
        
        Args:
            websocket: WebSocket connection
            job_id: Research job ID to associate with this connection
        """
        await websocket.accept()
        self.active_connections[job_id].add(websocket)
        logger.info(f"WebSocket connected for job {job_id}. Total connections for job: {len(self.active_connections[job_id])}")
    
    def disconnect(self, websocket: WebSocket, job_id: str) -> None:
        """
        Remove a WebSocket connection from the specified job.
        
        Args:
            websocket: WebSocket connection to remove
            job_id: Research job ID associated with this connection
        """
        if job_id in self.active_connections:
            self.active_connections[job_id].discard(websocket)
            # Clean up empty sets
            if not self.active_connections[job_id]:
                del self.active_connections[job_id]
        logger.info(f"WebSocket disconnected for job {job_id}")
    
    async def send_personal_message(self, message: dict, websocket: WebSocket) -> None:
        """Send a message to a specific WebSocket connection."""
        try:
            await websocket.send_json(message)
        except Exception as e:
            logger.error(f"Error sending WebSocket message: {e}")
    
    async def _broadcast_to_job(self, job_id: str, message: dict) -> None:
        """
        Internal method to broadcast a message to all connections for a specific job.
        
        Args:
            job_id: Research job ID to broadcast to
            message: Message dictionary to send
        """
        if job_id not in self.active_connections:
            logger.debug(f"No active connections for job {job_id}")
            return
        
        disconnected = set()
        connections = self.active_connections[job_id].copy()  # Copy to avoid modification during iteration
        
        for connection in connections:
            try:
                await connection.send_json(message)
            except Exception as e:
                logger.error(f"Error broadcasting to WebSocket for job {job_id}: {e}")
                disconnected.add(connection)
        
        # Remove disconnected connections
        for connection in disconnected:
            self.active_connections[job_id].discard(connection)
        
        # Clean up empty sets
        if not self.active_connections[job_id]:
            del self.active_connections[job_id]
    
    async def broadcast_status(self, job_id: str, status: str, progress: Optional[float] = None) -> None:
        """
        Broadcast status update to all clients connected to a specific job.
        
        Args:
            job_id: Research job ID
            status: Job status (pending, running, completed, failed, cancelled)
            progress: Optional progress percentage (0-100)
        """
        message = {
            "type": "status",
            "job_id": job_id,
            "data": {
                "status": status,
                "progress": progress
            }
        }
        await self._broadcast_to_job(job_id, message)
    
    async def broadcast_iteration(self, job_id: str, iteration_data: dict) -> None:
        """
        Broadcast iteration update to all clients connected to a specific job.
        
        Args:
            job_id: Research job ID
            iteration_data: Dictionary containing iteration information
        """
        message = {
            "type": "iteration",
            "job_id": job_id,
            "data": iteration_data
        }
        await self._broadcast_to_job(job_id, message)
    
    async def broadcast_source(self, job_id: str, source_data: dict) -> None:
        """
        Broadcast source discovery update to all clients connected to a specific job.
        
        Args:
            job_id: Research job ID
            source_data: Dictionary containing source information
        """
        message = {
            "type": "source",
            "job_id": job_id,
            "data": source_data
        }
        await self._broadcast_to_job(job_id, message)
    
    async def broadcast_report(self, job_id: str, report: str) -> None:
        """
        Broadcast final report to all clients connected to a specific job.
        
        Args:
            job_id: Research job ID
            report: Final research report text
        """
        message = {
            "type": "report",
            "job_id": job_id,
            "data": {
                "report": report
            }
        }
        await self._broadcast_to_job(job_id, message)
    
    async def broadcast_error(self, job_id: str, error_message: str) -> None:
        """
        Broadcast error message to all clients connected to a specific job.
        
        Args:
            job_id: Research job ID
            error_message: Error message to send
        """
        message = {
            "type": "error",
            "job_id": job_id,
            "data": {
                "error": error_message
            }
        }
        await self._broadcast_to_job(job_id, message)
    
    async def broadcast(self, message: dict) -> None:
        """
        Broadcast a message to all connected WebSocket clients (legacy method).
        Note: Prefer using job-specific broadcast methods instead.
        
        Args:
            message: Message dictionary to send
        """
        job_id = message.get("job_id")
        if job_id:
            await self._broadcast_to_job(job_id, message)
        else:
            logger.warning("Broadcast message missing job_id, skipping")


manager = ConnectionManager()


async def websocket_endpoint(websocket: WebSocket, job_id: str) -> None:
    """
    WebSocket endpoint for real-time research job updates.
    
    Args:
        websocket: WebSocket connection
        job_id: Research job ID to subscribe to
    """
    try:
        await manager.connect(websocket, job_id)
        logger.info(f"WebSocket connection established for job {job_id}")
    except Exception as e:
        logger.error(f"Error accepting WebSocket connection for job {job_id}: {e}", exc_info=True)
        return
    
    try:
        # Send initial connection confirmation
        try:
            await manager.send_personal_message({
                "type": "connected",
                "job_id": job_id,
                "data": {"message": "WebSocket connected successfully"}
            }, websocket)
            logger.info(f"Sent initial connection message for job {job_id}")
        except Exception as e:
            logger.error(f"Error sending initial connection message for job {job_id}: {e}", exc_info=True)
            # Don't return, continue to the message loop
        
        # Keep connection alive and listen for messages
        while True:
            try:
                # Wait for message from client (this will block until a message is received)
                data = await websocket.receive_text()
                logger.debug(f"Received raw message for job {job_id}: {data[:100]}...")
                
                try:
                    message = json.loads(data)
                    logger.info(f"Received WebSocket message for job {job_id}: {message}")
                    
                    # Handle client messages (ping, pause, cancel, etc.)
                    msg_type = message.get("type", "")
                    if msg_type == "ping":
                        # Respond to ping with pong
                        try:
                            await manager.send_personal_message({
                                "type": "pong",
                                "job_id": job_id,
                                "data": {"message": "pong"}
                            }, websocket)
                            logger.debug(f"Sent pong response for job {job_id}")
                        except Exception as e:
                            logger.error(f"Error sending pong for job {job_id}: {e}", exc_info=True)
                    else:
                        # Echo other messages back or handle them
                        logger.debug(f"Unhandled message type: {msg_type} for job {job_id}")
                        
                except json.JSONDecodeError as e:
                    logger.error(f"Invalid JSON received from client for job {job_id}: {e}. Data: {data[:100]}")
                    try:
                        await manager.send_personal_message({
                            "type": "error",
                            "job_id": job_id,
                            "data": {"error": "Invalid JSON format"}
                        }, websocket)
                    except Exception as send_error:
                        logger.error(f"Error sending error message for job {job_id}: {send_error}", exc_info=True)
                    
            except WebSocketDisconnect:
                # This is expected when client disconnects
                logger.info(f"WebSocket disconnected normally for job {job_id}")
                break
            except Exception as e:
                logger.error(f"Error processing WebSocket message for job {job_id}: {e}", exc_info=True)
                # Continue the loop to keep connection alive unless it's a disconnect
                # Check if websocket is still connected
                try:
                    # Try to send an error message to see if connection is still alive
                    await manager.send_personal_message({
                        "type": "error",
                        "job_id": job_id,
                        "data": {"error": "Internal error processing message"}
                    }, websocket)
                except Exception:
                    # Connection is likely dead, break out of loop
                    logger.warning(f"Connection appears to be dead for job {job_id}, breaking loop")
                    break
                continue
            
    except WebSocketDisconnect:
        logger.info(f"WebSocket disconnected for job {job_id}")
    except Exception as e:
        logger.error(f"Unexpected error in WebSocket endpoint for job {job_id}: {e}", exc_info=True)
    finally:
        # Always clean up the connection
        try:
            manager.disconnect(websocket, job_id)
        except Exception as e:
            logger.error(f"Error during WebSocket disconnect cleanup for job {job_id}: {e}", exc_info=True)
</file>

<file path="backend/app/config.py">
"""
Configuration management using environment variables.
"""
import os
from typing import Optional
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()


class Settings:
    """Application settings loaded from environment variables."""
    
    # API Keys
    OPENROUTER_API_KEY: str = os.getenv("OPENROUTER_API_KEY", "")
    BRAVE_SEARCH_API_KEY: str = os.getenv("BRAVE_SEARCH_API_KEY", "")
    JINA_READER_API_KEY: str = os.getenv("JINA_READER_API_KEY", "")
    
    # Supabase
    SUPABASE_URL: str = os.getenv("SUPABASE_URL", "")
    SUPABASE_KEY: str = os.getenv("SUPABASE_KEY", "")
    
    # OpenRouter Configuration
    OPENROUTER_BASE_URL: str = os.getenv("OPENROUTER_BASE_URL", "https://openrouter.ai/api/v1")
    TONGYI_MODEL: str = os.getenv("TONGYI_MODEL", "alibaba/tongyi-deepresearch-30b-a3b")
    
    # Server Configuration
    BACKEND_HOST: str = os.getenv("BACKEND_HOST", "0.0.0.0")
    BACKEND_PORT: int = int(os.getenv("BACKEND_PORT", "8000"))
    
    # CORS
    FRONTEND_URL: str = os.getenv("FRONTEND_URL", "http://localhost:3000")
    
    # Logging
    LOG_LEVEL: str = os.getenv("LOG_LEVEL", "INFO")
    
    @classmethod
    def validate(cls) -> None:
        """Validate that required environment variables are set."""
        required_vars = [
            "OPENROUTER_API_KEY",
            "BRAVE_SEARCH_API_KEY",
            "JINA_READER_API_KEY",
            "SUPABASE_URL",
            "SUPABASE_KEY",
        ]
        missing = [var for var in required_vars if not getattr(cls, var)]
        if missing:
            raise ValueError(f"Missing required environment variables: {', '.join(missing)}")


# Global settings instance
settings = Settings()
</file>

<file path="backend/app/db/supabase_client.py">
"""
Supabase database client for Agent Bletchley research jobs.
"""
import asyncio
import logging
from typing import Dict, List, Optional, Any
from uuid import UUID
from supabase import create_client, Client
from app.config import settings

logger = logging.getLogger(__name__)

# Initialize Supabase client singleton
_supabase_client: Optional[Client] = None


def get_client() -> Client:
    """Get or create Supabase client instance."""
    global _supabase_client
    if _supabase_client is None:
        if not settings.SUPABASE_URL or not settings.SUPABASE_KEY:
            raise ValueError("SUPABASE_URL and SUPABASE_KEY must be set in environment variables")
        _supabase_client = create_client(settings.SUPABASE_URL, settings.SUPABASE_KEY)
        logger.info("Initialized Supabase client")
    return _supabase_client


async def create_job(query: str, context: Optional[Dict[str, Any]] = None) -> str:
    """
    Create a new research job in the database.
    
    Args:
        query: The research query
        context: Optional context dictionary
        
    Returns:
        Job ID as string
    """
    try:
        client = get_client()
        
        job_data = {
            "query": query,
            "status": "pending",
            "progress": 0.0,
            "context": context or {}
        }
        
        response = await asyncio.to_thread(
            lambda: client.table("research_jobs").insert(job_data).execute()
        )
        
        if response.data and len(response.data) > 0:
            job_id = str(response.data[0]["id"])
            logger.info(f"Created research job {job_id} with query: {query}")
            return job_id
        else:
            raise ValueError("No data returned from database insert")
            
    except Exception as e:
        logger.error(f"Error creating research job: {e}", exc_info=True)
        raise


async def update_job_status(
    job_id: str, 
    status: str, 
    progress: Optional[float] = None
) -> None:
    """
    Update research job status and progress.
    
    Args:
        job_id: The job ID
        status: New status (pending, running, completed, failed, cancelled)
        progress: Optional progress percentage (0-100)
    """
    try:
        client = get_client()
        
        update_data: Dict[str, Any] = {"status": status}
        
        if progress is not None:
            # Ensure progress is within bounds
            update_data["progress"] = max(0.0, min(100.0, float(progress)))
        
        # Set completed_at timestamp if status is "completed"
        if status == "completed":
            from datetime import datetime, timezone
            update_data["completed_at"] = datetime.now(timezone.utc).isoformat()
        
        await asyncio.to_thread(
            lambda: client.table("research_jobs")
            .update(update_data)
            .eq("id", job_id)
            .execute()
        )
        
        logger.info(f"Updated job {job_id} status to {status}" + 
                   (f" with progress {progress}" if progress is not None else ""))
        
    except Exception as e:
        logger.error(f"Error updating job {job_id} status: {e}", exc_info=True)
        raise


async def update_job_report(job_id: str, report: str) -> None:
    """
    Update research job report.
    
    Args:
        job_id: The job ID
        report: The final research report
    """
    try:
        client = get_client()
        
        await asyncio.to_thread(
            lambda: client.table("research_jobs")
            .update({"report": report})
            .eq("id", job_id)
            .execute()
        )
        
        logger.info(f"Updated job {job_id} report")
        
    except Exception as e:
        logger.error(f"Error updating job {job_id} report: {e}", exc_info=True)
        raise


async def add_iteration(
    job_id: str, 
    step: int, 
    action: str, 
    results: Optional[Dict[str, Any]] = None
) -> str:
    """
    Add a research iteration to the database.
    
    Args:
        job_id: The job ID
        step: Step number
        action: Action description
        results: Optional results dictionary
        
    Returns:
        Iteration ID as string
    """
    try:
        client = get_client()
        
        iteration_data = {
            "job_id": job_id,
            "step": step,
            "action": action,
            "results": results or {}
        }
        
        response = await asyncio.to_thread(
            lambda: client.table("research_iterations")
            .insert(iteration_data)
            .execute()
        )
        
        if response.data and len(response.data) > 0:
            iteration_id = str(response.data[0]["id"])
            logger.info(f"Added iteration {iteration_id} for job {job_id}, step {step}")
            return iteration_id
        else:
            raise ValueError("No data returned from database insert")
            
    except Exception as e:
        logger.error(f"Error adding iteration for job {job_id}: {e}", exc_info=True)
        raise


async def add_source(
    job_id: str,
    url: str,
    title: Optional[str] = None,
    snippet: Optional[str] = None,
    content: Optional[str] = None
) -> str:
    """
    Add a research source to the database (with upsert for duplicate URLs).
    
    Args:
        job_id: The job ID
        url: Source URL
        title: Optional source title
        snippet: Optional snippet
        content: Optional full content
        
    Returns:
        Source ID as string
    """
    try:
        client = get_client()
        
        source_data = {
            "job_id": job_id,
            "url": url,
            "title": title,
            "snippet": snippet,
            "content": content
        }
        
        # Use upsert to handle duplicate URLs per job
        # The UNIQUE constraint on (job_id, url) will handle conflicts
        # Supabase will automatically detect the unique constraint
        try:
            # Try insert first
            response = await asyncio.to_thread(
                lambda: client.table("research_sources")
                .insert(source_data)
                .execute()
            )
        except Exception as insert_error:
            # If insert fails due to duplicate, try update
            if "duplicate" in str(insert_error).lower() or "unique" in str(insert_error).lower():
                # Update existing source
                response = await asyncio.to_thread(
                    lambda: client.table("research_sources")
                    .update(source_data)
                    .eq("job_id", job_id)
                    .eq("url", url)
                    .execute()
                )
            else:
                raise
        
        if response.data and len(response.data) > 0:
            source_id = str(response.data[0]["id"])
            logger.info(f"Added/updated source {source_id} for job {job_id}: {url}")
            return source_id
        else:
            raise ValueError("No data returned from database upsert")
            
    except Exception as e:
        logger.error(f"Error adding source for job {job_id}: {e}", exc_info=True)
        raise


async def get_sources_by_job(job_id: str) -> List[Dict[str, Any]]:
    """
    Get all sources for a research job.
    
    Args:
        job_id: The job ID
        
    Returns:
        List of source dictionaries, or empty list on error
    """
    try:
        client = get_client()
        
        response = await asyncio.to_thread(
            lambda: client.table("research_sources")
            .select("*")
            .eq("job_id", job_id)
            .order("fetched_at")
            .execute()
        )
        
        if response.data:
            # Convert UUIDs to strings for consistency
            sources = []
            for source in response.data:
                source["id"] = str(source["id"])
                source["job_id"] = str(source["job_id"])
                sources.append(source)
            logger.info(f"Fetched {len(sources)} sources for job {job_id}")
            return sources
        else:
            return []
            
    except Exception as e:
        logger.error(f"Error fetching sources for job {job_id}: {e}", exc_info=True)
        return []


async def get_job(job_id: str) -> Optional[Dict[str, Any]]:
    """
    Get a research job with related iterations and sources.
    
    Args:
        job_id: The job ID
        
    Returns:
        Complete job data as dictionary, or None if not found
    """
    try:
        client = get_client()
        
        # Fetch job
        job_response = await asyncio.to_thread(
            lambda: client.table("research_jobs")
            .select("*")
            .eq("id", job_id)
            .execute()
        )
        
        if not job_response.data or len(job_response.data) == 0:
            logger.warning(f"Job {job_id} not found")
            return None
        
        job = job_response.data[0]
        
        # Convert UUID to string
        job["id"] = str(job["id"])
        
        # Fetch iterations
        iterations_response = await asyncio.to_thread(
            lambda: client.table("research_iterations")
            .select("*")
            .eq("job_id", job_id)
            .order("step")
            .execute()
        )
        
        iterations = []
        if iterations_response.data:
            for iteration in iterations_response.data:
                iteration["id"] = str(iteration["id"])
                iteration["job_id"] = str(iteration["job_id"])
                iterations.append(iteration)
        
        # Fetch sources
        sources_response = await asyncio.to_thread(
            lambda: client.table("research_sources")
            .select("*")
            .eq("job_id", job_id)
            .order("fetched_at")
            .execute()
        )
        
        sources = []
        if sources_response.data:
            for source in sources_response.data:
                source["id"] = str(source["id"])
                source["job_id"] = str(source["job_id"])
                sources.append(source)
        
        # Combine job with iterations and sources
        job["iterations"] = iterations
        job["sources"] = sources
        
        logger.info(f"Retrieved job {job_id} with {len(iterations)} iterations and {len(sources)} sources")
        return job
        
    except Exception as e:
        logger.error(f"Error getting job {job_id}: {e}", exc_info=True)
        raise


async def list_jobs(skip: int = 0, limit: int = 100) -> List[Dict[str, Any]]:
    """
    List research jobs with pagination.
    
    Args:
        skip: Number of jobs to skip
        limit: Maximum number of jobs to return
        
    Returns:
        List of job dictionaries
    """
    try:
        client = get_client()
        
        response = await asyncio.to_thread(
            lambda: client.table("research_jobs")
            .select("*")
            .order("created_at", desc=True)
            .range(skip, skip + limit - 1)
            .execute()
        )
        
        jobs = []
        if response.data:
            for job in response.data:
                job["id"] = str(job["id"])
                jobs.append(job)
        
        logger.info(f"Listed {len(jobs)} jobs (skip={skip}, limit={limit})")
        return jobs
        
    except Exception as e:
        logger.error(f"Error listing jobs: {e}", exc_info=True)
        raise
</file>

<file path="backend/app/orchestrator/tongyi_client.py">
"""
OpenRouter/Tongyi DeepResearch client interface.
"""
import logging
import asyncio
import json
import httpx
from typing import Dict, List, Any, Optional
from app.config import settings

logger = logging.getLogger(__name__)


class TongyiClient:
    """
    Client for interacting with OpenRouter API for Tongyi DeepResearch.
    
    TODO: Implement full integration with OpenRouter API:
    1. Chat completions with tool calling support
    2. Streaming responses for real-time updates
    3. Tool definitions from ToolRegistry
    4. Error handling and retries
    """
    
    def __init__(self):
        """Initialize the Tongyi client."""
        self.api_key = settings.OPENROUTER_API_KEY
        self.base_url = settings.OPENROUTER_BASE_URL
        self.model = settings.TONGYI_MODEL
        self.client = httpx.AsyncClient(
            base_url=self.base_url,
            headers={
                "Authorization": f"Bearer {self.api_key}",
                "HTTP-Referer": "https://github.com/agent-bletchley",
                "X-Title": "Agent Bletchley",
            },
            timeout=60.0,
        )
    
    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        tools: Optional[List[Dict[str, Any]]] = None,
        stream: bool = False,
        timeout: Optional[float] = None
    ) -> Dict[str, Any]:
        """
        Send chat completion request to OpenRouter with retry logic and tool call parsing.
        
        Args:
            messages: List of message dictionaries with 'role' and 'content'
            tools: Optional list of tool definitions for function calling
            stream: Whether to stream the response (not implemented yet)
            timeout: Optional timeout in seconds (defaults to client's default timeout)
            
        Returns:
            Dictionary with keys:
            - tool_calls: List of tool call dictionaries (if any)
            - content: Final answer content (if no tool calls)
            - message: Full message object from API
            
        Raises:
            httpx.HTTPError: For API failures after retries
            ValueError: For invalid responses or missing fields
        """
        if stream:
            logger.warning("Streaming not yet implemented, falling back to non-streaming")
        
        payload = {
            "model": self.model,
            "messages": messages,
        }
        
        if tools:
            payload["tools"] = tools
            # Set tool_choice to "auto" to enable tool calling
            # This ensures the model will actually invoke tools when appropriate
            payload["tool_choice"] = "auto"
        
        # Use provided timeout or default client timeout
        request_timeout = timeout if timeout is not None else None
        
        # Exponential backoff retry logic: 1s, 2s, 4s delays, max 3 attempts
        max_attempts = 3
        delays = [1.0, 2.0, 4.0]
        last_exception = None
        
        for attempt in range(max_attempts):
            try:
                logger.info(f"Sending chat completion request to {self.model} (attempt {attempt + 1}/{max_attempts})")
                if request_timeout:
                    logger.debug(f"Using custom timeout: {request_timeout}s")
                
                # Log request payload for debugging (mask sensitive data)
                payload_log = payload.copy()
                if "messages" in payload_log:
                    payload_log["messages"] = [
                        {**msg, "content": msg.get("content", "")[:100] + "..." if len(msg.get("content", "")) > 100 else msg.get("content", "")}
                        for msg in payload_log["messages"]
                    ]
                logger.debug(f"Request payload: {json.dumps(payload_log, indent=2)}")
                
                # Create timeout for this request if specified
                if request_timeout:
                    timeout_obj = httpx.Timeout(request_timeout)
                    response = await self.client.post("/chat/completions", json=payload, timeout=timeout_obj)
                else:
                    response = await self.client.post("/chat/completions", json=payload)
                
                # Handle rate limiting (429) with special delay
                if response.status_code == 429:
                    retry_after = float(response.headers.get("Retry-After", delays[min(attempt, len(delays) - 1)]))
                    if attempt < max_attempts - 1:
                        logger.warning(f"Rate limited (429), retrying after {retry_after}s")
                        await asyncio.sleep(retry_after)
                        continue
                    else:
                        response.raise_for_status()
                
                response.raise_for_status()
                
                # Parse JSON response
                try:
                    response_data = response.json()
                except json.JSONDecodeError as e:
                    logger.error(f"Failed to parse JSON response. Status: {response.status_code}")
                    logger.error(f"Response headers: {dict(response.headers)}")
                    logger.error(f"Response body (first 2000 chars): {response.text[:2000]}")
                    raise ValueError(f"Invalid JSON response from API: {e}")
                
                # Validate response structure
                if not response_data or "choices" not in response_data:
                    # DIAGNOSTIC LOGGING - Root cause investigation
                    logger.error(f"Missing 'choices' field in response. Status: {response.status_code}")
                    logger.error(f"Response headers: {dict(response.headers)}")
                    logger.error(f"Response keys: {list(response_data.keys()) if isinstance(response_data, dict) else 'Not a dict'}")
                    logger.error(f"Response body (first 2000 chars): {json.dumps(response_data, indent=2)[:2000]}")
                    
                    # Check for error fields
                    if isinstance(response_data, dict):
                        if "error" in response_data:
                            error_info = response_data.get("error", {})
                            logger.error(f"API error found: {error_info}")
                            raise ValueError(f"API error: {error_info.get('message', 'Unknown error')}")
                        if "message" in response_data:
                            logger.error(f"Response message: {response_data.get('message')}")
                    
                    raise ValueError("Invalid response structure: missing 'choices' field")
                
                if not response_data["choices"] or len(response_data["choices"]) == 0:
                    raise ValueError("Invalid response structure: empty 'choices' array")
                
                choice = response_data["choices"][0]
                if "message" not in choice:
                    raise ValueError("Invalid response structure: missing 'message' field in choice")
                
                message = choice["message"]
                
                # Log raw response for debugging
                logger.debug(f"Raw API response message: {json.dumps(message, indent=2)}")
                
                # Parse tool calls
                tool_calls = message.get("tool_calls", [])
                
                # Detect if content contains tool call descriptions instead of actual tool calls
                content = message.get("content", "")
                if content and not tool_calls:
                    # Check if content contains tool call patterns (likely a description instead of actual call)
                    tool_call_indicators = [
                        '"arguments"',
                        '"function"',
                        '"name"',
                        'tool_call',
                        'function_call'
                    ]
                    if any(indicator in content for indicator in tool_call_indicators):
                        logger.warning(
                            f"Content appears to contain tool call description instead of actual tool call. "
                            f"Content preview: {content[:200]}..."
                        )
                
                # Parse tool call arguments (may be string or dict)
                parsed_tool_calls = []
                for tool_call in tool_calls:
                    # Validate tool call structure
                    if not isinstance(tool_call, dict):
                        logger.warning(f"Invalid tool call structure (not a dict): {tool_call}")
                        continue
                    
                    if "function" not in tool_call:
                        logger.warning(f"Tool call missing 'function' field: {tool_call}")
                        continue
                    
                    parsed_call = tool_call.copy()
                    if "arguments" in parsed_call["function"]:
                        args = parsed_call["function"]["arguments"]
                        if isinstance(args, str):
                            try:
                                parsed_call["function"]["arguments"] = json.loads(args)
                            except json.JSONDecodeError:
                                logger.warning(f"Failed to parse tool call arguments as JSON: {args}")
                                parsed_call["function"]["arguments"] = {}
                    parsed_tool_calls.append(parsed_call)
                
                result = {
                    "tool_calls": parsed_tool_calls if parsed_tool_calls else [],
                    "content": content if content else "",
                    "message": message,
                    "usage": response_data.get("usage", {}),
                }
                
                logger.info(f"Chat completion successful. Tool calls: {len(parsed_tool_calls)}, Content length: {len(content)}")
                return result
                
            except httpx.HTTPStatusError as e:
                last_exception = e
                status_code = e.response.status_code if e.response else None
                
                # Don't retry on 4xx errors (except 429 which is handled above)
                if status_code and 400 <= status_code < 500 and status_code != 429:
                    logger.error(f"Client error {status_code}: {e.response.text if e.response else str(e)}")
                    raise
                
                # Retry on 5xx errors and network issues
                if attempt < max_attempts - 1:
                    delay = delays[min(attempt, len(delays) - 1)]
                    logger.warning(f"HTTP error {status_code}, retrying after {delay}s: {e}")
                    await asyncio.sleep(delay)
                else:
                    logger.error(f"HTTP error after {max_attempts} attempts: {e}")
                    raise
                    
            except httpx.RequestError as e:
                last_exception = e
                if attempt < max_attempts - 1:
                    delay = delays[min(attempt, len(delays) - 1)]
                    logger.warning(f"Request error, retrying after {delay}s: {e}")
                    await asyncio.sleep(delay)
                else:
                    logger.error(f"Request error after {max_attempts} attempts: {e}")
                    raise
                    
            except httpx.TimeoutException as e:
                last_exception = e
                if attempt < max_attempts - 1:
                    delay = delays[min(attempt, len(delays) - 1)]
                    logger.warning(f"Timeout error, retrying after {delay}s: {e}")
                    await asyncio.sleep(delay)
                else:
                    logger.error(f"Timeout error after {max_attempts} attempts: {e}")
                    raise
                    
            except (ValueError, KeyError) as e:
                # Don't retry on parsing/validation errors
                logger.error(f"Response parsing error: {e}")
                raise
                
            except Exception as e:
                last_exception = e
                if attempt < max_attempts - 1:
                    delay = delays[min(attempt, len(delays) - 1)]
                    logger.warning(f"Unexpected error, retrying after {delay}s: {e}")
                    await asyncio.sleep(delay)
                else:
                    logger.error(f"Unexpected error after {max_attempts} attempts: {e}", exc_info=True)
                    raise
        
        # Should never reach here, but just in case
        if last_exception:
            raise last_exception
        raise RuntimeError("Failed to complete request after all retries")
    
    async def send_research_query(
        self,
        query: str,
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Send a research query to the Tongyi agent.
        
        Args:
            query: The research question/query
            context: Optional context from previous research steps
            
        Returns:
            Agent response with potential tool calls
        """
        # TODO: Format research query with context
        # TODO: Include available tools from ToolRegistry
        # TODO: Parse agent response for tool calls or final answer
        
        messages = [
            {
                "role": "system",
                "content": "You are a research assistant for investment due diligence. Use available tools to gather comprehensive information."
            },
            {
                "role": "user",
                "content": query
            }
        ]
        
        tools = []  # TODO: Get tools from ToolRegistry
        
        return await self.chat_completion(messages=messages, tools=tools)
    
    async def close(self) -> None:
        """Close the HTTP client."""
        await self.client.aclose()
</file>

<file path="backend/app/tools/tool_registry.py">
"""
Tool registry for defining available tools for the AI agent.
"""
from typing import List, Dict, Any
from app.tools.web_search import WebSearchTool
from app.tools.web_fetch import WebFetchTool


class ToolRegistry:
    """
    Registry of available tools for the research agent.
    
    Defines tool schemas compatible with OpenRouter function calling.
    """
    
    def __init__(self):
        """Initialize the tool registry."""
        self.web_search = WebSearchTool()
        self.web_fetch = WebFetchTool()
    
    def get_tool_definitions(self) -> List[Dict[str, Any]]:
        """
        Get tool definitions in OpenRouter function calling format.
        
        Returns:
            List of tool definition dictionaries
        """
        return [
            {
                "type": "function",
                "function": {
                    "name": "web_search",
                    "description": (
                        "INVOKE THIS TOOL to search the web for information using Brave Search. "
                        "You MUST call this function (not describe it) when you need to find sources. "
                        "Returns search results with URLs, titles, and snippets. "
                        "Use this FIRST to discover relevant articles and pages about a topic."
                    ),
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "query": {
                                "type": "string",
                                "description": "The search query string"
                            },
                            "count": {
                                "type": "integer",
                                "description": "Number of results to return (default: 10, max: 20)",
                                "default": 10
                            }
                        },
                        "required": ["query"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "web_fetch",
                    "description": (
                        "INVOKE THIS TOOL to fetch and parse content from a web URL. "
                        "You MUST call this function (not describe it) when you need to read full article content. "
                        "Returns the parsed text content, title, and metadata. "
                        "Use this AFTER web_search to read articles and pages you found."
                    ),
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "url": {
                                "type": "string",
                                "description": "The URL to fetch and parse"
                            },
                            "mode": {
                                "type": "string",
                                "description": "Reader mode: 'reader' for parsed content, 'raw' for raw HTML",
                                "enum": ["reader", "raw"],
                                "default": "reader"
                            }
                        },
                        "required": ["url"]
                    }
                }
            }
        ]
    
    async def execute_tool(
        self,
        tool_name: str,
        parameters: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Execute a tool by name with given parameters.
        
        Args:
            tool_name: Name of the tool to execute
            parameters: Tool parameters
            
        Returns:
            Tool execution result
        """
        if tool_name == "web_search":
            query = parameters.get("query")
            count = parameters.get("count", 10)
            results = await self.web_search.search(query=query, count=count)
            return {
                "tool": "web_search",
                "results": results
            }
        elif tool_name == "web_fetch":
            url = parameters.get("url")
            mode = parameters.get("mode", "reader")
            content = await self.web_fetch.fetch(url=url, mode=mode)
            return {
                "tool": "web_fetch",
                "content": content
            }
        else:
            raise ValueError(f"Unknown tool: {tool_name}")
</file>

<file path="frontend/app/page.tsx">
"use client";

import React, { useState, useEffect, useRef } from "react";
import { useRouter } from "next/navigation";
import type { ResearchJob, ResearchJobStatus } from "@/types/research";
import { createResearchJob, listResearchJobs } from "@/lib/api";

const EXAMPLE_QUERIES = [
  "Analyze the competitive landscape for quantum computing startups",
  "What are the key risks and opportunities in African fintech?",
  "Deep dive on AI regulation in healthcare",
  "Evaluate market opportunity for eVTOL aircraft",
  "Research semiconductor supply chain vulnerabilities",
  "Climate tech investment trends in 2025",
];

function formatRelativeTime(timestamp: string): string {
  const now = new Date();
  const time = new Date(timestamp);
  const diffInSeconds = Math.floor((now.getTime() - time.getTime()) / 1000);

  if (diffInSeconds < 60) {
    return "just now";
  }

  const diffInMinutes = Math.floor(diffInSeconds / 60);
  if (diffInMinutes < 60) {
    return `${diffInMinutes} minute${diffInMinutes > 1 ? "s" : ""} ago`;
  }

  const diffInHours = Math.floor(diffInMinutes / 60);
  if (diffInHours < 24) {
    return `${diffInHours} hour${diffInHours > 1 ? "s" : ""} ago`;
  }

  const diffInDays = Math.floor(diffInHours / 24);
  if (diffInDays < 7) {
    return `${diffInDays} day${diffInDays > 1 ? "s" : ""} ago`;
  }

  const diffInWeeks = Math.floor(diffInDays / 7);
  if (diffInWeeks < 4) {
    return `${diffInWeeks} week${diffInWeeks > 1 ? "s" : ""} ago`;
  }

  const diffInMonths = Math.floor(diffInDays / 30);
  return `${diffInMonths} month${diffInMonths > 1 ? "s" : ""} ago`;
}

function getStatusBadgeClass(status: ResearchJobStatus): string {
  switch (status) {
    case "pending":
      return "bg-gray-100 text-gray-800 dark:bg-gray-800 dark:text-gray-200";
    case "running":
      return "bg-blue-100 text-blue-800 dark:bg-blue-900 dark:text-blue-200";
    case "completed":
      return "bg-green-100 text-green-800 dark:bg-green-900 dark:text-green-200";
    case "failed":
      return "bg-red-100 text-red-800 dark:bg-red-900 dark:text-red-200";
    case "cancelled":
      return "bg-yellow-100 text-yellow-800 dark:bg-yellow-900 dark:text-yellow-200";
    default:
      return "bg-gray-100 text-gray-800 dark:bg-gray-800 dark:text-gray-200";
  }
}

export default function Home() {
  const router = useRouter();
  const textareaRef = useRef<HTMLTextAreaElement>(null);
  const [query, setQuery] = useState("");
  const [isSubmitting, setIsSubmitting] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const [recentJobs, setRecentJobs] = useState<ResearchJob[]>([]);
  const [loading, setLoading] = useState(true);

  // Auto-focus textarea on mount
  useEffect(() => {
    textareaRef.current?.focus();
  }, []);

  // Fetch recent jobs on mount
  useEffect(() => {
    const fetchJobs = async () => {
      try {
        setLoading(true);
        const jobs = await listResearchJobs(10);
        setRecentJobs(jobs);
      } catch (err) {
        console.error("Failed to fetch recent jobs:", err);
      } finally {
        setLoading(false);
      }
    };

    fetchJobs();
  }, []);

  // Clear error when user types
  useEffect(() => {
    if (error && query.length > 0) {
      setError(null);
    }
  }, [query, error]);

  const handleSubmit = async (e: React.FormEvent) => {
    e.preventDefault();
    if (query.length < 20) {
      setError("Query must be at least 20 characters");
      return;
    }

    setIsSubmitting(true);
    setError(null);

    try {
      const job = await createResearchJob({ query });
      router.push(`/research/${job.id}`);
    } catch (err) {
      setError(err instanceof Error ? err.message : "Failed to start research");
      setIsSubmitting(false);
    }
  };

  const handleExampleClick = (exampleQuery: string) => {
    setQuery(exampleQuery);
    setError(null);
    textareaRef.current?.focus();
  };

  const handleKeyDown = (e: React.KeyboardEvent<HTMLTextAreaElement>) => {
    if ((e.metaKey || e.ctrlKey) && e.key === "Enter") {
      e.preventDefault();
      if (query.length >= 20 && !isSubmitting) {
        handleSubmit(e);
      }
    }
  };

  const truncatedQuery = (text: string) => {
    return text.length > 100 ? text.substring(0, 100) + "..." : text;
  };

  return (
    <main className="min-h-screen bg-gradient-to-br from-gray-50 via-white to-gray-50 dark:from-gray-900 dark:via-gray-800 dark:to-gray-900">
      <div className="container mx-auto px-4 py-12 max-w-6xl">
        {/* Hero Section */}
        <div className="text-center mb-12">
          <h1 className="text-6xl font-bold mb-4 bg-gradient-to-r from-purple-600 via-purple-500 to-blue-600 bg-clip-text text-transparent">
            Agent Bletchley
          </h1>
          <h2 className="text-2xl font-semibold text-gray-700 dark:text-gray-300 mb-3">
            Autonomous Investment Research Powered by AI
          </h2>
          <p className="text-lg text-gray-600 dark:text-gray-400">
            Deep research in minutes, not hours. Powered by Tongyi DeepResearch.
          </p>
        </div>

        {/* Research Query Form */}
        <div className="mb-12">
          <form onSubmit={handleSubmit} className="space-y-4">
            <div>
              <textarea
                ref={textareaRef}
                value={query}
                onChange={(e) => setQuery(e.target.value)}
                onKeyDown={handleKeyDown}
                rows={5}
                placeholder="Enter your research query here... e.g., 'Analyze the competitive landscape for quantum computing startups'"
                className="w-full px-4 py-3 text-lg border border-gray-300 dark:border-gray-600 rounded-lg focus:ring-2 focus:ring-purple-500 focus:border-transparent bg-white dark:bg-gray-800 text-gray-900 dark:text-white placeholder-gray-500 dark:placeholder-gray-400 resize-none transition-all duration-200"
              />
              <div className="mt-2 flex items-center justify-between">
                <p className="text-sm text-gray-500 dark:text-gray-400">
                  {query.length < 20 ? (
                    <span className="text-orange-600 dark:text-orange-400">
                      Minimum 20 characters ({query.length}/20)
                    </span>
                  ) : (
                    <span className="text-green-600 dark:text-green-400">
                      {query.length} characters
                    </span>
                  )}
                </p>
                <p className="text-xs text-gray-400 dark:text-gray-500">
                  Press Cmd/Ctrl+Enter to submit
                </p>
              </div>
            </div>

            {error && (
              <div className="p-4 bg-red-50 dark:bg-red-900/20 border border-red-200 dark:border-red-800 rounded-lg">
                <p className="text-red-800 dark:text-red-200 text-sm">{error}</p>
              </div>
            )}

            <button
              type="submit"
              disabled={query.length < 20 || isSubmitting}
              className="w-full py-4 px-6 bg-gradient-to-r from-purple-600 to-blue-600 hover:from-purple-700 hover:to-blue-700 disabled:from-gray-400 disabled:to-gray-500 disabled:cursor-not-allowed text-white font-semibold rounded-lg shadow-lg hover:shadow-xl transition-all duration-200 flex items-center justify-center gap-2"
            >
              {isSubmitting ? (
                <>
                  <svg
                    className="animate-spin h-5 w-5 text-white"
                    xmlns="http://www.w3.org/2000/svg"
                    fill="none"
                    viewBox="0 0 24 24"
                  >
                    <circle
                      className="opacity-25"
                      cx="12"
                      cy="12"
                      r="10"
                      stroke="currentColor"
                      strokeWidth="4"
                    ></circle>
                    <path
                      className="opacity-75"
                      fill="currentColor"
                      d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"
                    ></path>
                  </svg>
                  <span>Researching...</span>
                </>
              ) : (
                <>
                  <span>Start Research</span>
                  <span>â†’</span>
                </>
              )}
            </button>
          </form>
        </div>

        {/* Example Queries */}
        <div className="mb-12">
          <h3 className="text-lg font-semibold text-gray-700 dark:text-gray-300 mb-4">
            Example Queries
          </h3>
          <div className="flex flex-wrap gap-3">
            {EXAMPLE_QUERIES.map((exampleQuery, index) => (
              <button
                key={index}
                onClick={() => handleExampleClick(exampleQuery)}
                className="px-4 py-2 bg-white dark:bg-gray-800 border border-gray-300 dark:border-gray-600 rounded-full text-sm text-gray-700 dark:text-gray-300 hover:bg-purple-50 dark:hover:bg-purple-900/20 hover:border-purple-300 dark:hover:border-purple-700 hover:text-purple-700 dark:hover:text-purple-400 transition-all duration-200 cursor-pointer"
              >
                {exampleQuery}
              </button>
            ))}
          </div>
        </div>

        {/* Recent Research Section */}
        <div>
          <h3 className="text-2xl font-bold text-gray-900 dark:text-white mb-6">
            Recent Research
          </h3>

          {loading ? (
            <div className="space-y-4">
              {[1, 2, 3].map((i) => (
                <div
                  key={i}
                  className="animate-pulse bg-white dark:bg-gray-800 rounded-lg border border-gray-200 dark:border-gray-700 p-6 h-24"
                ></div>
              ))}
            </div>
          ) : recentJobs.length === 0 ? (
            <div className="text-center py-12 bg-white dark:bg-gray-800 rounded-lg border border-gray-200 dark:border-gray-700">
              <p className="text-gray-500 dark:text-gray-400">
                No research jobs yet. Start your first investigation above!
              </p>
            </div>
          ) : (
            <div className="grid grid-cols-1 md:grid-cols-2 gap-4">
              {recentJobs.map((job) => (
                <button
                  key={job.id}
                  onClick={() => router.push(`/research/${job.id}`)}
                  className="text-left p-6 bg-white dark:bg-gray-800 rounded-lg border border-gray-200 dark:border-gray-700 shadow-sm hover:shadow-md transition-all duration-200 hover:border-purple-300 dark:hover:border-purple-700 cursor-pointer group"
                >
                  <div className="flex items-start justify-between mb-3">
                    <span
                      className={`px-3 py-1 rounded-full text-xs font-medium ${getStatusBadgeClass(
                        job.status
                      )}`}
                    >
                      {job.status}
                    </span>
                    <span className="text-xs text-gray-400 dark:text-gray-500">
                      {formatRelativeTime(job.created_at)}
                    </span>
                  </div>
                  <p className="text-gray-900 dark:text-white group-hover:text-purple-600 dark:group-hover:text-purple-400 transition-colors duration-200">
                    {truncatedQuery(job.query)}
                  </p>
                </button>
              ))}
            </div>
          )}
        </div>
      </div>
    </main>
  );
}
</file>

<file path="frontend/app/research/[jobId]/page.tsx">
/**
 * Research dashboard page for viewing a specific research job.
 */
"use client";

import React, { useEffect, useState, useCallback } from "react";
import { useParams } from "next/navigation";
import type { ResearchJob, WebSocketMessage } from "@/types/research";
import ProgressBar from "@/components/ProgressBar";
import IterationCard from "@/components/IterationCard";
import SourcePanel from "@/components/SourcePanel";
import OutputViewer from "@/components/OutputViewer";
import { useWebSocket } from "@/lib/websocket";
import { getResearchJob, ApiError } from "@/lib/api";

export default function ResearchJobPage() {
  const params = useParams();
  const jobId = params.jobId as string;
  const [job, setJob] = useState<ResearchJob | null>(null);
  const [loading, setLoading] = useState(true);
  const [error, setError] = useState<string | null>(null);

  // Fetch initial job data
  useEffect(() => {
    let cancelled = false;

    const fetchJob = async () => {
      if (!jobId) {
        setLoading(false);
        return;
      }

      try {
        setLoading(true);
        setError(null);
        const jobData = await getResearchJob(jobId);
        
        if (!cancelled) {
          setJob(jobData);
          setLoading(false);
        }
      } catch (err) {
        if (!cancelled) {
          if (err instanceof ApiError) {
            if (err.status === 404) {
              setError("Job not found");
            } else {
              setError(`Failed to fetch job: ${err.message}`);
            }
          } else {
            setError("An unexpected error occurred");
          }
          setLoading(false);
        }
      }
    };

    fetchJob();

    return () => {
      cancelled = true;
    };
  }, [jobId]);

  // Handle WebSocket messages
  const handleWebSocketMessage = useCallback((message: WebSocketMessage) => {
    console.log("WebSocket message:", message);

    setJob((prevJob) => {
      if (!prevJob) return prevJob;

      const updatedJob = { ...prevJob };

      switch (message.type) {
        case "status": {
          const data = message.data as { status?: string; progress?: number };
          if (data.status) {
            updatedJob.status = data.status as ResearchJob["status"];
          }
          if (typeof data.progress === "number") {
            updatedJob.progress = data.progress;
          }
          break;
        }

        case "iteration": {
          const iterationData = message.data as ResearchJob["iterations"][0];
          if (iterationData && iterationData.id) {
            const existingIndex = updatedJob.iterations.findIndex(
              (iter) => iter.id === iterationData.id
            );
            if (existingIndex >= 0) {
              // Update existing iteration
              updatedJob.iterations[existingIndex] = iterationData;
            } else {
              // Add new iteration
              updatedJob.iterations = [...updatedJob.iterations, iterationData];
            }
          }
          break;
        }

        case "source": {
          const sourceData = message.data as ResearchJob["sources"][0];
          if (sourceData && sourceData.url) {
            const existingIndex = updatedJob.sources.findIndex(
              (source) => source.url === sourceData.url
            );
            if (existingIndex >= 0) {
              // Update existing source
              updatedJob.sources[existingIndex] = sourceData;
            } else {
              // Add new source
              updatedJob.sources = [...updatedJob.sources, sourceData];
            }
          }
          break;
        }

        case "report": {
          const data = message.data as { report?: string };
          if (data.report) {
            updatedJob.report = data.report;
          }
          break;
        }

        case "error": {
          const data = message.data as { error?: string };
          if (data.error) {
            updatedJob.error = data.error;
            updatedJob.status = "failed" as ResearchJob["status"];
          }
          break;
        }

        default:
          console.warn("Unknown WebSocket message type:", message.type);
      }

      return updatedJob;
    });
  }, []);

  // WebSocket connection for real-time updates
  const { isConnected, lastMessage } = useWebSocket({
    jobId,
    onMessage: handleWebSocketMessage,
  });

  if (loading) {
    return (
      <div className="flex items-center justify-center min-h-screen">
        <div className="text-gray-500 dark:text-gray-400">Loading...</div>
      </div>
    );
  }

  if (error || !job) {
    return (
      <div className="flex items-center justify-center min-h-screen">
        <div className="text-gray-500 dark:text-gray-400">
          {error || "Job not found"}
        </div>
      </div>
    );
  }

  return (
    <div className="container mx-auto px-4 py-8 max-w-6xl">
      <div className="mb-8">
        <h1 className="text-3xl font-bold text-gray-900 dark:text-white mb-2">
          Research: {job.query}
        </h1>
        <div className="flex items-center gap-4">
          <span
            className={`px-3 py-1 rounded-full text-sm font-medium ${
              isConnected
                ? "bg-green-100 text-green-800 dark:bg-green-900 dark:text-green-200"
                : "bg-gray-100 text-gray-800 dark:bg-gray-900 dark:text-gray-200"
            }`}
          >
            {isConnected ? "Connected" : "Disconnected"}
          </span>
        </div>
      </div>

      <div className="mb-6">
        <ProgressBar progress={job.progress} status={job.status} />
      </div>

      <div className="grid grid-cols-1 lg:grid-cols-2 gap-6 mb-6">
        <div>
          <h2 className="text-xl font-semibold text-gray-900 dark:text-white mb-4">
            Research Iterations
          </h2>
          <div className="space-y-3">
            {job.iterations.map((iteration) => (
              <IterationCard key={iteration.id} iteration={iteration} />
            ))}
          </div>
        </div>

        <div>
          <h2 className="text-xl font-semibold text-gray-900 dark:text-white mb-4">
            Sources
          </h2>
          <SourcePanel sources={job.sources} />
        </div>
      </div>

      {job.report && (
        <div className="mt-6">
          <OutputViewer report={job.report} />
        </div>
      )}

      {job.error && (
        <div className="mt-6 p-4 bg-red-50 dark:bg-red-900/20 border border-red-200 dark:border-red-800 rounded-lg">
          <p className="text-red-800 dark:text-red-200">{job.error}</p>
        </div>
      )}
    </div>
  );
}
</file>

<file path="backend/app/orchestrator/research_engine.py">
"""
Main research engine that orchestrates the research process.
"""
import logging
import asyncio
import json
from datetime import datetime
from typing import Dict, List, Any, Optional, TYPE_CHECKING
from app.models.research_job import ResearchJob, ResearchJobStatus
from app.orchestrator.tongyi_client import TongyiClient
from app.tools.tool_registry import ToolRegistry
from app.db.supabase_client import (
    update_job_status as db_update_job_status,
    update_job_report,
    add_iteration,
    add_source,
    get_sources_by_job,
    get_job
)

if TYPE_CHECKING:
    from app.api.websocket import ConnectionManager

logger = logging.getLogger(__name__)


class ResearchEngine:
    """
    Main research engine that coordinates AI agent research activities.
    
    TODO: Implement the main research loop that:
    1. Receives research query
    2. Uses Tongyi DeepResearch agent to plan research steps
    3. Executes tools (web search, web fetch) based on agent decisions
    4. Aggregates and synthesizes results
    5. Returns comprehensive research report
    """
    
    def __init__(self):
        """Initialize the research engine."""
        self.tongyi_client = TongyiClient()
        self.tool_registry = ToolRegistry()
    
    async def start_research(
        self, 
        query: str, 
        job_id: str, 
        connection_manager: Optional["ConnectionManager"] = None
    ) -> ResearchJob:
        """
        Start a new research job.
        
        Args:
            query: The research query/question
            job_id: Unique identifier for this research job
            connection_manager: Optional WebSocket connection manager for real-time updates
            
        Returns:
            ResearchJob instance with initial status
        """
        # TODO: Create initial research job in database
        # TODO: Initialize research context and state
        
        logger.info(f"Starting research job {job_id} with query: {query}")
        
        # Update job status to running in database
        await db_update_job_status(job_id, "running", 0.0)
        
        job = ResearchJob(
            id=job_id,
            query=query,
            status=ResearchJobStatus.RUNNING,
        )
        
        # Broadcast initial RUNNING status
        if connection_manager:
            asyncio.create_task(
                connection_manager.broadcast_status(job_id, ResearchJobStatus.RUNNING.value, 0.0)
            )
        
        # Begin research loop
        asyncio.create_task(self._run_research_loop(job_id, query, connection_manager))
        
        return job
    
    async def _run_research_loop(
        self,
        job_id: str,
        query: str,
        connection_manager: Optional["ConnectionManager"] = None
    ) -> None:
        """
        Run the main research loop with Tongyi DeepResearch agent.
        
        Args:
            job_id: Research job ID
            query: Research query
            connection_manager: Optional WebSocket connection manager
        """
        max_iterations = 20
        MIN_ITERATIONS = 5  # Require at least 5 iterations before allowing final answer
        iteration_count = 0
        
        # Track research depth metrics
        research_metrics = {
            "search_queries": 0,
            "sources_fetched": 0,
        }
        
        try:
            # Update status to RUNNING in database (already done in start_research, but ensure it's set)
            await db_update_job_status(job_id, "running", 0.0)
            
            # Broadcast status to WebSocket
            if connection_manager:
                asyncio.create_task(
                    connection_manager.broadcast_status(job_id, ResearchJobStatus.RUNNING.value, 0.0)
                )
            
            # Initialize conversation with system prompt and get tool definitions
            tools = self.tool_registry.get_tool_definitions()
            
            system_prompt = (
    "You are an expert research assistant conducting investment due diligence. "
    "Your task is to gather comprehensive information using the available tools.\n\n"
    
    "ðŸ”§ CRITICAL: HOW TO USE TOOLS\n"
    "You MUST use tools by making function calls in the following EXACT format:\n"
    "- The system will provide tool definitions in the API request\n"
    "- When you need information, you MUST invoke tools using tool_calls (NOT by describing them in text)\n"
    "- Tool calls are JSON-structured function invocations that the system will execute automatically\n\n"
    
    "CORRECT behavior (tool call in proper field):\n"
    "âœ“ Your response contains tool_calls field with function invocations\n"
    "âœ“ The system executes your tools and returns results\n"
    "âœ“ You receive the results and continue research\n\n"
    
    "INCORRECT behavior (describing tools in content):\n"
    "âœ— Writing 'I should search for...' or 'I will use web_search...' in your response\n"
    "âœ— Writing JSON-like tool structures in your text content\n"
    "âœ— Describing what tools to use instead of actually using them\n"
    "âœ— ANY text description of tools instead of actual function calls\n\n"
    
    "âš¡ MANDATORY FIRST ACTION RULE\n"
    "In your FIRST response to ANY research query, you MUST:\n"
    "1. Make at least one web_search tool call to begin gathering information\n"
    "2. Do NOT provide any analysis or conclusions without first gathering data\n"
    "3. Do NOT write text about what you plan to search - just search\n\n"
    
    "Example FIRST response (CORRECT):\n"
    "[System executes your web_search tool call and returns results]\n\n"
    
    "Example FIRST response (INCORRECT):\n"
    "'I will search for information about X using these queries: ...'\n"
    "This is WRONG - you must actually invoke the tool, not describe it!\n\n"
    
    "ðŸ“‹ AVAILABLE TOOLS\n\n"
    "1. web_search(query: str or list[str])\n"
    "   - Searches the web using Brave Search\n"
    "   - Returns list of URLs, titles, and snippets\n"
    "   - Use this FIRST to find relevant sources\n"
    "   - Can pass single query string or list of query strings\n\n"
    
    "2. web_fetch(url: str)\n"
    "   - Fetches full content from a specific URL\n"
    "   - Use AFTER web_search to read important articles\n"
    "   - Returns parsed text content from the page\n\n"
    
    "ðŸŽ¯ MINIMUM RESEARCH DEPTH REQUIREMENT\n\n"
    f"You MUST conduct at least {MIN_ITERATIONS} research iterations before providing a final answer.\n"
    "This ensures comprehensive, multi-angle exploration of the topic.\n\n"
    
    "Each iteration should involve:\n"
    "- Making tool calls (web_search or web_fetch)\n"
    "- Analyzing results\n"
    "- Planning next exploration steps\n\n"
    
    f"Do NOT attempt to provide a final answer before completing at least {MIN_ITERATIONS} iterations.\n"
    "If you try to stop early, the system will force you to continue researching.\n\n"
    
    "ðŸ”„ DEEP RESEARCH WORKFLOW (IterResearch-Inspired)\n\n"
    "Iteration 1-2: Initial Exploration\n"
    "- Make web_search call(s) to find initial sources covering broad aspects\n"
    "- Use multiple distinct search queries to explore different angles:\n"
    "  * Technical/scientific aspects\n"
    "  * Market trends and commercial developments\n"
    "  * Recent news and breakthroughs\n"
    "  * Expert opinions and analysis\n"
    "- Receive search results with URLs and snippets\n\n"
    
    "Iteration 3-4: Deepening Investigation\n"
    "- Use web_fetch to read the most promising articles from search results\n"
    "- Based on what you learn, make NEW web_search calls to explore:\n"
    "  * Specific sub-topics that emerged\n"
    "  * Contradictory viewpoints or competing solutions\n"
    "  * Recent developments mentioned in sources\n"
    "  * Related technologies or alternative approaches\n"
    "- Continue fetching and analyzing sources\n\n"
    
    "Iteration 5+: Comprehensive Coverage\n"
    "- Synthesize what you've learned so far\n"
    "- Identify gaps in your knowledge\n"
    "- Make targeted searches to fill those gaps\n"
    "- Ensure you've explored multiple perspectives\n"
    f"- Only after {MIN_ITERATIONS} iterations AND comprehensive coverage, provide final answer\n\n"
    
    "ðŸ” MULTI-ANGLE EXPLORATION REQUIREMENTS\n\n"
    "For deep research, you MUST explore multiple angles:\n"
    "1. Technical/Scientific: How does it work? What are the underlying principles?\n"
    "2. Market/Commercial: What's the business landscape? Who are key players?\n"
    "3. Recent Developments: What's new in 2024-2025? Latest breakthroughs?\n"
    "4. Expert Analysis: What do industry experts say? Academic research?\n"
    "5. Challenges/Risks: What are the limitations? What could go wrong?\n\n"
    
    "Use DISTINCT search queries for each angle. Don't repeat the same query.\n"
    "Example: For 'renewable energy storage', explore:\n"
    "- 'solid-state battery technology 2025'\n"
    "- 'grid-scale energy storage market trends'\n"
    "- 'flow battery commercialization updates'\n"
    "- 'hydrogen storage cost reduction 2025'\n"
    "- 'thermal energy storage innovations'\n\n"
    
    "ðŸ“Š ITERATIVE DEEPENING STRATEGY\n\n"
    "After each iteration, use what you learned to guide the next:\n"
    "- If you find a technology mentioned, search specifically for that technology\n"
    "- If you see a company name, search for that company's latest developments\n"
    "- If sources mention a problem, search for solutions to that problem\n"
    "- Build on previous findings rather than just collecting more general information\n\n"
    
    f"Final Iteration (after {MIN_ITERATIONS}+ iterations):\n"
    "- Provide your final analysis ONLY when you have:\n"
    f"  * Completed at least {MIN_ITERATIONS} research iterations\n"
    "  * Explored multiple angles of the topic\n"
    "  * Gathered sources from different perspectives\n"
    "  * Fetched and analyzed key articles\n"
    "- Your final response should contain NO tool calls (indicates research is complete)\n\n"
    
    "ðŸŽ¯ DECISION RULE: SHOULD I USE TOOLS NOW?\n\n"
    "Ask yourself:\n"
    f"1. Have I completed at least {MIN_ITERATIONS} iterations? (If NO â†’ make tool calls)\n"
    "2. Have I explored multiple angles (technical, market, recent, expert)? (If NO â†’ make tool calls)\n"
    "3. Have I used distinct search queries covering different aspects? (If NO â†’ make tool calls)\n"
    "4. Do I have enough information from diverse sources? (If NO â†’ make tool calls)\n\n"
    
    "IF ANY answer is NO â†’ Make tool calls to gather more information (web_search or web_fetch)\n"
    "IF ALL answers are YES â†’ Provide your final answer with NO tool calls\n\n"
    
    "Remember:\n"
    "- Research queries ALWAYS need information gathering\n"
    "- The first response ALWAYS needs tool usage\n"
    f"- Minimum {MIN_ITERATIONS} iterations are REQUIRED before final answer\n"
    "- Explore multiple angles with distinct search queries\n"
    "- Build on previous findings to deepen investigation\n"
    "- If you're not providing a final answer, you should be making tool calls\n"
    "- NEVER describe tools in text - always invoke them\n\n"
    
    "ðŸš¨ WHAT HAPPENS IF YOU DESCRIBE TOOLS INSTEAD OF USING THEM\n\n"
    "If you write text like 'I should search for...' or 'I will use web_search with arguments...', the system will:\n"
    "- Detect this as an error\n"
    "- Log a warning about incorrect behavior\n"
    "- Your research will fail with 0 sources gathered\n"
    "- You will need to be fixed with prompt engineering\n\n"
    
    "To avoid this: ALWAYS use proper tool calling format. NEVER describe tools in your response content.\n\n"
    
    "Now begin your research. Remember: Your FIRST action must be making web_search tool calls, "
    f"and you must complete at least {MIN_ITERATIONS} iterations before providing a final answer!"
)



            messages = [
                {
                    "role": "system",
                    "content": system_prompt
                },
                {
                    "role": "user",
                    "content": query
                }
            ]
            
            logger.info(f"Starting research loop for job {job_id} with query: {query}")
            
            # Main research loop
            while iteration_count < max_iterations:
                iteration_count += 1
                logger.info(f"Research iteration {iteration_count}/{max_iterations} for job {job_id}")
                
                try:
                    # Call Tongyi API with current conversation and tools
                    response = await self.tongyi_client.chat_completion(
                        messages=messages,
                        tools=tools
                    )
                    
                    # Extract tool calls and content
                    tool_calls = response.get("tool_calls", [])
                    content = response.get("content", "")
                    assistant_message = response.get("message", {})
                    
                    # Add assistant message to conversation
                    messages.append(assistant_message)
                    
                    # Process tool calls if any
                    if tool_calls:
                        logger.info(f"Processing {len(tool_calls)} tool call(s) in iteration {iteration_count}")
                        
                        # Execute tools sequentially
                        tool_results = []
                        for tool_call in tool_calls:
                            tool_id = tool_call.get("id", "")
                            function = tool_call.get("function", {})
                            tool_name = function.get("name", "")
                            tool_args = function.get("arguments", {})
                            
                            if not tool_name:
                                logger.warning(f"Skipping tool call with missing name: {tool_call}")
                                continue
                            
                            try:
                                # Execute tool
                                logger.info(f"Executing tool: {tool_name} with args: {tool_args}")
                                tool_result = await self.tool_registry.execute_tool(tool_name, tool_args)
                                
                                # Handle tool results and update metrics
                                if tool_name == "web_search":
                                    # Track search query
                                    research_metrics["search_queries"] += 1
                                    # Extract search results and save sources
                                    search_results = tool_result.get("results", [])
                                    for result in search_results:
                                        url = result.get("url", "")
                                        title = result.get("title", "")
                                        snippet = result.get("snippet", "")
                                        
                                        if url:
                                            try:
                                                await add_source(job_id, url, title, snippet)
                                                
                                                # Broadcast source discovery
                                                if connection_manager:
                                                    source_data = {
                                                        "url": url,
                                                        "title": title,
                                                        "snippet": snippet,
                                                        "fetched_at": None
                                                    }
                                                    asyncio.create_task(
                                                        connection_manager.broadcast_source(job_id, source_data)
                                                    )
                                            except Exception as e:
                                                logger.error(f"Error saving source {url}: {e}", exc_info=True)
                                
                                elif tool_name == "web_fetch":
                                    # Track source fetch
                                    research_metrics["sources_fetched"] += 1
                                    # Extract fetched content and save source
                                    fetch_content = tool_result.get("content", {})
                                    if isinstance(fetch_content, dict):
                                        url = fetch_content.get("url", "")
                                        title = fetch_content.get("title", "")
                                        content = fetch_content.get("content", "")
                                        
                                        if url:
                                            try:
                                                await add_source(job_id, url, title, None, content)
                                                
                                                # Broadcast source fetch
                                                if connection_manager:
                                                    source_data = {
                                                        "url": url,
                                                        "title": title,
                                                        "snippet": content[:200] + "..." if len(content) > 200 else content,
                                                        "fetched_at": datetime.utcnow().isoformat()
                                                    }
                                                    asyncio.create_task(
                                                        connection_manager.broadcast_source(job_id, source_data)
                                                    )
                                            except Exception as e:
                                                logger.error(f"Error saving fetched source {url}: {e}", exc_info=True)
                                
                                # Format tool result for message
                                # Note: OpenAI/OpenRouter format requires only role, tool_call_id, and content
                                # The "name" field is NOT part of tool result messages (it's only in function definitions)
                                tool_results.append({
                                    "tool_call_id": tool_id,
                                    "role": "tool",
                                    "content": json.dumps(tool_result)
                                })
                                
                            except Exception as e:
                                logger.error(f"Error executing tool {tool_name}: {e}", exc_info=True)
                                # Add error result to continue loop
                                # Note: OpenAI/OpenRouter format requires only role, tool_call_id, and content
                                # The "name" field is NOT part of tool result messages
                                tool_results.append({
                                    "tool_call_id": tool_id,
                                    "role": "tool",
                                    "content": json.dumps({"error": str(e)})
                                })
                        
                        # Add tool results to conversation
                        messages.extend(tool_results)
                        
                        # Log research metrics periodically
                        logger.info(f"Research metrics after iteration {iteration_count}: {research_metrics}")
                        
                        # Prepare iteration data
                        action = f"tool_execution: {', '.join([tc.get('function', {}).get('name', 'unknown') for tc in tool_calls])}"
                        step_result = {
                            "status": "completed",
                            "action": action,
                            "timestamp": datetime.utcnow().isoformat(),
                            "step": iteration_count,
                            "tool_calls": tool_calls,
                            "tool_results": [r.get("content", "") for r in tool_results],
                            "research_metrics": research_metrics.copy()
                        }
                        
                    else:
                        # No tool calls - check if minimum iterations reached
                        if iteration_count < MIN_ITERATIONS:
                            # Force continuation - need more research iterations
                            logger.info(f"Model attempted to stop at iteration {iteration_count}, but minimum {MIN_ITERATIONS} iterations required. Forcing continuation.")
                            
                            # Add system message encouraging more research
                            messages.append({
                                "role": "system",
                                "content": (
                                    f"You need to conduct at least {MIN_ITERATIONS} research iterations before providing a final answer. "
                                    f"You're currently at iteration {iteration_count}. "
                                    f"Continue exploring with tool calls - search for different angles, fetch more sources, "
                                    f"and deepen your investigation. Do not provide a final answer yet."
                                )
                            })
                            
                            # Prepare iteration data for forced continuation
                            action = "forced_continuation"
                            step_result = {
                                "status": "completed",
                                "action": action,
                                "timestamp": datetime.utcnow().isoformat(),
                                "step": iteration_count,
                                "content": content,
                                "reason": f"Minimum {MIN_ITERATIONS} iterations not reached (currently {iteration_count})"
                            }
                            
                            # Persist iteration
                            await add_iteration(job_id, iteration_count, action, step_result)
                            
                            # Broadcast iteration
                            if connection_manager:
                                iteration_data = {
                                    "id": f"{job_id}-iter-{iteration_count}",
                                    "step": iteration_count,
                                    "action": action,
                                    "timestamp": step_result.get("timestamp"),
                                    "results": step_result
                                }
                                asyncio.create_task(
                                    connection_manager.broadcast_iteration(job_id, iteration_data)
                                )
                            
                            # Continue loop - don't break
                            continue
                        
                        # Minimum iterations reached - final answer allowed
                        logger.info(f"Final answer received in iteration {iteration_count} (minimum {MIN_ITERATIONS} iterations satisfied)")
                        
                        # Prepare iteration data
                        action = "final_answer"
                        step_result = {
                            "status": "completed",
                            "action": action,
                            "timestamp": datetime.utcnow().isoformat(),
                            "step": iteration_count,
                            "content": content
                        }
                        
                        # Persist final iteration
                        await add_iteration(job_id, iteration_count, action, step_result)
                        
                        # Broadcast final iteration
                        if connection_manager:
                            iteration_data = {
                                "id": f"{job_id}-iter-{iteration_count}",
                                "step": iteration_count,
                                "action": action,
                                "timestamp": step_result.get("timestamp"),
                                "results": step_result
                            }
                            asyncio.create_task(
                                connection_manager.broadcast_iteration(job_id, iteration_data)
                            )
                        
                        # Break from loop - research complete
                        break
                    
                    # Persist iteration to database
                    await add_iteration(job_id, iteration_count, action, step_result)
                    
                    # Broadcast iteration update
                    if connection_manager:
                        iteration_data = {
                            "id": f"{job_id}-iter-{iteration_count}",
                            "step": iteration_count,
                            "action": action,
                            "timestamp": step_result.get("timestamp"),
                            "results": step_result
                        }
                        asyncio.create_task(
                            connection_manager.broadcast_iteration(job_id, iteration_data)
                        )
                    
                    # Update progress: research phase is 0-90%
                    progress = min(90, (iteration_count / max_iterations) * 90)
                    await db_update_job_status(job_id, "running", progress)
                    
                    # Broadcast progress update
                    if connection_manager:
                        asyncio.create_task(
                            connection_manager.broadcast_status(job_id, ResearchJobStatus.RUNNING.value, progress)
                        )
                    
                except Exception as e:
                    logger.error(f"Error in iteration {iteration_count} for job {job_id}: {e}", exc_info=True)
                    # Continue to next iteration - don't break entire loop
                    # Save error iteration
                    error_result = {
                        "status": "error",
                        "action": "error",
                        "timestamp": datetime.utcnow().isoformat(),
                        "step": iteration_count,
                        "error": str(e)
                    }
                    try:
                        await add_iteration(job_id, iteration_count, "error", error_result)
                    except Exception:
                        pass  # Don't fail on iteration save error
                    continue
            
            # Synthesize results
            logger.info(f"Starting synthesis for job {job_id}")
            
            # Update progress to 90% (entering synthesis phase)
            await db_update_job_status(job_id, "running", 90.0)
            if connection_manager:
                asyncio.create_task(
                    connection_manager.broadcast_status(job_id, ResearchJobStatus.RUNNING.value, 90.0)
                )
            
            # Fetch all sources from database
            sources = await get_sources_by_job(job_id)
            logger.info(f"Found {len(sources)} sources for synthesis")
            
            # Generate final report
            report = await self.synthesize_results(job_id, sources)
            
            # Update progress to 100%
            await db_update_job_status(job_id, "running", 100.0)
            if connection_manager:
                asyncio.create_task(
                    connection_manager.broadcast_status(job_id, ResearchJobStatus.RUNNING.value, 100.0)
                )
            
            # Persist report to database
            await update_job_report(job_id, report)
            
            # Broadcast final report
            if connection_manager:
                asyncio.create_task(
                    connection_manager.broadcast_report(job_id, report)
                )
            
            # Update status to COMPLETED in database
            await db_update_job_status(job_id, "completed", 100.0)
            
            # Broadcast completion status
            if connection_manager:
                asyncio.create_task(
                    connection_manager.broadcast_status(job_id, ResearchJobStatus.COMPLETED.value, 100.0)
                )
            
            logger.info(f"Research loop completed successfully for job {job_id}")
                
        except Exception as e:
            logger.error(f"Error in research loop for job {job_id}: {e}", exc_info=True)
            
            # Update status to FAILED in database
            await db_update_job_status(job_id, "failed", None)
            
            # Broadcast error
            if connection_manager:
                asyncio.create_task(
                    connection_manager.broadcast_error(job_id, str(e))
                )
                asyncio.create_task(
                    connection_manager.broadcast_status(job_id, ResearchJobStatus.FAILED.value, None)
                )
    
    async def execute_research_step(
        self,
        job_id: str,
        step: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Execute a single research step.
        
        Args:
            job_id: The research job ID
            step: Step definition from AI agent
            
        Returns:
            Step execution results
        """
        # TODO: Parse step instruction from agent
        # TODO: Select appropriate tool(s) from registry
        # TODO: Execute tool with parameters
        # TODO: Return results to agent for next step
        
        from datetime import datetime
        
        logger.info(f"Executing research step {step.get('step', 'unknown')} for job {job_id}")
        return {
            "status": "completed",
            "action": step.get("action", "research"),
            "timestamp": datetime.utcnow().isoformat(),
            "step": step.get("step")
        }
    
    async def synthesize_results(
        self,
        job_id: str,
        sources: List[Dict[str, Any]]
    ) -> str:
        """
        Synthesize research results into final report.
        
        Args:
            job_id: The research job ID
            sources: List of gathered sources and findings (fallback if fetch fails)
            
        Returns:
            Final research report as formatted markdown text
        """
        logger.info(f"Synthesizing results for job {job_id}")
        
        try:
            # Fetch job to get the original query
            job_data = await get_job(job_id)
            if not job_data:
                logger.error(f"Job {job_id} not found for synthesis")
                return "# Research Report\n\nError: Could not retrieve job data."
            
            query = job_data.get("query", "Unknown query")
            
            # Fetch all sources from database (as per plan)
            try:
                sources = await get_sources_by_job(job_id)
                logger.info(f"Fetched {len(sources)} sources for synthesis")
            except Exception as e:
                logger.warning(f"Error fetching sources, using provided sources: {e}")
                # Use provided sources as fallback
                if not sources:
                    sources = []
            
            # Build sources list for prompt
            sources_list = []
            for idx, source in enumerate(sources, 1):
                source_info = f"[{idx}] {source.get('title', 'Untitled')}\n   URL: {source.get('url', 'N/A')}"
                if source.get('snippet'):
                    source_info += f"\n   Preview: {source.get('snippet', '')[:200]}"
                sources_list.append(source_info)
            
            sources_text = "\n".join(sources_list) if sources_list else "No sources were gathered during research."
            
            # Build synthesis prompt
            synthesis_prompt = f"""You are synthesizing a comprehensive investment research report based on gathered information.

Research Query: {query}

Sources Gathered ({len(sources)} total):
{sources_text}

Please generate a comprehensive, well-structured research report in markdown format that includes:

1. **Executive Summary** - A concise overview of key findings and conclusions
2. **Key Findings** - Main discoveries with source citations using [1], [2], etc. format
3. **Detailed Analysis** - In-depth examination of important aspects
4. **Risk Factors** - Potential risks and concerns identified
5. **Sources** - Numbered list of all sources with full URLs

Requirements:
- Use markdown formatting for structure and readability
- Cite sources using [1], [2], etc. format corresponding to the numbered sources above
- Be thorough and objective
- Include specific details and data points where available
- Maintain professional tone suitable for investment due diligence

Generate the report now:"""
            
            # Call Tongyi API for synthesis
            messages = [
                {
                    "role": "system",
                    "content": "You are an expert financial analyst specializing in investment research and due diligence reports."
                },
                {
                    "role": "user",
                    "content": synthesis_prompt
                }
            ]
            
            logger.info(f"Calling Tongyi API for report synthesis")
            # Use longer timeout for synthesis (120s) as it may take longer with large contexts
            response = await self.tongyi_client.chat_completion(
                messages=messages,
                tools=None,  # No tools needed for synthesis
                timeout=120.0  # 2 minutes for synthesis requests
            )
            
            # Extract report content
            report_content = response.get("content", "")
            
            if not report_content or len(report_content.strip()) < 50:
                logger.warning("Synthesis returned empty or very short report, generating fallback")
                # Generate fallback report
                report_content = self._generate_fallback_report(query, sources)
            
            # Ensure sources section is included
            if "## Sources" not in report_content and "**Sources**" not in report_content:
                report_content += self._format_sources_section(sources)
            
            logger.info(f"Successfully synthesized report for job {job_id} ({len(report_content)} characters)")
            return report_content
            
        except Exception as e:
            logger.error(f"Error synthesizing results for job {job_id}: {e}", exc_info=True)
            # Return fallback report
            try:
                job_data = await get_job(job_id)
                query = job_data.get("query", "Unknown query") if job_data else "Unknown query"
            except Exception:
                query = "Unknown query"
            
            return self._generate_fallback_report(query, sources)
    
    def _generate_fallback_report(self, query: str, sources: List[Dict[str, Any]]) -> str:
        """Generate a fallback report when synthesis fails."""
        report = f"""# Research Report

## Executive Summary

Research was conducted on: {query}

## Key Findings

{len(sources)} source(s) were gathered during research, but automatic synthesis encountered an error. Please review the sources below for detailed information.

## Sources

{self._format_sources_section(sources)}
"""
        return report
    
    def _format_sources_section(self, sources: List[Dict[str, Any]]) -> str:
        """Format sources into a numbered list section."""
        if not sources:
            return "\n## Sources\n\nNo sources were gathered during research.\n"
        
        sources_text = "\n## Sources\n\n"
        for idx, source in enumerate(sources, 1):
            title = source.get("title", "Untitled")
            url = source.get("url", "")
            sources_text += f"{idx}. **{title}**\n   - {url}\n\n"
        
        return sources_text
    
    async def update_job_status(
        self,
        job_id: str,
        status: ResearchJobStatus,
        progress: Optional[float] = None,
        connection_manager: Optional["ConnectionManager"] = None
    ) -> None:
        """
        Update research job status.
        
        Args:
            job_id: The research job ID
            status: New status
            progress: Optional progress percentage (0-100)
            connection_manager: Optional WebSocket connection manager for real-time updates
        """
        # Update job status in database
        await db_update_job_status(job_id, status.value, progress)
        
        logger.info(f"Updating job {job_id} status to {status}")
        
        # Broadcast status update via WebSocket
        if connection_manager:
            asyncio.create_task(
                connection_manager.broadcast_status(job_id, status.value, progress)
            )
</file>

<file path="backend/app/tools/web_fetch.py">
"""
Jina Reader API wrapper for fetching and parsing web content.
"""
import logging
import httpx
import re
from typing import Dict, Any, Optional, Union, List
from app.config import settings

logger = logging.getLogger(__name__)


class WebFetchTool:
    """
    Tool for fetching and parsing web content using Jina Reader API.
    
    Provides async web content fetching functionality with proper error handling,
    response parsing, and logging.
    """
    
    def __init__(self):
        """Initialize the web fetch tool."""
        self.api_key = settings.JINA_READER_API_KEY
        self.base_url = "https://r.jina.ai"
        self.client = httpx.AsyncClient(
            headers={
                "Authorization": f"Bearer {self.api_key}",
            },
            timeout=60.0,
        )
    
    def _truncate_content(self, content: str, max_words: int = 10000) -> str:
        """
        Truncate content to a maximum number of words, preserving word boundaries.
        
        Args:
            content: The content to truncate
            max_words: Maximum number of words to keep
            
        Returns:
            Truncated content string
        """
        words = content.split()
        if len(words) <= max_words:
            return content
        return " ".join(words[:max_words])
    
    def _extract_title_from_markdown(self, markdown_content: str) -> str:
        """
        Extract title from markdown content (first heading).
        
        Args:
            markdown_content: Markdown content string
            
        Returns:
            Extracted title or empty string
        """
        # Try to find first heading (# Title or ## Title)
        heading_match = re.search(r'^#+\s+(.+)$', markdown_content, re.MULTILINE)
        if heading_match:
            return heading_match.group(1).strip()
        return ""
    
    async def fetch(
        self,
        url: Union[str, List[str]],
        mode: str = "reader"
    ) -> Dict[str, Any]:
        """
        Fetch and parse content from a URL using Jina Reader API.
        
        Args:
            url: The URL to fetch (e.g., https://example.com) or a list of URLs
            mode: Reader mode (reader, raw, etc.) - currently only "reader" is supported
            
        Returns:
            Dictionary with url, title, content, word_count on success.
            Dictionary with "error" key on failure.
        """
        # Handle list of URLs - use first URL (same pattern as web_search)
        if isinstance(url, list):
            if not url:
                return {"error": "URL is required"}
            logger.warning(f"Tongyi sent multiple URLs: {url}. Using first URL: {url[0]}")
            url = url[0]
        
        # Validate input
        if not url or not url.strip():
            logger.warning("Empty URL provided")
            return {"error": "Empty URL provided"}
        
        # Ensure URL is properly formatted
        url = url.strip()
        if not url.startswith(("http://", "https://")):
            url = f"https://{url}"
        
        # Construct Jina API URL
        jina_url = f"{self.base_url}/{url}"
        
        logger.info(f"Fetching content from Jina Reader API: {url}")
        
        try:
            response = await self.client.get(jina_url)
            response.raise_for_status()
            
            # Jina Reader returns markdown/text content
            content = response.text
            
            # Extract title from markdown (first heading)
            title = self._extract_title_from_markdown(content)
            if not title:
                # Fallback: use URL domain or last part as title
                try:
                    from urllib.parse import urlparse
                    parsed = urlparse(url)
                    title = parsed.netloc or parsed.path.split("/")[-1] or url
                except Exception:
                    title = url
            
            # Truncate content to max 10,000 words
            truncated_content = self._truncate_content(content, max_words=10000)
            
            # Calculate word count (on truncated content if it was truncated)
            word_count = len(truncated_content.split())
            
            result = {
                "url": url,
                "title": title,
                "content": truncated_content,
                "word_count": word_count,
            }
            
            logger.info(f"Successfully fetched content from {url}: {word_count} words")
            return result
            
        except httpx.HTTPStatusError as e:
            logger.error(f"Jina Reader API HTTP error: {e.response.status_code} - {e.response.text}")
            return {"error": f"HTTP error {e.response.status_code}: Failed to fetch content"}
        except httpx.RequestError as e:
            logger.error(f"Jina Reader API request error: {e}")
            return {"error": f"Request error: Failed to fetch content"}
        except httpx.TimeoutException as e:
            logger.error(f"Jina Reader API timeout error: {e}")
            return {"error": "Request timeout: Failed to fetch content within 60 seconds"}
        except Exception as e:
            logger.error(f"Jina Reader API unexpected error: {e}", exc_info=True)
            return {"error": f"Unexpected error: {str(e)}"}
    
    async def close(self) -> None:
        """Close the HTTP client."""
        await self.client.aclose()
</file>

<file path="backend/app/tools/web_search.py">
"""
Brave Search API wrapper for web search functionality.
"""
import logging
import httpx
from typing import List, Dict, Any, Optional, Union
from app.config import settings

logger = logging.getLogger(__name__)


class WebSearchTool:
    """
    Tool for performing web searches using Brave Search API.
    
    Provides async web search functionality with proper error handling,
    response parsing, and logging.
    """
    
    def __init__(self):
        """Initialize the web search tool."""
        self.api_key = settings.BRAVE_SEARCH_API_KEY
        self.base_url = "https://api.search.brave.com/res/v1"
        self.client = httpx.AsyncClient(
            headers={
                "X-Subscription-Token": self.api_key,
            },
            timeout=30.0,
        )
    
    async def search(
        self,
        query: Union[str, List[str]],
        count: int = 10,
        offset: int = 0,
        safesearch: str = "moderate"
    ) -> List[Dict[str, Any]]:
        """
        Perform a web search.
        
        Args:
            query: Search query string or list of query strings (if list, uses first element)
            count: Number of results to return (max 20)
            offset: Offset for pagination
            safesearch: Safe search setting (off, moderate, strict)
            
        Returns:
            List of search result dictionaries with title, url, snippet.
            Returns empty list on error.
        """
        # Handle array queries from Tongyi (convert to single string)
        if isinstance(query, list):
            if not query:
                logger.warning("Empty query list provided")
                return []
            if len(query) > 1:
                logger.warning(f"Tongyi sent multiple queries: {query}. Using first query: {query[0]}")
            query = query[0]
        
        # Validate input
        if not query or not query.strip():
            logger.warning("Empty search query provided")
            return []
        
        # Limit count to max 10 per requirements
        count = min(count, 10)
        
        logger.info(f"Searching Brave Search API for: {query} (count={count})")
        
        params = {
            "q": query,
            "count": min(count, 20),  # Brave API supports up to 20, but we cap at 10
            "offset": offset,
            "safesearch": safesearch,
        }
        
        url = f"{self.base_url}/web/search"
        
        try:
            response = await self.client.get(url, params=params)
            response.raise_for_status()
            data = response.json()
            
            # Parse and format results
            raw_results = data.get("web", {}).get("results", [])
            
            # Extract and format results
            results = []
            for item in raw_results[:count]:
                result = {
                    "title": item.get("title", ""),
                    "url": item.get("url", ""),
                    "snippet": item.get("description", ""),
                }
                # Only add result if it has required fields
                if result["title"] and result["url"]:
                    results.append(result)
            
            logger.info(f"Brave Search API returned {len(results)} results for query: {query}")
            return results
            
        except httpx.HTTPStatusError as e:
            logger.error(f"Brave Search API HTTP error: {e.response.status_code} - {e.response.text}")
            return []
        except httpx.RequestError as e:
            logger.error(f"Brave Search API request error: {e}")
            return []
        except httpx.TimeoutException as e:
            logger.error(f"Brave Search API timeout error: {e}")
            return []
        except Exception as e:
            logger.error(f"Brave Search API unexpected error: {e}", exc_info=True)
            return []
    
    async def close(self) -> None:
        """Close the HTTP client."""
        await self.client.aclose()
</file>

</files>
