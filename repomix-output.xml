This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.gitignore
backend/.env.example
backend/app/__init__.py
backend/app/api/__init__.py
backend/app/api/routes.py
backend/app/api/websocket.py
backend/app/config.py
backend/app/db/__init__.py
backend/app/db/supabase_client.py
backend/app/main.py
backend/app/models/__init__.py
backend/app/models/research_job.py
backend/app/orchestrator/__init__.py
backend/app/orchestrator/research_engine.py
backend/app/orchestrator/tongyi_client.py
backend/app/tools/__init__.py
backend/app/tools/tool_registry.py
backend/app/tools/web_fetch.py
backend/app/tools/web_search.py
backend/README.md
backend/requirements.txt
backend/start.ps1
backend/test_websocket.py
frontend/.env.local.example
frontend/app/api/research/route.ts
frontend/app/globals.css
frontend/app/layout.tsx
frontend/app/page.tsx
frontend/app/research/[jobId]/page.tsx
frontend/components/IterationCard.tsx
frontend/components/OutputViewer.tsx
frontend/components/ProgressBar.tsx
frontend/components/SourcePanel.tsx
frontend/next-env.d.ts
frontend/next.config.js
frontend/package.json
frontend/postcss.config.js
frontend/README.md
frontend/tailwind.config.ts
frontend/tsconfig.json
frontend/types/research.ts
QUICKSTART.md
README.md
supabase/migrations/001_initial_schema.sql
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="backend/app/db/supabase_client.py">
"""
Supabase database client for Agent Bletchley research jobs.
"""
import asyncio
import logging
from typing import Dict, List, Optional, Any
from uuid import UUID
from supabase import create_client, Client
from app.config import settings

logger = logging.getLogger(__name__)

# Initialize Supabase client singleton
_supabase_client: Optional[Client] = None


def get_client() -> Client:
    """Get or create Supabase client instance."""
    global _supabase_client
    if _supabase_client is None:
        if not settings.SUPABASE_URL or not settings.SUPABASE_KEY:
            raise ValueError("SUPABASE_URL and SUPABASE_KEY must be set in environment variables")
        _supabase_client = create_client(settings.SUPABASE_URL, settings.SUPABASE_KEY)
        logger.info("Initialized Supabase client")
    return _supabase_client


async def create_job(query: str, context: Optional[Dict[str, Any]] = None) -> str:
    """
    Create a new research job in the database.
    
    Args:
        query: The research query
        context: Optional context dictionary
        
    Returns:
        Job ID as string
    """
    try:
        client = get_client()
        
        job_data = {
            "query": query,
            "status": "pending",
            "progress": 0.0,
            "context": context or {}
        }
        
        response = await asyncio.to_thread(
            lambda: client.table("research_jobs").insert(job_data).execute()
        )
        
        if response.data and len(response.data) > 0:
            job_id = str(response.data[0]["id"])
            logger.info(f"Created research job {job_id} with query: {query}")
            return job_id
        else:
            raise ValueError("No data returned from database insert")
            
    except Exception as e:
        logger.error(f"Error creating research job: {e}", exc_info=True)
        raise


async def update_job_status(
    job_id: str, 
    status: str, 
    progress: Optional[float] = None
) -> None:
    """
    Update research job status and progress.
    
    Args:
        job_id: The job ID
        status: New status (pending, running, completed, failed, cancelled)
        progress: Optional progress percentage (0-100)
    """
    try:
        client = get_client()
        
        update_data: Dict[str, Any] = {"status": status}
        
        if progress is not None:
            # Ensure progress is within bounds
            update_data["progress"] = max(0.0, min(100.0, float(progress)))
        
        # Set completed_at timestamp if status is "completed"
        if status == "completed":
            from datetime import datetime, timezone
            update_data["completed_at"] = datetime.now(timezone.utc).isoformat()
        
        await asyncio.to_thread(
            lambda: client.table("research_jobs")
            .update(update_data)
            .eq("id", job_id)
            .execute()
        )
        
        logger.info(f"Updated job {job_id} status to {status}" + 
                   (f" with progress {progress}" if progress is not None else ""))
        
    except Exception as e:
        logger.error(f"Error updating job {job_id} status: {e}", exc_info=True)
        raise


async def update_job_report(job_id: str, report: str) -> None:
    """
    Update research job report.
    
    Args:
        job_id: The job ID
        report: The final research report
    """
    try:
        client = get_client()
        
        await asyncio.to_thread(
            lambda: client.table("research_jobs")
            .update({"report": report})
            .eq("id", job_id)
            .execute()
        )
        
        logger.info(f"Updated job {job_id} report")
        
    except Exception as e:
        logger.error(f"Error updating job {job_id} report: {e}", exc_info=True)
        raise


async def add_iteration(
    job_id: str, 
    step: int, 
    action: str, 
    results: Optional[Dict[str, Any]] = None
) -> str:
    """
    Add a research iteration to the database.
    
    Args:
        job_id: The job ID
        step: Step number
        action: Action description
        results: Optional results dictionary
        
    Returns:
        Iteration ID as string
    """
    try:
        client = get_client()
        
        iteration_data = {
            "job_id": job_id,
            "step": step,
            "action": action,
            "results": results or {}
        }
        
        response = await asyncio.to_thread(
            lambda: client.table("research_iterations")
            .insert(iteration_data)
            .execute()
        )
        
        if response.data and len(response.data) > 0:
            iteration_id = str(response.data[0]["id"])
            logger.info(f"Added iteration {iteration_id} for job {job_id}, step {step}")
            return iteration_id
        else:
            raise ValueError("No data returned from database insert")
            
    except Exception as e:
        logger.error(f"Error adding iteration for job {job_id}: {e}", exc_info=True)
        raise


async def add_source(
    job_id: str,
    url: str,
    title: Optional[str] = None,
    snippet: Optional[str] = None,
    content: Optional[str] = None
) -> str:
    """
    Add a research source to the database (with upsert for duplicate URLs).
    
    Args:
        job_id: The job ID
        url: Source URL
        title: Optional source title
        snippet: Optional snippet
        content: Optional full content
        
    Returns:
        Source ID as string
    """
    try:
        client = get_client()
        
        source_data = {
            "job_id": job_id,
            "url": url,
            "title": title,
            "snippet": snippet,
            "content": content
        }
        
        # Use upsert to handle duplicate URLs per job
        # The UNIQUE constraint on (job_id, url) will handle conflicts
        # Supabase will automatically detect the unique constraint
        try:
            # Try insert first
            response = await asyncio.to_thread(
                lambda: client.table("research_sources")
                .insert(source_data)
                .execute()
            )
        except Exception as insert_error:
            # If insert fails due to duplicate, try update
            if "duplicate" in str(insert_error).lower() or "unique" in str(insert_error).lower():
                # Update existing source
                response = await asyncio.to_thread(
                    lambda: client.table("research_sources")
                    .update(source_data)
                    .eq("job_id", job_id)
                    .eq("url", url)
                    .execute()
                )
            else:
                raise
        
        if response.data and len(response.data) > 0:
            source_id = str(response.data[0]["id"])
            logger.info(f"Added/updated source {source_id} for job {job_id}: {url}")
            return source_id
        else:
            raise ValueError("No data returned from database upsert")
            
    except Exception as e:
        logger.error(f"Error adding source for job {job_id}: {e}", exc_info=True)
        raise


async def get_job(job_id: str) -> Optional[Dict[str, Any]]:
    """
    Get a research job with related iterations and sources.
    
    Args:
        job_id: The job ID
        
    Returns:
        Complete job data as dictionary, or None if not found
    """
    try:
        client = get_client()
        
        # Fetch job
        job_response = await asyncio.to_thread(
            lambda: client.table("research_jobs")
            .select("*")
            .eq("id", job_id)
            .execute()
        )
        
        if not job_response.data or len(job_response.data) == 0:
            logger.warning(f"Job {job_id} not found")
            return None
        
        job = job_response.data[0]
        
        # Convert UUID to string
        job["id"] = str(job["id"])
        
        # Fetch iterations
        iterations_response = await asyncio.to_thread(
            lambda: client.table("research_iterations")
            .select("*")
            .eq("job_id", job_id)
            .order("step")
            .execute()
        )
        
        iterations = []
        if iterations_response.data:
            for iteration in iterations_response.data:
                iteration["id"] = str(iteration["id"])
                iteration["job_id"] = str(iteration["job_id"])
                iterations.append(iteration)
        
        # Fetch sources
        sources_response = await asyncio.to_thread(
            lambda: client.table("research_sources")
            .select("*")
            .eq("job_id", job_id)
            .order("fetched_at")
            .execute()
        )
        
        sources = []
        if sources_response.data:
            for source in sources_response.data:
                source["id"] = str(source["id"])
                source["job_id"] = str(source["job_id"])
                sources.append(source)
        
        # Combine job with iterations and sources
        job["iterations"] = iterations
        job["sources"] = sources
        
        logger.info(f"Retrieved job {job_id} with {len(iterations)} iterations and {len(sources)} sources")
        return job
        
    except Exception as e:
        logger.error(f"Error getting job {job_id}: {e}", exc_info=True)
        raise


async def list_jobs(skip: int = 0, limit: int = 100) -> List[Dict[str, Any]]:
    """
    List research jobs with pagination.
    
    Args:
        skip: Number of jobs to skip
        limit: Maximum number of jobs to return
        
    Returns:
        List of job dictionaries
    """
    try:
        client = get_client()
        
        response = await asyncio.to_thread(
            lambda: client.table("research_jobs")
            .select("*")
            .order("created_at", desc=True)
            .range(skip, skip + limit - 1)
            .execute()
        )
        
        jobs = []
        if response.data:
            for job in response.data:
                job["id"] = str(job["id"])
                jobs.append(job)
        
        logger.info(f"Listed {len(jobs)} jobs (skip={skip}, limit={limit})")
        return jobs
        
    except Exception as e:
        logger.error(f"Error listing jobs: {e}", exc_info=True)
        raise
</file>

<file path="backend/test_websocket.py">
"""
Test script for WebSocket connection to the research job endpoint.
"""
import asyncio
import websockets
import json


async def test_websocket():
    """Test WebSocket connection to research job endpoint."""
    uri = "ws://localhost:8000/ws/research/test-job-123"
    
    # First, check if server is running by testing HTTP endpoint
    import httpx
    try:
        with httpx.Client(timeout=2.0) as client:
            resp = client.get("http://localhost:8000/health")
            if resp.status_code == 200:
                print("✓ Server is running")
            else:
                print(f"⚠ Server responded with status {resp.status_code}")
    except httpx.ConnectError:
        print(f"✗ Cannot reach server at http://localhost:8000")
        print(f"\n💡 Make sure the server is running:")
        print(f"   cd backend")
        print(f"   .\\venv\\Scripts\\Activate.ps1")
        print(f"   uvicorn app.main:app --reload")
        return
    except Exception as e:
        print(f"✗ Error checking server: {e}")
        print(f"\n💡 Make sure the server is running:")
        print(f"   cd backend")
        print(f"   .\\venv\\Scripts\\Activate.ps1")
        print(f"   uvicorn app.main:app --reload")
        return
    
    try:
        print(f"\nConnecting to {uri}...")
        async with websockets.connect(uri, ping_interval=20, ping_timeout=10) as websocket:
            print("✓ Connected! Waiting for initial message from server...")
            
            # Create a task to receive messages
            async def receive_messages():
                try:
                    while True:
                        # Use a timeout to detect if connection is still alive
                        try:
                            message = await asyncio.wait_for(websocket.recv(), timeout=5.0)
                            data = json.loads(message)
                            print(f"\n[Received] Type: {data.get('type', 'unknown')}")
                            print(f"         Data: {json.dumps(data.get('data', {}), indent=2)}")
                        except asyncio.TimeoutError:
                            print("(Still waiting for messages...)")
                            continue
                except websockets.exceptions.ConnectionClosed as e:
                    print(f"\n✗ Connection closed by server (code: {e.code}, reason: {e.reason})")
                    return
                except Exception as e:
                    print(f"\n✗ Error receiving messages: {e}")
                    import traceback
                    traceback.print_exc()
                    return
            
            # Create a task to send ping messages periodically
            async def send_ping():
                await asyncio.sleep(1)  # Wait a bit before first ping
                ping_count = 0
                while True:
                    try:
                        ping_count += 1
                        await websocket.send(json.dumps({
                            "type": "ping", 
                            "message": f"Ping #{ping_count} from test client"
                        }))
                        print(f"\n✓ Sent ping #{ping_count}")
                        await asyncio.sleep(3)  # Send ping every 3 seconds
                    except websockets.exceptions.ConnectionClosed:
                        return
                    except Exception as e:
                        print(f"\n✗ Error sending ping: {e}")
                        return
            
            # Run both tasks concurrently
            try:
                await asyncio.gather(
                    receive_messages(),
                    send_ping()
                )
            except KeyboardInterrupt:
                print("\n\n✓ Test interrupted by user (Ctrl+C)")
                print("✓ WebSocket connection test completed successfully!")
                
    except websockets.exceptions.InvalidURI:
        print(f"✗ Invalid URI: {uri}")
    except websockets.exceptions.InvalidStatus as e:
        print(f"✗ Connection failed with status {e.status_code}: {e.status_line}")
    except ConnectionRefusedError:
        print("✗ Connection refused. Is the server running on port 8000?")
    except websockets.exceptions.ConnectionClosed as e:
        print(f"✗ Connection closed (code: {e.code}, reason: {e.reason})")
    except Exception as e:
        print(f"\n✗ Unexpected error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(test_websocket())
</file>

<file path=".gitignore">
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST
venv/
env/
ENV/
.venv

# Node
node_modules/
npm-debug.log*
yarn-debug.log*
yarn-error.log*
.pnpm-debug.log*
.next/
out/
build/
dist/

# Environment variables
.env
.env.local
.env.development.local
.env.test.local
.env.production.local
.env*.local

# IDE
.vscode/
.idea/
*.swp
*.swo
*~
.DS_Store

# Testing
.coverage
.pytest_cache/
.tox/
htmlcov/

# Logs
*.log
logs/

# Supabase
.supabase/

# Misc
*.tmp
*.bak
.cache/
</file>

<file path="backend/.env.example">
# OpenRouter API Configuration
OPENROUTER_API_KEY=your_openrouter_api_key_here
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
TONGYI_MODEL=deepseek/deepseek-r1:free

# Brave Search API
BRAVE_SEARCH_API_KEY=your_brave_search_api_key_here

# Jina Reader API
JINA_READER_API_KEY=your_jina_reader_api_key_here

# Supabase Configuration
SUPABASE_URL=your_supabase_project_url
SUPABASE_KEY=your_supabase_anon_key

# Server Configuration
BACKEND_HOST=0.0.0.0
BACKEND_PORT=8000

# Frontend URL (for CORS)
FRONTEND_URL=http://localhost:3000

# Logging
LOG_LEVEL=INFO
</file>

<file path="backend/app/__init__.py">
"""
Agent Bletchley Backend Application
"""
</file>

<file path="backend/app/api/__init__.py">
"""
API routes module.
"""
</file>

<file path="backend/app/api/routes.py">
"""
REST API routes for research jobs.
"""
import logging
from datetime import datetime
from typing import List
from fastapi import APIRouter, HTTPException, BackgroundTasks
from app.models.research_job import ResearchJob, ResearchJobCreate, ResearchJobStatus
from app.orchestrator.research_engine import ResearchEngine
from app.api.websocket import manager as connection_manager
from app.db.supabase_client import create_job, get_job, list_jobs

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api/research", tags=["research"])
research_engine = ResearchEngine()


@router.post("/jobs", response_model=ResearchJob)
async def create_research_job(
    job_data: ResearchJobCreate,
    background_tasks: BackgroundTasks
) -> ResearchJob:
    """
    Create a new research job.
    
    Args:
        job_data: Research job creation data
        background_tasks: FastAPI background tasks
        
    Returns:
        Created research job
    """
    # Store job in database
    job_id = await create_job(job_data.query, job_data.context)
    logger.info(f"Created research job {job_id} with query: {job_data.query}")
    
    # Start research in background with WebSocket support
    background_tasks.add_task(
        research_engine.start_research,
        job_data.query,
        job_id,
        connection_manager
    )
    
    # Fetch and return the created job
    db_job = await get_job(job_id)
    if not db_job:
        raise HTTPException(status_code=500, detail="Failed to retrieve created job")
    
    # Convert database dict to ResearchJob model
    job = _dict_to_research_job(db_job)
    return job


@router.get("/jobs/{job_id}", response_model=ResearchJob)
async def get_research_job(job_id: str) -> ResearchJob:
    """
    Get a research job by ID.
    
    Args:
        job_id: Research job ID
        
    Returns:
        Research job
    """
    # Fetch job from database
    db_job = await get_job(job_id)
    
    if not db_job:
        raise HTTPException(status_code=404, detail="Job not found")
    
    # Convert database dict to ResearchJob model
    job = _dict_to_research_job(db_job)
    return job


@router.get("/jobs", response_model=List[ResearchJob])
async def list_research_jobs(
    skip: int = 0,
    limit: int = 100
) -> List[ResearchJob]:
    """
    List all research jobs.
    
    Args:
        skip: Number of jobs to skip
        limit: Maximum number of jobs to return
        
    Returns:
        List of research jobs
    """
    # Fetch jobs from database with pagination
    db_jobs = await list_jobs(skip, limit)
    
    # Convert list of dicts to list of ResearchJob models
    jobs = [_dict_to_research_job(db_job) for db_job in db_jobs]
    return jobs


def _dict_to_research_job(db_job: dict) -> ResearchJob:
    """
    Convert database dictionary to ResearchJob model.
    
    Args:
        db_job: Database job dictionary
        
    Returns:
        ResearchJob model instance
    """
    # Parse timestamps
    created_at = db_job.get("created_at")
    if isinstance(created_at, str):
        created_at = datetime.fromisoformat(created_at.replace("Z", "+00:00"))
    
    updated_at = db_job.get("updated_at")
    if updated_at and isinstance(updated_at, str):
        updated_at = datetime.fromisoformat(updated_at.replace("Z", "+00:00"))
    
    completed_at = db_job.get("completed_at")
    if completed_at and isinstance(completed_at, str):
        completed_at = datetime.fromisoformat(completed_at.replace("Z", "+00:00"))
    
    return ResearchJob(
        id=str(db_job["id"]),
        query=db_job["query"],
        status=ResearchJobStatus(db_job["status"]),
        progress=float(db_job.get("progress", 0.0)),
        created_at=created_at or datetime.utcnow(),
        updated_at=updated_at,
        completed_at=completed_at,
        sources=db_job.get("sources", []),
        iterations=db_job.get("iterations", []),
        report=db_job.get("report"),
        error=db_job.get("error")
    )


@router.delete("/jobs/{job_id}")
async def delete_research_job(job_id: str) -> dict:
    """
    Delete a research job.
    
    Args:
        job_id: Research job ID
        
    Returns:
        Success message
    """
    # TODO: Delete job from database
    
    return {"message": "Job deleted"}
</file>

<file path="backend/app/api/websocket.py">
"""
WebSocket handler for real-time research updates.
"""
import logging
import json
from typing import Set, Dict, Optional
from collections import defaultdict
from fastapi import WebSocket, WebSocketDisconnect

logger = logging.getLogger(__name__)


class ConnectionManager:
    """Manages WebSocket connections for real-time updates."""
    
    def __init__(self):
        """Initialize the connection manager."""
        self.active_connections: Dict[str, Set[WebSocket]] = defaultdict(set)
    
    async def connect(self, websocket: WebSocket, job_id: str) -> None:
        """
        Accept a new WebSocket connection and associate it with a job_id.
        
        Args:
            websocket: WebSocket connection
            job_id: Research job ID to associate with this connection
        """
        await websocket.accept()
        self.active_connections[job_id].add(websocket)
        logger.info(f"WebSocket connected for job {job_id}. Total connections for job: {len(self.active_connections[job_id])}")
    
    def disconnect(self, websocket: WebSocket, job_id: str) -> None:
        """
        Remove a WebSocket connection from the specified job.
        
        Args:
            websocket: WebSocket connection to remove
            job_id: Research job ID associated with this connection
        """
        if job_id in self.active_connections:
            self.active_connections[job_id].discard(websocket)
            # Clean up empty sets
            if not self.active_connections[job_id]:
                del self.active_connections[job_id]
        logger.info(f"WebSocket disconnected for job {job_id}")
    
    async def send_personal_message(self, message: dict, websocket: WebSocket) -> None:
        """Send a message to a specific WebSocket connection."""
        try:
            await websocket.send_json(message)
        except Exception as e:
            logger.error(f"Error sending WebSocket message: {e}")
    
    async def _broadcast_to_job(self, job_id: str, message: dict) -> None:
        """
        Internal method to broadcast a message to all connections for a specific job.
        
        Args:
            job_id: Research job ID to broadcast to
            message: Message dictionary to send
        """
        if job_id not in self.active_connections:
            logger.debug(f"No active connections for job {job_id}")
            return
        
        disconnected = set()
        connections = self.active_connections[job_id].copy()  # Copy to avoid modification during iteration
        
        for connection in connections:
            try:
                await connection.send_json(message)
            except Exception as e:
                logger.error(f"Error broadcasting to WebSocket for job {job_id}: {e}")
                disconnected.add(connection)
        
        # Remove disconnected connections
        for connection in disconnected:
            self.active_connections[job_id].discard(connection)
        
        # Clean up empty sets
        if not self.active_connections[job_id]:
            del self.active_connections[job_id]
    
    async def broadcast_status(self, job_id: str, status: str, progress: Optional[float] = None) -> None:
        """
        Broadcast status update to all clients connected to a specific job.
        
        Args:
            job_id: Research job ID
            status: Job status (pending, running, completed, failed, cancelled)
            progress: Optional progress percentage (0-100)
        """
        message = {
            "type": "status",
            "job_id": job_id,
            "data": {
                "status": status,
                "progress": progress
            }
        }
        await self._broadcast_to_job(job_id, message)
    
    async def broadcast_iteration(self, job_id: str, iteration_data: dict) -> None:
        """
        Broadcast iteration update to all clients connected to a specific job.
        
        Args:
            job_id: Research job ID
            iteration_data: Dictionary containing iteration information
        """
        message = {
            "type": "iteration",
            "job_id": job_id,
            "data": iteration_data
        }
        await self._broadcast_to_job(job_id, message)
    
    async def broadcast_source(self, job_id: str, source_data: dict) -> None:
        """
        Broadcast source discovery update to all clients connected to a specific job.
        
        Args:
            job_id: Research job ID
            source_data: Dictionary containing source information
        """
        message = {
            "type": "source",
            "job_id": job_id,
            "data": source_data
        }
        await self._broadcast_to_job(job_id, message)
    
    async def broadcast_report(self, job_id: str, report: str) -> None:
        """
        Broadcast final report to all clients connected to a specific job.
        
        Args:
            job_id: Research job ID
            report: Final research report text
        """
        message = {
            "type": "report",
            "job_id": job_id,
            "data": {
                "report": report
            }
        }
        await self._broadcast_to_job(job_id, message)
    
    async def broadcast_error(self, job_id: str, error_message: str) -> None:
        """
        Broadcast error message to all clients connected to a specific job.
        
        Args:
            job_id: Research job ID
            error_message: Error message to send
        """
        message = {
            "type": "error",
            "job_id": job_id,
            "data": {
                "error": error_message
            }
        }
        await self._broadcast_to_job(job_id, message)
    
    async def broadcast(self, message: dict) -> None:
        """
        Broadcast a message to all connected WebSocket clients (legacy method).
        Note: Prefer using job-specific broadcast methods instead.
        
        Args:
            message: Message dictionary to send
        """
        job_id = message.get("job_id")
        if job_id:
            await self._broadcast_to_job(job_id, message)
        else:
            logger.warning("Broadcast message missing job_id, skipping")


manager = ConnectionManager()


async def websocket_endpoint(websocket: WebSocket, job_id: str) -> None:
    """
    WebSocket endpoint for real-time research job updates.
    
    Args:
        websocket: WebSocket connection
        job_id: Research job ID to subscribe to
    """
    try:
        await manager.connect(websocket, job_id)
        logger.info(f"WebSocket connection established for job {job_id}")
    except Exception as e:
        logger.error(f"Error accepting WebSocket connection for job {job_id}: {e}", exc_info=True)
        return
    
    try:
        # Send initial connection confirmation
        try:
            await manager.send_personal_message({
                "type": "connected",
                "job_id": job_id,
                "data": {"message": "WebSocket connected successfully"}
            }, websocket)
            logger.info(f"Sent initial connection message for job {job_id}")
        except Exception as e:
            logger.error(f"Error sending initial connection message for job {job_id}: {e}", exc_info=True)
            # Don't return, continue to the message loop
        
        # Keep connection alive and listen for messages
        while True:
            try:
                # Wait for message from client (this will block until a message is received)
                data = await websocket.receive_text()
                logger.debug(f"Received raw message for job {job_id}: {data[:100]}...")
                
                try:
                    message = json.loads(data)
                    logger.info(f"Received WebSocket message for job {job_id}: {message}")
                    
                    # Handle client messages (ping, pause, cancel, etc.)
                    msg_type = message.get("type", "")
                    if msg_type == "ping":
                        # Respond to ping with pong
                        try:
                            await manager.send_personal_message({
                                "type": "pong",
                                "job_id": job_id,
                                "data": {"message": "pong"}
                            }, websocket)
                            logger.debug(f"Sent pong response for job {job_id}")
                        except Exception as e:
                            logger.error(f"Error sending pong for job {job_id}: {e}", exc_info=True)
                    else:
                        # Echo other messages back or handle them
                        logger.debug(f"Unhandled message type: {msg_type} for job {job_id}")
                        
                except json.JSONDecodeError as e:
                    logger.error(f"Invalid JSON received from client for job {job_id}: {e}. Data: {data[:100]}")
                    try:
                        await manager.send_personal_message({
                            "type": "error",
                            "job_id": job_id,
                            "data": {"error": "Invalid JSON format"}
                        }, websocket)
                    except Exception as send_error:
                        logger.error(f"Error sending error message for job {job_id}: {send_error}", exc_info=True)
                    
            except WebSocketDisconnect:
                # This is expected when client disconnects
                logger.info(f"WebSocket disconnected normally for job {job_id}")
                break
            except Exception as e:
                logger.error(f"Error processing WebSocket message for job {job_id}: {e}", exc_info=True)
                # Continue the loop to keep connection alive unless it's a disconnect
                # Check if websocket is still connected
                try:
                    # Try to send an error message to see if connection is still alive
                    await manager.send_personal_message({
                        "type": "error",
                        "job_id": job_id,
                        "data": {"error": "Internal error processing message"}
                    }, websocket)
                except Exception:
                    # Connection is likely dead, break out of loop
                    logger.warning(f"Connection appears to be dead for job {job_id}, breaking loop")
                    break
                continue
            
    except WebSocketDisconnect:
        logger.info(f"WebSocket disconnected for job {job_id}")
    except Exception as e:
        logger.error(f"Unexpected error in WebSocket endpoint for job {job_id}: {e}", exc_info=True)
    finally:
        # Always clean up the connection
        try:
            manager.disconnect(websocket, job_id)
        except Exception as e:
            logger.error(f"Error during WebSocket disconnect cleanup for job {job_id}: {e}", exc_info=True)
</file>

<file path="backend/app/main.py">
"""
FastAPI application entry point.
"""
import logging
from fastapi import FastAPI, WebSocket
from fastapi.middleware.cors import CORSMiddleware
from app.config import settings
from app.api import routes
from app.api.websocket import websocket_endpoint

# Configure logging
logging.basicConfig(
    level=getattr(logging, settings.LOG_LEVEL),
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# Create FastAPI app
app = FastAPI(
    title="Agent Bletchley API",
    description="Agentic web research system for investment due diligence",
    version="1.0.0",
)

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=[settings.FRONTEND_URL],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include routers
app.include_router(routes.router)


@app.get("/")
async def root() -> dict[str, str]:
    """Root endpoint returning greeting."""
    return {"message": "Hello Agent Bletchley"}


@app.get("/health")
async def health_check() -> dict[str, str]:
    """Health check endpoint."""
    return {"status": "healthy"}


@app.websocket("/ws/research/{job_id}")
async def websocket_route(websocket: WebSocket, job_id: str) -> None:
    """WebSocket endpoint for real-time research updates."""
    await websocket_endpoint(websocket, job_id)


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "app.main:app",
        host=settings.BACKEND_HOST,
        port=settings.BACKEND_PORT,
        reload=True,
    )
</file>

<file path="backend/app/models/__init__.py">
"""
Data models module.
"""
</file>

<file path="backend/app/models/research_job.py">
"""
Pydantic models for research jobs.
"""
from datetime import datetime
from typing import Optional, List, Dict, Any
from enum import Enum
from pydantic import BaseModel, Field


class ResearchJobStatus(str, Enum):
    """Research job status enumeration."""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"


class ResearchJobCreate(BaseModel):
    """Model for creating a new research job."""
    query: str = Field(..., description="The research query or question")
    context: Optional[Dict[str, Any]] = Field(None, description="Optional context for the research")


class ResearchJob(BaseModel):
    """Model representing a research job."""
    id: str = Field(..., description="Unique job identifier")
    query: str = Field(..., description="The research query")
    status: ResearchJobStatus = Field(..., description="Current job status")
    progress: float = Field(0.0, ge=0.0, le=100.0, description="Progress percentage")
    created_at: datetime = Field(default_factory=datetime.utcnow, description="Job creation timestamp")
    updated_at: Optional[datetime] = Field(None, description="Last update timestamp")
    completed_at: Optional[datetime] = Field(None, description="Completion timestamp")
    sources: List[Dict[str, Any]] = Field(default_factory=list, description="Gathered sources")
    iterations: List[Dict[str, Any]] = Field(default_factory=list, description="Research iterations")
    report: Optional[str] = Field(None, description="Final research report")
    error: Optional[str] = Field(None, description="Error message if job failed")
    
    class Config:
        """Pydantic config."""
        use_enum_values = True
</file>

<file path="backend/app/orchestrator/__init__.py">
"""
Research orchestrator module.
"""
</file>

<file path="backend/app/orchestrator/research_engine.py">
"""
Main research engine that orchestrates the research process.
"""
import logging
import asyncio
from typing import Dict, List, Any, Optional, TYPE_CHECKING
from app.models.research_job import ResearchJob, ResearchJobStatus
from app.orchestrator.tongyi_client import TongyiClient
from app.tools.tool_registry import ToolRegistry
from app.db.supabase_client import (
    update_job_status as db_update_job_status,
    update_job_report,
    add_iteration,
    add_source
)

if TYPE_CHECKING:
    from app.api.websocket import ConnectionManager

logger = logging.getLogger(__name__)


class ResearchEngine:
    """
    Main research engine that coordinates AI agent research activities.
    
    TODO: Implement the main research loop that:
    1. Receives research query
    2. Uses Tongyi DeepResearch agent to plan research steps
    3. Executes tools (web search, web fetch) based on agent decisions
    4. Aggregates and synthesizes results
    5. Returns comprehensive research report
    """
    
    def __init__(self):
        """Initialize the research engine."""
        self.tongyi_client = TongyiClient()
        self.tool_registry = ToolRegistry()
    
    async def start_research(
        self, 
        query: str, 
        job_id: str, 
        connection_manager: Optional["ConnectionManager"] = None
    ) -> ResearchJob:
        """
        Start a new research job.
        
        Args:
            query: The research query/question
            job_id: Unique identifier for this research job
            connection_manager: Optional WebSocket connection manager for real-time updates
            
        Returns:
            ResearchJob instance with initial status
        """
        # TODO: Create initial research job in database
        # TODO: Initialize research context and state
        
        logger.info(f"Starting research job {job_id} with query: {query}")
        
        # Update job status to running in database
        await db_update_job_status(job_id, "running", 0.0)
        
        job = ResearchJob(
            id=job_id,
            query=query,
            status=ResearchJobStatus.RUNNING,
        )
        
        # Broadcast initial RUNNING status
        if connection_manager:
            asyncio.create_task(
                connection_manager.broadcast_status(job_id, ResearchJobStatus.RUNNING.value, 0.0)
            )
        
        # Begin research loop
        asyncio.create_task(self._run_research_loop(job_id, query, connection_manager))
        
        return job
    
    async def _run_research_loop(
        self,
        job_id: str,
        query: str,
        connection_manager: Optional["ConnectionManager"] = None
    ) -> None:
        """
        Run the main research loop (simulated for now).
        
        Args:
            job_id: Research job ID
            query: Research query
            connection_manager: Optional WebSocket connection manager
        """
        max_iterations = 20
        iteration_count = 0
        
        try:
            # Update status to RUNNING in database (already done in start_research, but ensure it's set)
            await db_update_job_status(job_id, "running", 0.0)
            
            # Broadcast status to WebSocket
            if connection_manager:
                asyncio.create_task(
                    connection_manager.broadcast_status(job_id, ResearchJobStatus.RUNNING.value, 0.0)
                )
            
            # TODO: Implement actual research loop with Tongyi agent
            # For now, simulate iterations
            while iteration_count < max_iterations:
                iteration_count += 1
                progress = min(100, (iteration_count / max_iterations) * 100)
                
                # Execute research step
                step_result = await self.execute_research_step(
                    job_id,
                    {"step": iteration_count, "query": query}
                )
                
                # Persist iteration to database
                action = step_result.get("action", "research")
                results = step_result
                await add_iteration(job_id, iteration_count, action, results)
                
                # Broadcast iteration update
                if connection_manager:
                    iteration_data = {
                        "id": f"{job_id}-iter-{iteration_count}",
                        "step": iteration_count,
                        "action": action,
                        "timestamp": step_result.get("timestamp"),
                        "results": step_result
                    }
                    asyncio.create_task(
                        connection_manager.broadcast_iteration(job_id, iteration_data)
                    )
                
                # Update progress in database
                await db_update_job_status(job_id, "running", progress)
                
                # Broadcast progress update
                if connection_manager:
                    asyncio.create_task(
                        connection_manager.broadcast_status(job_id, ResearchJobStatus.RUNNING.value, progress)
                    )
                
                # Simulate discovering sources
                if iteration_count % 3 == 0:  # Every 3rd iteration
                    source_url = f"https://example.com/source-{iteration_count}"
                    source_title = f"Source {iteration_count}"
                    source_snippet = f"Relevant information for: {query}"
                    
                    # Persist source to database
                    await add_source(job_id, source_url, source_title, source_snippet)
                    
                    # Broadcast source
                    if connection_manager:
                        source_data = {
                            "url": source_url,
                            "title": source_title,
                            "snippet": source_snippet,
                            "fetched_at": None
                        }
                        asyncio.create_task(
                            connection_manager.broadcast_source(job_id, source_data)
                        )
                
                # Simulate delay between iterations
                await asyncio.sleep(0.1)
            
            # Synthesize results
            sources = []  # TODO: Collect actual sources from database
            report = await self.synthesize_results(job_id, sources)
            
            # Persist report to database
            await update_job_report(job_id, report)
            
            # Broadcast final report
            if connection_manager:
                asyncio.create_task(
                    connection_manager.broadcast_report(job_id, report)
                )
            
            # Update status to COMPLETED in database
            await db_update_job_status(job_id, "completed", 100.0)
            
            # Broadcast completion status
            if connection_manager:
                asyncio.create_task(
                    connection_manager.broadcast_status(job_id, ResearchJobStatus.COMPLETED.value, 100.0)
                )
                
        except Exception as e:
            logger.error(f"Error in research loop for job {job_id}: {e}", exc_info=True)
            
            # Update status to FAILED in database
            await db_update_job_status(job_id, "failed", None)
            
            # Broadcast error
            if connection_manager:
                asyncio.create_task(
                    connection_manager.broadcast_error(job_id, str(e))
                )
                asyncio.create_task(
                    connection_manager.broadcast_status(job_id, ResearchJobStatus.FAILED.value, None)
                )
    
    async def execute_research_step(
        self,
        job_id: str,
        step: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Execute a single research step.
        
        Args:
            job_id: The research job ID
            step: Step definition from AI agent
            
        Returns:
            Step execution results
        """
        # TODO: Parse step instruction from agent
        # TODO: Select appropriate tool(s) from registry
        # TODO: Execute tool with parameters
        # TODO: Return results to agent for next step
        
        from datetime import datetime
        
        logger.info(f"Executing research step {step.get('step', 'unknown')} for job {job_id}")
        return {
            "status": "completed",
            "action": step.get("action", "research"),
            "timestamp": datetime.utcnow().isoformat(),
            "step": step.get("step")
        }
    
    async def synthesize_results(
        self,
        job_id: str,
        sources: List[Dict[str, Any]]
    ) -> str:
        """
        Synthesize research results into final report.
        
        Args:
            job_id: The research job ID
            sources: List of gathered sources and findings
            
        Returns:
            Final research report as formatted text
        """
        # TODO: Use Tongyi agent to synthesize all gathered information
        # TODO: Structure findings into organized report
        # TODO: Include source citations
        # Note: Report is saved to database in _run_research_loop after this function returns
        
        logger.info(f"Synthesizing results for job {job_id}")
        return "Research synthesis pending implementation"
    
    async def update_job_status(
        self,
        job_id: str,
        status: ResearchJobStatus,
        progress: Optional[float] = None,
        connection_manager: Optional["ConnectionManager"] = None
    ) -> None:
        """
        Update research job status.
        
        Args:
            job_id: The research job ID
            status: New status
            progress: Optional progress percentage (0-100)
            connection_manager: Optional WebSocket connection manager for real-time updates
        """
        # Update job status in database
        await db_update_job_status(job_id, status.value, progress)
        
        logger.info(f"Updating job {job_id} status to {status}")
        
        # Broadcast status update via WebSocket
        if connection_manager:
            asyncio.create_task(
                connection_manager.broadcast_status(job_id, status.value, progress)
            )
</file>

<file path="backend/app/orchestrator/tongyi_client.py">
"""
OpenRouter/Tongyi DeepResearch client interface.
"""
import logging
import httpx
from typing import Dict, List, Any, Optional
from app.config import settings

logger = logging.getLogger(__name__)


class TongyiClient:
    """
    Client for interacting with OpenRouter API for Tongyi DeepResearch.
    
    TODO: Implement full integration with OpenRouter API:
    1. Chat completions with tool calling support
    2. Streaming responses for real-time updates
    3. Tool definitions from ToolRegistry
    4. Error handling and retries
    """
    
    def __init__(self):
        """Initialize the Tongyi client."""
        self.api_key = settings.OPENROUTER_API_KEY
        self.base_url = settings.OPENROUTER_BASE_URL
        self.model = settings.TONGYI_MODEL
        self.client = httpx.AsyncClient(
            base_url=self.base_url,
            headers={
                "Authorization": f"Bearer {self.api_key}",
                "HTTP-Referer": "https://github.com/agent-bletchley",
                "X-Title": "Agent Bletchley",
            },
            timeout=60.0,
        )
    
    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        tools: Optional[List[Dict[str, Any]]] = None,
        stream: bool = False
    ) -> Dict[str, Any]:
        """
        Send chat completion request to OpenRouter.
        
        Args:
            messages: List of message dictionaries with 'role' and 'content'
            tools: Optional list of tool definitions for function calling
            stream: Whether to stream the response
            
        Returns:
            API response dictionary
        """
        # TODO: Implement chat completion with tool calling
        # TODO: Handle streaming responses if stream=True
        # TODO: Parse tool calls from response
        # TODO: Implement retry logic for failed requests
        
        payload = {
            "model": self.model,
            "messages": messages,
        }
        
        if tools:
            payload["tools"] = tools
        
        if stream:
            payload["stream"] = True
        
        logger.info(f"Sending chat completion request to {self.model}")
        
        try:
            response = await self.client.post("/chat/completions", json=payload)
            response.raise_for_status()
            return response.json()
        except httpx.HTTPError as e:
            logger.error(f"OpenRouter API error: {e}")
            raise
    
    async def send_research_query(
        self,
        query: str,
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Send a research query to the Tongyi agent.
        
        Args:
            query: The research question/query
            context: Optional context from previous research steps
            
        Returns:
            Agent response with potential tool calls
        """
        # TODO: Format research query with context
        # TODO: Include available tools from ToolRegistry
        # TODO: Parse agent response for tool calls or final answer
        
        messages = [
            {
                "role": "system",
                "content": "You are a research assistant for investment due diligence. Use available tools to gather comprehensive information."
            },
            {
                "role": "user",
                "content": query
            }
        ]
        
        tools = []  # TODO: Get tools from ToolRegistry
        
        return await self.chat_completion(messages=messages, tools=tools)
    
    async def close(self) -> None:
        """Close the HTTP client."""
        await self.client.aclose()
</file>

<file path="backend/app/tools/__init__.py">
"""
Research tools module.
"""
</file>

<file path="backend/app/tools/tool_registry.py">
"""
Tool registry for defining available tools for the AI agent.
"""
from typing import List, Dict, Any
from app.tools.web_search import WebSearchTool
from app.tools.web_fetch import WebFetchTool


class ToolRegistry:
    """
    Registry of available tools for the research agent.
    
    Defines tool schemas compatible with OpenRouter function calling.
    """
    
    def __init__(self):
        """Initialize the tool registry."""
        self.web_search = WebSearchTool()
        self.web_fetch = WebFetchTool()
    
    def get_tool_definitions(self) -> List[Dict[str, Any]]:
        """
        Get tool definitions in OpenRouter function calling format.
        
        Returns:
            List of tool definition dictionaries
        """
        return [
            {
                "type": "function",
                "function": {
                    "name": "web_search",
                    "description": "Search the web for information using Brave Search. Use this to find relevant sources and articles about a topic.",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "query": {
                                "type": "string",
                                "description": "The search query string"
                            },
                            "count": {
                                "type": "integer",
                                "description": "Number of results to return (default: 10, max: 20)",
                                "default": 10
                            }
                        },
                        "required": ["query"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "web_fetch",
                    "description": "Fetch and parse content from a web URL. Use this to get the full text content of articles or web pages found through search.",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "url": {
                                "type": "string",
                                "description": "The URL to fetch and parse"
                            },
                            "mode": {
                                "type": "string",
                                "description": "Reader mode: 'reader' for parsed content, 'raw' for raw HTML",
                                "enum": ["reader", "raw"],
                                "default": "reader"
                            }
                        },
                        "required": ["url"]
                    }
                }
            }
        ]
    
    async def execute_tool(
        self,
        tool_name: str,
        parameters: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Execute a tool by name with given parameters.
        
        Args:
            tool_name: Name of the tool to execute
            parameters: Tool parameters
            
        Returns:
            Tool execution result
        """
        if tool_name == "web_search":
            query = parameters.get("query")
            count = parameters.get("count", 10)
            results = await self.web_search.search(query=query, count=count)
            return {
                "tool": "web_search",
                "results": results
            }
        elif tool_name == "web_fetch":
            url = parameters.get("url")
            mode = parameters.get("mode", "reader")
            content = await self.web_fetch.fetch(url=url, mode=mode)
            return {
                "tool": "web_fetch",
                "content": content
            }
        else:
            raise ValueError(f"Unknown tool: {tool_name}")
</file>

<file path="backend/app/tools/web_fetch.py">
"""
Jina Reader API wrapper for fetching and parsing web content.
"""
import logging
import httpx
import re
from typing import Dict, Any, Optional
from app.config import settings

logger = logging.getLogger(__name__)


class WebFetchTool:
    """
    Tool for fetching and parsing web content using Jina Reader API.
    
    Provides async web content fetching functionality with proper error handling,
    response parsing, and logging.
    """
    
    def __init__(self):
        """Initialize the web fetch tool."""
        self.api_key = settings.JINA_READER_API_KEY
        self.base_url = "https://r.jina.ai"
        self.client = httpx.AsyncClient(
            headers={
                "Authorization": f"Bearer {self.api_key}",
            },
            timeout=60.0,
        )
    
    def _truncate_content(self, content: str, max_words: int = 10000) -> str:
        """
        Truncate content to a maximum number of words, preserving word boundaries.
        
        Args:
            content: The content to truncate
            max_words: Maximum number of words to keep
            
        Returns:
            Truncated content string
        """
        words = content.split()
        if len(words) <= max_words:
            return content
        return " ".join(words[:max_words])
    
    def _extract_title_from_markdown(self, markdown_content: str) -> str:
        """
        Extract title from markdown content (first heading).
        
        Args:
            markdown_content: Markdown content string
            
        Returns:
            Extracted title or empty string
        """
        # Try to find first heading (# Title or ## Title)
        heading_match = re.search(r'^#+\s+(.+)$', markdown_content, re.MULTILINE)
        if heading_match:
            return heading_match.group(1).strip()
        return ""
    
    async def fetch(
        self,
        url: str,
        mode: str = "reader"
    ) -> Dict[str, Any]:
        """
        Fetch and parse content from a URL using Jina Reader API.
        
        Args:
            url: The URL to fetch (e.g., https://example.com)
            mode: Reader mode (reader, raw, etc.) - currently only "reader" is supported
            
        Returns:
            Dictionary with url, title, content, word_count on success.
            Dictionary with "error" key on failure.
        """
        # Validate input
        if not url or not url.strip():
            logger.warning("Empty URL provided")
            return {"error": "Empty URL provided"}
        
        # Ensure URL is properly formatted
        url = url.strip()
        if not url.startswith(("http://", "https://")):
            url = f"https://{url}"
        
        # Construct Jina API URL
        jina_url = f"{self.base_url}/{url}"
        
        logger.info(f"Fetching content from Jina Reader API: {url}")
        
        try:
            response = await self.client.get(jina_url)
            response.raise_for_status()
            
            # Jina Reader returns markdown/text content
            content = response.text
            
            # Extract title from markdown (first heading)
            title = self._extract_title_from_markdown(content)
            if not title:
                # Fallback: use URL domain or last part as title
                try:
                    from urllib.parse import urlparse
                    parsed = urlparse(url)
                    title = parsed.netloc or parsed.path.split("/")[-1] or url
                except Exception:
                    title = url
            
            # Truncate content to max 10,000 words
            truncated_content = self._truncate_content(content, max_words=10000)
            
            # Calculate word count (on truncated content if it was truncated)
            word_count = len(truncated_content.split())
            
            result = {
                "url": url,
                "title": title,
                "content": truncated_content,
                "word_count": word_count,
            }
            
            logger.info(f"Successfully fetched content from {url}: {word_count} words")
            return result
            
        except httpx.HTTPStatusError as e:
            logger.error(f"Jina Reader API HTTP error: {e.response.status_code} - {e.response.text}")
            return {"error": f"HTTP error {e.response.status_code}: Failed to fetch content"}
        except httpx.RequestError as e:
            logger.error(f"Jina Reader API request error: {e}")
            return {"error": f"Request error: Failed to fetch content"}
        except httpx.TimeoutException as e:
            logger.error(f"Jina Reader API timeout error: {e}")
            return {"error": "Request timeout: Failed to fetch content within 60 seconds"}
        except Exception as e:
            logger.error(f"Jina Reader API unexpected error: {e}", exc_info=True)
            return {"error": f"Unexpected error: {str(e)}"}
    
    async def close(self) -> None:
        """Close the HTTP client."""
        await self.client.aclose()
</file>

<file path="backend/app/tools/web_search.py">
"""
Brave Search API wrapper for web search functionality.
"""
import logging
import httpx
from typing import List, Dict, Any, Optional
from app.config import settings

logger = logging.getLogger(__name__)


class WebSearchTool:
    """
    Tool for performing web searches using Brave Search API.
    
    Provides async web search functionality with proper error handling,
    response parsing, and logging.
    """
    
    def __init__(self):
        """Initialize the web search tool."""
        self.api_key = settings.BRAVE_SEARCH_API_KEY
        self.base_url = "https://api.search.brave.com/res/v1"
        self.client = httpx.AsyncClient(
            headers={
                "X-Subscription-Token": self.api_key,
            },
            timeout=30.0,
        )
    
    async def search(
        self,
        query: str,
        count: int = 10,
        offset: int = 0,
        safesearch: str = "moderate"
    ) -> List[Dict[str, Any]]:
        """
        Perform a web search.
        
        Args:
            query: Search query string
            count: Number of results to return (max 20)
            offset: Offset for pagination
            safesearch: Safe search setting (off, moderate, strict)
            
        Returns:
            List of search result dictionaries with title, url, snippet.
            Returns empty list on error.
        """
        # Validate input
        if not query or not query.strip():
            logger.warning("Empty search query provided")
            return []
        
        # Limit count to max 10 per requirements
        count = min(count, 10)
        
        logger.info(f"Searching Brave Search API for: {query} (count={count})")
        
        params = {
            "q": query,
            "count": min(count, 20),  # Brave API supports up to 20, but we cap at 10
            "offset": offset,
            "safesearch": safesearch,
        }
        
        url = f"{self.base_url}/web/search"
        
        try:
            response = await self.client.get(url, params=params)
            response.raise_for_status()
            data = response.json()
            
            # Parse and format results
            raw_results = data.get("web", {}).get("results", [])
            
            # Extract and format results
            results = []
            for item in raw_results[:count]:
                result = {
                    "title": item.get("title", ""),
                    "url": item.get("url", ""),
                    "snippet": item.get("description", ""),
                }
                # Only add result if it has required fields
                if result["title"] and result["url"]:
                    results.append(result)
            
            logger.info(f"Brave Search API returned {len(results)} results for query: {query}")
            return results
            
        except httpx.HTTPStatusError as e:
            logger.error(f"Brave Search API HTTP error: {e.response.status_code} - {e.response.text}")
            return []
        except httpx.RequestError as e:
            logger.error(f"Brave Search API request error: {e}")
            return []
        except httpx.TimeoutException as e:
            logger.error(f"Brave Search API timeout error: {e}")
            return []
        except Exception as e:
            logger.error(f"Brave Search API unexpected error: {e}", exc_info=True)
            return []
    
    async def close(self) -> None:
        """Close the HTTP client."""
        await self.client.aclose()
</file>

<file path="backend/README.md">
# Agent Bletchley Backend

FastAPI backend for the Agent Bletchley research system.

## Setup

### Prerequisites

- Python 3.11 or higher
- Virtual environment (recommended)

### Installation

**IMPORTANT: Run all commands from the `backend` directory!**

1. Navigate to the backend directory:
```powershell
cd backend
```

2. Create a virtual environment (if it doesn't exist):
```bash
python -m venv venv
```

3. Activate the virtual environment:
```powershell
# On Windows PowerShell (from backend directory):
.\venv\Scripts\Activate.ps1

# If you get an execution policy error, run this first:
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser

# On Windows Command Prompt:
venv\Scripts\activate

# On macOS/Linux:
source venv/bin/activate
```

4. Install dependencies:
```bash
pip install -r requirements.txt
```

5. Configure environment variables:
```bash
# On Windows PowerShell:
Copy-Item .env.example .env

# On macOS/Linux:
cp .env.example .env

# Edit .env with your API keys and configuration
```

### Required Environment Variables

- `OPENROUTER_API_KEY` - Your OpenRouter API key
- `BRAVE_SEARCH_API_KEY` - Your Brave Search API key
- `JINA_READER_API_KEY` - Your Jina Reader API key
- `SUPABASE_URL` - Your Supabase project URL
- `SUPABASE_KEY` - Your Supabase anon key

### Running the Server

Development server with auto-reload:
```bash
uvicorn app.main:app --reload
```

Production server:
```bash
uvicorn app.main:app --host 0.0.0.0 --port 8000
```

The API will be available at `http://localhost:8000`

### API Documentation

Once the server is running, visit:
- Swagger UI: `http://localhost:8000/docs`
- ReDoc: `http://localhost:8000/redoc`

### Project Structure

```
backend/
├── app/
│   ├── main.py              # FastAPI application entry point
│   ├── config.py            # Configuration management
│   ├── orchestrator/         # Research orchestration logic
│   ├── tools/                # External API integrations
│   ├── api/                  # REST API routes
│   └── models/               # Pydantic data models
├── requirements.txt          # Python dependencies
└── README.md                 # This file
```

### Development

The codebase uses:
- Async/await patterns throughout
- Type hints on all functions
- TODO comments marking areas for implementation
- Proper logging configuration
</file>

<file path="backend/requirements.txt">
fastapi==0.115.0
uvicorn[standard]==0.32.0
httpx==0.27.2
python-dotenv==1.0.1
pydantic==2.9.2
websockets==13.1
supabase==2.8.0
</file>

<file path="backend/start.ps1">
# PowerShell script to start the backend development server
# Run this script from the backend directory: .\start.ps1

Write-Host "Agent Bletchley Backend - Starting..." -ForegroundColor Cyan

# Change to backend directory if not already there
$scriptPath = Split-Path -Parent $MyInvocation.MyCommand.Path
Set-Location $scriptPath

# Check if venv exists
if (-not (Test-Path "venv")) {
    Write-Host "Creating virtual environment..." -ForegroundColor Yellow
    python -m venv venv
}

# Activate virtual environment
Write-Host "Activating virtual environment..." -ForegroundColor Yellow
& "$scriptPath\venv\Scripts\Activate.ps1"

# Check if .env exists
if (-not (Test-Path ".env")) {
    Write-Host "Creating .env file from .env.example..." -ForegroundColor Yellow
    Copy-Item .env.example .env
    Write-Host "Please edit .env file with your API keys!" -ForegroundColor Red
}

# Install/upgrade dependencies
Write-Host "Installing dependencies..." -ForegroundColor Yellow
pip install -r requirements.txt

# Run the server
Write-Host "Starting FastAPI server..." -ForegroundColor Green
uvicorn app.main:app --reload
</file>

<file path="frontend/.env.local.example">
# Backend API URL
NEXT_PUBLIC_BACKEND_URL=http://localhost:8000

# WebSocket URL
NEXT_PUBLIC_WS_URL=ws://localhost:8000

# Supabase Configuration
NEXT_PUBLIC_SUPABASE_URL=your_supabase_project_url
NEXT_PUBLIC_SUPABASE_ANON_KEY=your_supabase_anon_key
</file>

<file path="frontend/app/api/research/route.ts">
/**
 * API route for proxying requests to the backend.
 */
import { NextRequest, NextResponse } from "next/server";

const BACKEND_URL = process.env.NEXT_PUBLIC_BACKEND_URL || "http://localhost:8000";

export async function GET(request: NextRequest) {
  try {
    // TODO: Implement GET request to backend
    const response = await fetch(`${BACKEND_URL}/api/research/jobs`);
    
    if (!response.ok) {
      throw new Error(`Backend API error: ${response.statusText}`);
    }
    
    const data = await response.json();
    return NextResponse.json(data);
  } catch (error) {
    console.error("API route error:", error);
    return NextResponse.json(
      { error: "Failed to fetch research jobs" },
      { status: 500 }
    );
  }
}

export async function POST(request: NextRequest) {
  try {
    const body = await request.json();
    
    // TODO: Implement POST request to backend
    const response = await fetch(`${BACKEND_URL}/api/research/jobs`, {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
      },
      body: JSON.stringify(body),
    });
    
    if (!response.ok) {
      throw new Error(`Backend API error: ${response.statusText}`);
    }
    
    const data = await response.json();
    return NextResponse.json(data);
  } catch (error) {
    console.error("API route error:", error);
    return NextResponse.json(
      { error: "Failed to create research job" },
      { status: 500 }
    );
  }
}
</file>

<file path="frontend/app/globals.css">
@tailwind base;
@tailwind components;
@tailwind utilities;

:root {
  --foreground-rgb: 0, 0, 0;
  --background-start-rgb: 214, 219, 220;
  --background-end-rgb: 255, 255, 255;
}

@media (prefers-color-scheme: dark) {
  :root {
    --foreground-rgb: 255, 255, 255;
    --background-start-rgb: 0, 0, 0;
    --background-end-rgb: 0, 0, 0;
  }
}

body {
  color: rgb(var(--foreground-rgb));
  background: linear-gradient(
      to bottom,
      transparent,
      rgb(var(--background-end-rgb))
    )
    rgb(var(--background-start-rgb));
}

@layer utilities {
  .text-balance {
    text-wrap: balance;
  }
}
</file>

<file path="frontend/app/layout.tsx">
/**
 * Root layout component with dark mode support.
 */
import type { Metadata } from "next";
import { Inter } from "next/font/google";
import "./globals.css";

const inter = Inter({ subsets: ["latin"] });

export const metadata: Metadata = {
  title: "Agent Bletchley",
  description: "Agentic web research system for investment due diligence",
};

export default function RootLayout({
  children,
}: Readonly<{
  children: React.ReactNode;
}>) {
  return (
    <html lang="en" suppressHydrationWarning>
      <body className={`${inter.className} antialiased`}>{children}</body>
    </html>
  );
}
</file>

<file path="frontend/app/page.tsx">
/**
 * Home page displaying greeting.
 */
"use client";

import React from "react";

export default function Home() {
  return (
    <main className="flex min-h-screen flex-col items-center justify-center p-24">
      <div className="text-center">
        <h1 className="text-4xl font-bold text-gray-900 dark:text-white mb-4">
          Hello Agent Bletchley
        </h1>
        <p className="text-lg text-gray-600 dark:text-gray-400">
          Agentic web research system for investment due diligence
        </p>
      </div>
    </main>
  );
}
</file>

<file path="frontend/app/research/[jobId]/page.tsx">
/**
 * Research dashboard page for viewing a specific research job.
 */
"use client";

import React, { useEffect, useState } from "react";
import { useParams } from "next/navigation";
import type { ResearchJob } from "@/types/research";
import ProgressBar from "@/components/ProgressBar";
import IterationCard from "@/components/IterationCard";
import SourcePanel from "@/components/SourcePanel";
import OutputViewer from "@/components/OutputViewer";
import { useWebSocket } from "@/lib/websocket";

export default function ResearchJobPage() {
  const params = useParams();
  const jobId = params.jobId as string;
  const [job, setJob] = useState<ResearchJob | null>(null);
  const [loading, setLoading] = useState(true);

  // TODO: Fetch job data from API
  useEffect(() => {
    // TODO: Implement API call to fetch job
    setLoading(false);
  }, [jobId]);

  // WebSocket connection for real-time updates
  const { isConnected, lastMessage } = useWebSocket({
    jobId,
    onMessage: (message) => {
      // TODO: Update job state based on WebSocket messages
      console.log("WebSocket message:", message);
    },
  });

  if (loading) {
    return (
      <div className="flex items-center justify-center min-h-screen">
        <div className="text-gray-500 dark:text-gray-400">Loading...</div>
      </div>
    );
  }

  if (!job) {
    return (
      <div className="flex items-center justify-center min-h-screen">
        <div className="text-gray-500 dark:text-gray-400">Job not found</div>
      </div>
    );
  }

  return (
    <div className="container mx-auto px-4 py-8 max-w-6xl">
      <div className="mb-8">
        <h1 className="text-3xl font-bold text-gray-900 dark:text-white mb-2">
          Research: {job.query}
        </h1>
        <div className="flex items-center gap-4">
          <span
            className={`px-3 py-1 rounded-full text-sm font-medium ${
              isConnected
                ? "bg-green-100 text-green-800 dark:bg-green-900 dark:text-green-200"
                : "bg-gray-100 text-gray-800 dark:bg-gray-900 dark:text-gray-200"
            }`}
          >
            {isConnected ? "Connected" : "Disconnected"}
          </span>
        </div>
      </div>

      <div className="mb-6">
        <ProgressBar progress={job.progress} status={job.status} />
      </div>

      <div className="grid grid-cols-1 lg:grid-cols-2 gap-6 mb-6">
        <div>
          <h2 className="text-xl font-semibold text-gray-900 dark:text-white mb-4">
            Research Iterations
          </h2>
          <div className="space-y-3">
            {job.iterations.map((iteration) => (
              <IterationCard key={iteration.id} iteration={iteration} />
            ))}
          </div>
        </div>

        <div>
          <h2 className="text-xl font-semibold text-gray-900 dark:text-white mb-4">
            Sources
          </h2>
          <SourcePanel sources={job.sources} />
        </div>
      </div>

      {job.report && (
        <div className="mt-6">
          <OutputViewer report={job.report} />
        </div>
      )}

      {job.error && (
        <div className="mt-6 p-4 bg-red-50 dark:bg-red-900/20 border border-red-200 dark:border-red-800 rounded-lg">
          <p className="text-red-800 dark:text-red-200">{job.error}</p>
        </div>
      )}
    </div>
  );
}
</file>

<file path="frontend/components/IterationCard.tsx">
/**
 * Component for displaying a research iteration.
 */
import React from "react";
import type { ResearchIteration } from "@/types/research";

interface IterationCardProps {
  iteration: ResearchIteration;
}

export default function IterationCard({ iteration }: IterationCardProps) {
  return (
    <div className="rounded-lg border border-gray-200 dark:border-gray-700 bg-white dark:bg-gray-800 p-4 shadow-sm">
      <div className="flex items-center justify-between mb-2">
        <span className="text-sm font-medium text-gray-500 dark:text-gray-400">
          Step {iteration.step}
        </span>
        <span className="text-xs text-gray-400 dark:text-gray-500">
          {new Date(iteration.timestamp).toLocaleTimeString()}
        </span>
      </div>
      <p className="text-sm text-gray-900 dark:text-gray-100">{iteration.action}</p>
      {/* TODO: Display iteration results if available */}
    </div>
  );
}
</file>

<file path="frontend/components/OutputViewer.tsx">
/**
 * Component for displaying the final research output.
 */
import React from "react";

interface OutputViewerProps {
  report: string;
}

export default function OutputViewer({ report }: OutputViewerProps) {
  return (
    <div className="rounded-lg border border-gray-200 dark:border-gray-700 bg-white dark:bg-gray-800 p-6">
      <h3 className="text-lg font-semibold text-gray-900 dark:text-gray-100 mb-4">
        Research Report
      </h3>
      <div className="prose dark:prose-invert max-w-none">
        <div className="whitespace-pre-wrap text-gray-700 dark:text-gray-300">
          {report}
        </div>
      </div>
    </div>
  );
}
</file>

<file path="frontend/components/ProgressBar.tsx">
/**
 * Animated progress bar component.
 */
"use client";

import React from "react";
import { motion } from "framer-motion";

interface ProgressBarProps {
  progress: number;
  status?: string;
}

export default function ProgressBar({ progress, status }: ProgressBarProps) {
  return (
    <div className="w-full">
      <div className="flex items-center justify-between mb-2">
        <span className="text-sm font-medium text-gray-700 dark:text-gray-300">
          {status || "Progress"}
        </span>
        <span className="text-sm text-gray-500 dark:text-gray-400">{Math.round(progress)}%</span>
      </div>
      <div className="w-full bg-gray-200 dark:bg-gray-700 rounded-full h-2 overflow-hidden">
        <motion.div
          className="h-full bg-blue-600 dark:bg-blue-500 rounded-full"
          initial={{ width: 0 }}
          animate={{ width: `${progress}%` }}
          transition={{ duration: 0.3, ease: "easeOut" }}
        />
      </div>
    </div>
  );
}
</file>

<file path="frontend/components/SourcePanel.tsx">
/**
 * Component for displaying research sources.
 */
import React from "react";
import type { Source } from "@/types/research";

interface SourcePanelProps {
  sources: Source[];
}

export default function SourcePanel({ sources }: SourcePanelProps) {
  if (sources.length === 0) {
    return (
      <div className="text-center py-8 text-gray-500 dark:text-gray-400">
        No sources found yet.
      </div>
    );
  }

  return (
    <div className="space-y-3">
      {sources.map((source, index) => (
        <div
          key={index}
          className="rounded-lg border border-gray-200 dark:border-gray-700 bg-white dark:bg-gray-800 p-4 hover:shadow-md transition-shadow"
        >
          <a
            href={source.url}
            target="_blank"
            rel="noopener noreferrer"
            className="text-sm font-medium text-blue-600 dark:text-blue-400 hover:underline block mb-1"
          >
            {source.title}
          </a>
          {source.snippet && (
            <p className="text-xs text-gray-600 dark:text-gray-300 line-clamp-2">
              {source.snippet}
            </p>
          )}
          {source.fetched_at && (
            <p className="text-xs text-gray-400 dark:text-gray-500 mt-2">
              Fetched: {new Date(source.fetched_at).toLocaleString()}
            </p>
          )}
        </div>
      ))}
    </div>
  );
}
</file>

<file path="frontend/next-env.d.ts">
/// <reference types="next" />
/// <reference types="next/image-types/global" />
import "./.next/dev/types/routes.d.ts";

// NOTE: This file should not be edited
// see https://nextjs.org/docs/app/api-reference/config/typescript for more information.
</file>

<file path="frontend/next.config.js">
/** @type {import('next').NextConfig} */
const nextConfig = {
  reactStrictMode: true,
  typescript: {
    // TODO: Set to false once all TypeScript errors are resolved
    ignoreBuildErrors: false,
  },
  eslint: {
    // TODO: Set to false once all ESLint errors are resolved
    ignoreDuringBuilds: false,
  },
};

module.exports = nextConfig;
</file>

<file path="frontend/package.json">
{
  "name": "agent-bletchley-frontend",
  "version": "1.0.0",
  "private": true,
  "scripts": {
    "dev": "next dev",
    "build": "next build",
    "start": "next start",
    "lint": "next lint"
  },
  "dependencies": {
    "next": "^16.0.0",
    "react": "^18.3.1",
    "react-dom": "^18.3.1",
    "typescript": "^5.6.0",
    "tailwindcss": "^3.4.0",
    "framer-motion": "^11.0.0",
    "@supabase/supabase-js": "^2.39.0"
  },
  "devDependencies": {
    "@types/node": "^20.11.0",
    "@types/react": "^18.3.0",
    "@types/react-dom": "^18.3.0",
    "autoprefixer": "^10.4.17",
    "postcss": "^8.4.33",
    "eslint": "^9.0.0",
    "eslint-config-next": "^16.0.0"
  }
}
</file>

<file path="frontend/postcss.config.js">
module.exports = {
  plugins: {
    tailwindcss: {},
    autoprefixer: {},
  },
};
</file>

<file path="frontend/README.md">
# Agent Bletchley Frontend

Next.js 16 frontend application for the Agent Bletchley research system.

## Setup

### Prerequisites

- Node.js 18 or higher
- npm or yarn

### Installation

1. Install dependencies:
```bash
npm install
```

2. Configure environment variables:
```bash
cp .env.local.example .env.local
# Edit .env.local with your configuration
```

### Required Environment Variables

- `NEXT_PUBLIC_BACKEND_URL` - Backend API URL (default: http://localhost:8000)
- `NEXT_PUBLIC_WS_URL` - WebSocket URL (default: ws://localhost:8000)
- `NEXT_PUBLIC_SUPABASE_URL` - Your Supabase project URL
- `NEXT_PUBLIC_SUPABASE_ANON_KEY` - Your Supabase anon key

### Running the Development Server

```bash
npm run dev
```

The frontend will be available at `http://localhost:3000`

### Building for Production

```bash
npm run build
npm start
```

### Project Structure

```
frontend/
├── app/                    # Next.js App Router pages
│   ├── layout.tsx         # Root layout
│   ├── page.tsx           # Home page
│   ├── research/          # Research dashboard pages
│   └── api/               # API routes
├── components/            # React components
├── lib/                   # Utility libraries
├── types/                 # TypeScript type definitions
├── package.json           # Dependencies
└── README.md              # This file
```

### Features

- **Next.js 16 App Router** - Modern routing and server components
- **TypeScript** - Type-safe development
- **Tailwind CSS** - Utility-first CSS with dark mode support
- **Framer Motion** - Animations and transitions
- **Supabase** - Database integration
- **WebSocket** - Real-time updates for research jobs

### Development

The codebase uses:
- TypeScript strict mode
- App Router (not Pages Router)
- Tailwind CSS with dark mode
- Proper error boundaries
- TypeScript interfaces for all props
</file>

<file path="frontend/tailwind.config.ts">
import type { Config } from "tailwindcss";

const config: Config = {
  content: [
    "./pages/**/*.{js,ts,jsx,tsx,mdx}",
    "./components/**/*.{js,ts,jsx,tsx,mdx}",
    "./app/**/*.{js,ts,jsx,tsx,mdx}",
  ],
  darkMode: "class",
  theme: {
    extend: {
      colors: {
        background: "var(--background)",
        foreground: "var(--foreground)",
      },
    },
  },
  plugins: [],
};
export default config;
</file>

<file path="frontend/tsconfig.json">
{
  "compilerOptions": {
    "target": "ES2020",
    "lib": [
      "dom",
      "dom.iterable",
      "esnext"
    ],
    "allowJs": true,
    "skipLibCheck": true,
    "strict": true,
    "noEmit": true,
    "esModuleInterop": true,
    "module": "esnext",
    "moduleResolution": "bundler",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "jsx": "react-jsx",
    "incremental": true,
    "plugins": [
      {
        "name": "next"
      }
    ],
    "paths": {
      "@/*": [
        "./*"
      ]
    }
  },
  "include": [
    "next-env.d.ts",
    "**/*.ts",
    "**/*.tsx",
    ".next/types/**/*.ts",
    ".next/dev/types/**/*.ts"
  ],
  "exclude": [
    "node_modules"
  ]
}
</file>

<file path="frontend/types/research.ts">
/**
 * TypeScript types for research jobs and related data structures.
 */

export enum ResearchJobStatus {
  PENDING = "pending",
  RUNNING = "running",
  COMPLETED = "completed",
  FAILED = "failed",
  CANCELLED = "cancelled",
}

export interface ResearchJobCreate {
  query: string;
  context?: Record<string, unknown>;
}

export interface ResearchJob {
  id: string;
  query: string;
  status: ResearchJobStatus;
  progress: number;
  created_at: string;
  updated_at?: string;
  completed_at?: string;
  sources: Source[];
  iterations: ResearchIteration[];
  report?: string;
  error?: string;
}

export interface Source {
  url: string;
  title: string;
  snippet?: string;
  fetched_at?: string;
  content?: string;
}

export interface ResearchIteration {
  id: string;
  step: number;
  action: string;
  timestamp: string;
  results?: unknown;
}

export interface WebSocketMessage {
  type: "status" | "progress" | "iteration" | "source" | "report" | "error";
  job_id: string;
  data: unknown;
}
</file>

<file path="QUICKSTART.md">
# Quick Start Guide - Windows PowerShell

## Backend Setup (from project root)

```powershell
# 1. Navigate to backend directory
cd backend

# 2. Activate virtual environment
.\venv\Scripts\Activate.ps1

# If you get execution policy error, run this first:
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser

# 3. Install dependencies (if not already installed)
pip install -r requirements.txt

# 4. Create .env file (if not exists)
Copy-Item .env.example .env
# Then edit .env with your API keys

# 5. Start the server
uvicorn app.main:app --reload
```

## Alternative: Use the startup script

From the `backend` directory:
```powershell
.\start.ps1
```

## Verify it's working

Once the server starts, visit:
- API: http://localhost:8000
- Should return: `{"message": "Hello Agent Bletchley"}`
- Docs: http://localhost:8000/docs

## Troubleshooting

**Problem**: `Activate.ps1` not found
- **Solution**: Make sure you're in the `backend` directory, not the root

**Problem**: Execution policy error
- **Solution**: Run `Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser`

**Problem**: `uvicorn` not found
- **Solution**: Make sure venv is activated (you should see `(venv)` in your prompt)

**Problem**: `requirements.txt` not found
- **Solution**: Make sure you're in the `backend` directory
</file>

<file path="supabase/migrations/001_initial_schema.sql">
-- Initial database schema for Agent Bletchley
-- Run this migration in your Supabase SQL editor

-- Research jobs table
CREATE TABLE IF NOT EXISTS research_jobs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    query TEXT NOT NULL,
    status TEXT NOT NULL DEFAULT 'pending' CHECK (status IN ('pending', 'running', 'completed', 'failed', 'cancelled')),
    progress DECIMAL(5, 2) DEFAULT 0.0 CHECK (progress >= 0.0 AND progress <= 100.0),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE,
    completed_at TIMESTAMP WITH TIME ZONE,
    report TEXT,
    error TEXT,
    context JSONB DEFAULT '{}'::jsonb
);

-- Research iterations table
CREATE TABLE IF NOT EXISTS research_iterations (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    job_id UUID NOT NULL REFERENCES research_jobs(id) ON DELETE CASCADE,
    step INTEGER NOT NULL,
    action TEXT NOT NULL,
    results JSONB DEFAULT '{}'::jsonb,
    timestamp TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    CONSTRAINT unique_job_step UNIQUE (job_id, step)
);

-- Research sources table
CREATE TABLE IF NOT EXISTS research_sources (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    job_id UUID NOT NULL REFERENCES research_jobs(id) ON DELETE CASCADE,
    url TEXT NOT NULL,
    title TEXT,
    snippet TEXT,
    content TEXT,
    fetched_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    UNIQUE(job_id, url)
);

-- Indexes for better query performance
CREATE INDEX IF NOT EXISTS idx_research_jobs_status ON research_jobs(status);
CREATE INDEX IF NOT EXISTS idx_research_jobs_created_at ON research_jobs(created_at DESC);
CREATE INDEX IF NOT EXISTS idx_research_iterations_job_id ON research_iterations(job_id);
CREATE INDEX IF NOT EXISTS idx_research_sources_job_id ON research_sources(job_id);

-- Function to automatically update updated_at timestamp
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = NOW();
    RETURN NEW;
END;
$$ language 'plpgsql';

-- Trigger to update updated_at on research_jobs
CREATE TRIGGER update_research_jobs_updated_at
    BEFORE UPDATE ON research_jobs
    FOR EACH ROW
    EXECUTE FUNCTION update_updated_at_column();

-- Enable Row Level Security (RLS)
ALTER TABLE research_jobs ENABLE ROW LEVEL SECURITY;
ALTER TABLE research_iterations ENABLE ROW LEVEL SECURITY;
ALTER TABLE research_sources ENABLE ROW LEVEL SECURITY;

-- TODO: Create RLS policies based on your authentication requirements
-- Example policy (adjust as needed):
-- CREATE POLICY "Users can view their own research jobs"
--     ON research_jobs FOR SELECT
--     USING (auth.uid() = user_id);
--
-- CREATE POLICY "Users can create research jobs"
--     ON research_jobs FOR INSERT
--     WITH CHECK (true);
--
-- CREATE POLICY "Users can update their own research jobs"
--     ON research_jobs FOR UPDATE
--     USING (auth.uid() = user_id);
</file>

<file path="backend/app/config.py">
"""
Configuration management using environment variables.
"""
import os
from typing import Optional
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()


class Settings:
    """Application settings loaded from environment variables."""
    
    # API Keys
    OPENROUTER_API_KEY: str = os.getenv("OPENROUTER_API_KEY", "")
    BRAVE_SEARCH_API_KEY: str = os.getenv("BRAVE_SEARCH_API_KEY", "")
    JINA_READER_API_KEY: str = os.getenv("JINA_READER_API_KEY", "")
    
    # Supabase
    SUPABASE_URL: str = os.getenv("SUPABASE_URL", "")
    SUPABASE_KEY: str = os.getenv("SUPABASE_KEY", "")
    
    # OpenRouter Configuration
    OPENROUTER_BASE_URL: str = os.getenv("OPENROUTER_BASE_URL", "https://openrouter.ai/api/v1")
    TONGYI_MODEL: str = os.getenv("TONGYI_MODEL", "alibaba/tongyi-deepresearch-30b-a3b")
    
    # Server Configuration
    BACKEND_HOST: str = os.getenv("BACKEND_HOST", "0.0.0.0")
    BACKEND_PORT: int = int(os.getenv("BACKEND_PORT", "8000"))
    
    # CORS
    FRONTEND_URL: str = os.getenv("FRONTEND_URL", "http://localhost:3000")
    
    # Logging
    LOG_LEVEL: str = os.getenv("LOG_LEVEL", "INFO")
    
    @classmethod
    def validate(cls) -> None:
        """Validate that required environment variables are set."""
        required_vars = [
            "OPENROUTER_API_KEY",
            "BRAVE_SEARCH_API_KEY",
            "JINA_READER_API_KEY",
            "SUPABASE_URL",
            "SUPABASE_KEY",
        ]
        missing = [var for var in required_vars if not getattr(cls, var)]
        if missing:
            raise ValueError(f"Missing required environment variables: {', '.join(missing)}")


# Global settings instance
settings = Settings()
</file>

</files>
