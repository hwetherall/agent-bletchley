"""
Main research engine that orchestrates the research process.
"""
import logging
import asyncio
import json
from datetime import datetime
from typing import Dict, List, Any, Optional, TYPE_CHECKING
from app.models.research_job import ResearchJob, ResearchJobStatus
from app.orchestrator.tongyi_client import TongyiClient
from app.tools.tool_registry import ToolRegistry
from app.db.supabase_client import (
    update_job_status as db_update_job_status,
    update_job_report,
    add_iteration,
    add_source,
    get_sources_by_job,
    get_job
)

if TYPE_CHECKING:
    from app.api.websocket import ConnectionManager

logger = logging.getLogger(__name__)


class ResearchEngine:
    """
    Main research engine that coordinates AI agent research activities.
    
    TODO: Implement the main research loop that:
    1. Receives research query
    2. Uses Tongyi DeepResearch agent to plan research steps
    3. Executes tools (web search, web fetch) based on agent decisions
    4. Aggregates and synthesizes results
    5. Returns comprehensive research report
    """
    
    def __init__(self):
        """Initialize the research engine."""
        self.tongyi_client = TongyiClient()
        self.tool_registry = ToolRegistry()
    
    async def start_research(
        self, 
        query: str, 
        job_id: str, 
        connection_manager: Optional["ConnectionManager"] = None
    ) -> ResearchJob:
        """
        Start a new research job.
        
        Args:
            query: The research query/question
            job_id: Unique identifier for this research job
            connection_manager: Optional WebSocket connection manager for real-time updates
            
        Returns:
            ResearchJob instance with initial status
        """
        # TODO: Create initial research job in database
        # TODO: Initialize research context and state
        
        logger.info(f"Starting research job {job_id} with query: {query}")
        
        # Update job status to running in database
        await db_update_job_status(job_id, "running", 0.0)
        
        job = ResearchJob(
            id=job_id,
            query=query,
            status=ResearchJobStatus.RUNNING,
        )
        
        # Broadcast initial RUNNING status
        if connection_manager:
            asyncio.create_task(
                connection_manager.broadcast_status(job_id, ResearchJobStatus.RUNNING.value, 0.0)
            )
        
        # Begin research loop
        asyncio.create_task(self._run_research_loop(job_id, query, connection_manager))
        
        return job
    
    async def _run_research_loop(
        self,
        job_id: str,
        query: str,
        connection_manager: Optional["ConnectionManager"] = None
    ) -> None:
        """
        Run the main research loop with Tongyi DeepResearch agent.
        
        Args:
            job_id: Research job ID
            query: Research query
            connection_manager: Optional WebSocket connection manager
        """
        max_iterations = 20
        iteration_count = 0
        
        try:
            # Update status to RUNNING in database (already done in start_research, but ensure it's set)
            await db_update_job_status(job_id, "running", 0.0)
            
            # Broadcast status to WebSocket
            if connection_manager:
                asyncio.create_task(
                    connection_manager.broadcast_status(job_id, ResearchJobStatus.RUNNING.value, 0.0)
                )
            
            # Initialize conversation with system prompt and get tool definitions
            tools = self.tool_registry.get_tool_definitions()
            
            system_prompt = (
                "You are an expert research assistant specialized in investment due diligence. "
                "Your goal is to conduct thorough, comprehensive research on companies, markets, and investment opportunities.\n\n"
                
                "MANDATORY TOOL USAGE RULES:\n"
                "- You MUST use tool calls (not text descriptions) to gather information.\n"
                "- When you need information, invoke tools using the tool_calls format (JSON-structured function calls).\n"
                "- NEVER describe what tools to use in your response content - always make actual tool calls.\n"
                "- DO NOT output text like 'I should use web_search' or 'I need to call web_fetch'.\n"
                "- DO NOT write tool names, function names, or arguments as plain text.\n"
                "- ALWAYS invoke tools directly - the system will execute them automatically.\n\n"
                
                "Available tools:\n"
                "- web_search: Search the web for information using Brave Search. Use this FIRST to find relevant sources.\n"
                "- web_fetch: Fetch and parse content from web URLs. Use this AFTER search to read articles.\n\n"
                
                "Workflow:\n"
                "1. Start by using web_search to find relevant sources about the topic\n"
                "2. Use web_fetch to read important articles and pages you found\n"
                "3. Continue gathering information until you have comprehensive coverage\n"
                "4. Only provide your final answer when you have sufficient information from tools\n\n"
                
                "Explore multiple perspectives, verify facts, and synthesize your findings into a well-structured analysis. "
                "Remember: ALWAYS use tool calls, NEVER describe tools in text."
            )
            
            messages = [
                {
                    "role": "system",
                    "content": system_prompt
                },
                {
                    "role": "user",
                    "content": query
                }
            ]
            
            logger.info(f"Starting research loop for job {job_id} with query: {query}")
            
            # Main research loop
            while iteration_count < max_iterations:
                iteration_count += 1
                logger.info(f"Research iteration {iteration_count}/{max_iterations} for job {job_id}")
                
                try:
                    # Call Tongyi API with current conversation and tools
                    response = await self.tongyi_client.chat_completion(
                        messages=messages,
                        tools=tools
                    )
                    
                    # Extract tool calls and content
                    tool_calls = response.get("tool_calls", [])
                    content = response.get("content", "")
                    assistant_message = response.get("message", {})
                    
                    # Add assistant message to conversation
                    messages.append(assistant_message)
                    
                    # Process tool calls if any
                    if tool_calls:
                        logger.info(f"Processing {len(tool_calls)} tool call(s) in iteration {iteration_count}")
                        
                        # Execute tools sequentially
                        tool_results = []
                        for tool_call in tool_calls:
                            tool_id = tool_call.get("id", "")
                            function = tool_call.get("function", {})
                            tool_name = function.get("name", "")
                            tool_args = function.get("arguments", {})
                            
                            if not tool_name:
                                logger.warning(f"Skipping tool call with missing name: {tool_call}")
                                continue
                            
                            try:
                                # Execute tool
                                logger.info(f"Executing tool: {tool_name} with args: {tool_args}")
                                tool_result = await self.tool_registry.execute_tool(tool_name, tool_args)
                                
                                # Handle tool results
                                if tool_name == "web_search":
                                    # Extract search results and save sources
                                    search_results = tool_result.get("results", [])
                                    for result in search_results:
                                        url = result.get("url", "")
                                        title = result.get("title", "")
                                        snippet = result.get("snippet", "")
                                        
                                        if url:
                                            try:
                                                await add_source(job_id, url, title, snippet)
                                                
                                                # Broadcast source discovery
                                                if connection_manager:
                                                    source_data = {
                                                        "url": url,
                                                        "title": title,
                                                        "snippet": snippet,
                                                        "fetched_at": None
                                                    }
                                                    asyncio.create_task(
                                                        connection_manager.broadcast_source(job_id, source_data)
                                                    )
                                            except Exception as e:
                                                logger.error(f"Error saving source {url}: {e}", exc_info=True)
                                
                                elif tool_name == "web_fetch":
                                    # Extract fetched content and save source
                                    fetch_content = tool_result.get("content", {})
                                    if isinstance(fetch_content, dict):
                                        url = fetch_content.get("url", "")
                                        title = fetch_content.get("title", "")
                                        content = fetch_content.get("content", "")
                                        
                                        if url:
                                            try:
                                                await add_source(job_id, url, title, None, content)
                                                
                                                # Broadcast source fetch
                                                if connection_manager:
                                                    source_data = {
                                                        "url": url,
                                                        "title": title,
                                                        "snippet": content[:200] + "..." if len(content) > 200 else content,
                                                        "fetched_at": datetime.utcnow().isoformat()
                                                    }
                                                    asyncio.create_task(
                                                        connection_manager.broadcast_source(job_id, source_data)
                                                    )
                                            except Exception as e:
                                                logger.error(f"Error saving fetched source {url}: {e}", exc_info=True)
                                
                                # Format tool result for message
                                # Note: OpenAI/OpenRouter format requires only role, tool_call_id, and content
                                # The "name" field is NOT part of tool result messages (it's only in function definitions)
                                tool_results.append({
                                    "tool_call_id": tool_id,
                                    "role": "tool",
                                    "content": json.dumps(tool_result)
                                })
                                
                            except Exception as e:
                                logger.error(f"Error executing tool {tool_name}: {e}", exc_info=True)
                                # Add error result to continue loop
                                # Note: OpenAI/OpenRouter format requires only role, tool_call_id, and content
                                # The "name" field is NOT part of tool result messages
                                tool_results.append({
                                    "tool_call_id": tool_id,
                                    "role": "tool",
                                    "content": json.dumps({"error": str(e)})
                                })
                        
                        # Add tool results to conversation
                        messages.extend(tool_results)
                        
                        # Prepare iteration data
                        action = f"tool_execution: {', '.join([tc.get('function', {}).get('name', 'unknown') for tc in tool_calls])}"
                        step_result = {
                            "status": "completed",
                            "action": action,
                            "timestamp": datetime.utcnow().isoformat(),
                            "step": iteration_count,
                            "tool_calls": tool_calls,
                            "tool_results": [r.get("content", "") for r in tool_results]
                        }
                        
                    else:
                        # No tool calls - final answer received
                        logger.info(f"Final answer received in iteration {iteration_count}")
                        
                        # Prepare iteration data
                        action = "final_answer"
                        step_result = {
                            "status": "completed",
                            "action": action,
                            "timestamp": datetime.utcnow().isoformat(),
                            "step": iteration_count,
                            "content": content
                        }
                        
                        # Persist final iteration
                        await add_iteration(job_id, iteration_count, action, step_result)
                        
                        # Broadcast final iteration
                        if connection_manager:
                            iteration_data = {
                                "id": f"{job_id}-iter-{iteration_count}",
                                "step": iteration_count,
                                "action": action,
                                "timestamp": step_result.get("timestamp"),
                                "results": step_result
                            }
                            asyncio.create_task(
                                connection_manager.broadcast_iteration(job_id, iteration_data)
                            )
                        
                        # Break from loop - research complete
                        break
                    
                    # Persist iteration to database
                    await add_iteration(job_id, iteration_count, action, step_result)
                    
                    # Broadcast iteration update
                    if connection_manager:
                        iteration_data = {
                            "id": f"{job_id}-iter-{iteration_count}",
                            "step": iteration_count,
                            "action": action,
                            "timestamp": step_result.get("timestamp"),
                            "results": step_result
                        }
                        asyncio.create_task(
                            connection_manager.broadcast_iteration(job_id, iteration_data)
                        )
                    
                    # Update progress: research phase is 0-90%
                    progress = min(90, (iteration_count / max_iterations) * 90)
                    await db_update_job_status(job_id, "running", progress)
                    
                    # Broadcast progress update
                    if connection_manager:
                        asyncio.create_task(
                            connection_manager.broadcast_status(job_id, ResearchJobStatus.RUNNING.value, progress)
                        )
                    
                except Exception as e:
                    logger.error(f"Error in iteration {iteration_count} for job {job_id}: {e}", exc_info=True)
                    # Continue to next iteration - don't break entire loop
                    # Save error iteration
                    error_result = {
                        "status": "error",
                        "action": "error",
                        "timestamp": datetime.utcnow().isoformat(),
                        "step": iteration_count,
                        "error": str(e)
                    }
                    try:
                        await add_iteration(job_id, iteration_count, "error", error_result)
                    except Exception:
                        pass  # Don't fail on iteration save error
                    continue
            
            # Synthesize results
            logger.info(f"Starting synthesis for job {job_id}")
            
            # Update progress to 90% (entering synthesis phase)
            await db_update_job_status(job_id, "running", 90.0)
            if connection_manager:
                asyncio.create_task(
                    connection_manager.broadcast_status(job_id, ResearchJobStatus.RUNNING.value, 90.0)
                )
            
            # Fetch all sources from database
            sources = await get_sources_by_job(job_id)
            logger.info(f"Found {len(sources)} sources for synthesis")
            
            # Generate final report
            report = await self.synthesize_results(job_id, sources)
            
            # Update progress to 100%
            await db_update_job_status(job_id, "running", 100.0)
            if connection_manager:
                asyncio.create_task(
                    connection_manager.broadcast_status(job_id, ResearchJobStatus.RUNNING.value, 100.0)
                )
            
            # Persist report to database
            await update_job_report(job_id, report)
            
            # Broadcast final report
            if connection_manager:
                asyncio.create_task(
                    connection_manager.broadcast_report(job_id, report)
                )
            
            # Update status to COMPLETED in database
            await db_update_job_status(job_id, "completed", 100.0)
            
            # Broadcast completion status
            if connection_manager:
                asyncio.create_task(
                    connection_manager.broadcast_status(job_id, ResearchJobStatus.COMPLETED.value, 100.0)
                )
            
            logger.info(f"Research loop completed successfully for job {job_id}")
                
        except Exception as e:
            logger.error(f"Error in research loop for job {job_id}: {e}", exc_info=True)
            
            # Update status to FAILED in database
            await db_update_job_status(job_id, "failed", None)
            
            # Broadcast error
            if connection_manager:
                asyncio.create_task(
                    connection_manager.broadcast_error(job_id, str(e))
                )
                asyncio.create_task(
                    connection_manager.broadcast_status(job_id, ResearchJobStatus.FAILED.value, None)
                )
    
    async def execute_research_step(
        self,
        job_id: str,
        step: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Execute a single research step.
        
        Args:
            job_id: The research job ID
            step: Step definition from AI agent
            
        Returns:
            Step execution results
        """
        # TODO: Parse step instruction from agent
        # TODO: Select appropriate tool(s) from registry
        # TODO: Execute tool with parameters
        # TODO: Return results to agent for next step
        
        from datetime import datetime
        
        logger.info(f"Executing research step {step.get('step', 'unknown')} for job {job_id}")
        return {
            "status": "completed",
            "action": step.get("action", "research"),
            "timestamp": datetime.utcnow().isoformat(),
            "step": step.get("step")
        }
    
    async def synthesize_results(
        self,
        job_id: str,
        sources: List[Dict[str, Any]]
    ) -> str:
        """
        Synthesize research results into final report.
        
        Args:
            job_id: The research job ID
            sources: List of gathered sources and findings (fallback if fetch fails)
            
        Returns:
            Final research report as formatted markdown text
        """
        logger.info(f"Synthesizing results for job {job_id}")
        
        try:
            # Fetch job to get the original query
            job_data = await get_job(job_id)
            if not job_data:
                logger.error(f"Job {job_id} not found for synthesis")
                return "# Research Report\n\nError: Could not retrieve job data."
            
            query = job_data.get("query", "Unknown query")
            
            # Fetch all sources from database (as per plan)
            try:
                sources = await get_sources_by_job(job_id)
                logger.info(f"Fetched {len(sources)} sources for synthesis")
            except Exception as e:
                logger.warning(f"Error fetching sources, using provided sources: {e}")
                # Use provided sources as fallback
                if not sources:
                    sources = []
            
            # Build sources list for prompt
            sources_list = []
            for idx, source in enumerate(sources, 1):
                source_info = f"[{idx}] {source.get('title', 'Untitled')}\n   URL: {source.get('url', 'N/A')}"
                if source.get('snippet'):
                    source_info += f"\n   Preview: {source.get('snippet', '')[:200]}"
                sources_list.append(source_info)
            
            sources_text = "\n".join(sources_list) if sources_list else "No sources were gathered during research."
            
            # Build synthesis prompt
            synthesis_prompt = f"""You are synthesizing a comprehensive investment research report based on gathered information.

Research Query: {query}

Sources Gathered ({len(sources)} total):
{sources_text}

Please generate a comprehensive, well-structured research report in markdown format that includes:

1. **Executive Summary** - A concise overview of key findings and conclusions
2. **Key Findings** - Main discoveries with source citations using [1], [2], etc. format
3. **Detailed Analysis** - In-depth examination of important aspects
4. **Risk Factors** - Potential risks and concerns identified
5. **Sources** - Numbered list of all sources with full URLs

Requirements:
- Use markdown formatting for structure and readability
- Cite sources using [1], [2], etc. format corresponding to the numbered sources above
- Be thorough and objective
- Include specific details and data points where available
- Maintain professional tone suitable for investment due diligence

Generate the report now:"""
            
            # Call Tongyi API for synthesis
            messages = [
                {
                    "role": "system",
                    "content": "You are an expert financial analyst specializing in investment research and due diligence reports."
                },
                {
                    "role": "user",
                    "content": synthesis_prompt
                }
            ]
            
            logger.info(f"Calling Tongyi API for report synthesis")
            response = await self.tongyi_client.chat_completion(
                messages=messages,
                tools=None  # No tools needed for synthesis
            )
            
            # Extract report content
            report_content = response.get("content", "")
            
            if not report_content or len(report_content.strip()) < 50:
                logger.warning("Synthesis returned empty or very short report, generating fallback")
                # Generate fallback report
                report_content = self._generate_fallback_report(query, sources)
            
            # Ensure sources section is included
            if "## Sources" not in report_content and "**Sources**" not in report_content:
                report_content += self._format_sources_section(sources)
            
            logger.info(f"Successfully synthesized report for job {job_id} ({len(report_content)} characters)")
            return report_content
            
        except Exception as e:
            logger.error(f"Error synthesizing results for job {job_id}: {e}", exc_info=True)
            # Return fallback report
            try:
                job_data = await get_job(job_id)
                query = job_data.get("query", "Unknown query") if job_data else "Unknown query"
            except Exception:
                query = "Unknown query"
            
            return self._generate_fallback_report(query, sources)
    
    def _generate_fallback_report(self, query: str, sources: List[Dict[str, Any]]) -> str:
        """Generate a fallback report when synthesis fails."""
        report = f"""# Research Report

## Executive Summary

Research was conducted on: {query}

## Key Findings

{len(sources)} source(s) were gathered during research, but automatic synthesis encountered an error. Please review the sources below for detailed information.

## Sources

{self._format_sources_section(sources)}
"""
        return report
    
    def _format_sources_section(self, sources: List[Dict[str, Any]]) -> str:
        """Format sources into a numbered list section."""
        if not sources:
            return "\n## Sources\n\nNo sources were gathered during research.\n"
        
        sources_text = "\n## Sources\n\n"
        for idx, source in enumerate(sources, 1):
            title = source.get("title", "Untitled")
            url = source.get("url", "")
            sources_text += f"{idx}. **{title}**\n   - {url}\n\n"
        
        return sources_text
    
    async def update_job_status(
        self,
        job_id: str,
        status: ResearchJobStatus,
        progress: Optional[float] = None,
        connection_manager: Optional["ConnectionManager"] = None
    ) -> None:
        """
        Update research job status.
        
        Args:
            job_id: The research job ID
            status: New status
            progress: Optional progress percentage (0-100)
            connection_manager: Optional WebSocket connection manager for real-time updates
        """
        # Update job status in database
        await db_update_job_status(job_id, status.value, progress)
        
        logger.info(f"Updating job {job_id} status to {status}")
        
        # Broadcast status update via WebSocket
        if connection_manager:
            asyncio.create_task(
                connection_manager.broadcast_status(job_id, status.value, progress)
            )

